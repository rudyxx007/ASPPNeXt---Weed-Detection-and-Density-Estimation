{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550fe726-542f-47d2-ab73-2905ae98b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torchvision as TV\n",
    "import torchaudio as TA\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import segmentation_models_pytorch as smp\n",
    "from glob import glob\n",
    "import albumentations as A\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536f24c4-31da-4781-8b6f-03a75276059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- DEVICE -----------------------\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f30aa41-32ab-4d82-8665-74505221a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Paths -----------------------\n",
    "train_images = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\train_new\"\n",
    "train_masks = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\trainlabel_new\"\n",
    "validation_images = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\validation_new\"\n",
    "validation_masks = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\validationlabel_new\"\n",
    "test_images = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\test_new\"\n",
    "test_masks = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\testlabel_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e07beed-1b72-458b-b846-23816cd8bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "Verifying Training File Pairs üîç: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:00<00:00, 99986.39it/s]\n",
      "Verifying Validation File Pairs üîç: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<?, ?it/s]\n",
      "Verifying Testing File Pairs üîç: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [00:00<00:00, 243477.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Augmentations -----------------------\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.ElasticTransform(p=0.5),\n",
    "    A.D4(p=1),\n",
    "    A.ISONoise(color_shift=[0.01, 0.05], intensity=[0.1, 0.5], p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.2], contrast_limit=[-0.2, 0.2], p=0.5),\n",
    "    A.ElasticTransform(alpha=300, sigma=10, interpolation=cv2.INTER_NEAREST, mask_interpolation=cv2.INTER_NEAREST,\n",
    "                       same_dxdy=True, border_mode=cv2.BORDER_CONSTANT, fill=0, fill_mask=0, p=0.5),\n",
    "    A.Resize(512, 384),\n",
    "])\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ---------------------- Dataset Class -----------------------\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, train_transform=None, base_transform=None, dataset_type=\"Unknown\"):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.train_transform = train_transform\n",
    "        self.base_transform = base_transform\n",
    "        self.dataset_type = dataset_type\n",
    "        self.image_files = sorted(glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "        self.mask_files = sorted(glob(os.path.join(mask_dir, \"*.png\")))\n",
    "        self._verify_file_pairs()\n",
    "        \n",
    "    def _verify_file_pairs(self):\n",
    "        if len(self.image_files) != len(self.mask_files):\n",
    "            raise ValueError(f\"Mismatched counts in {self.dataset_type} dataset: {len(self.image_files)} images vs {len(self.mask_files)} masks\")\n",
    "            \n",
    "        for img_path, mask_path in tqdm(zip(self.image_files, self.mask_files), total=len(self.image_files), desc=f\"Verifying {self.dataset_type} File Pairs üîç\"):\n",
    "            img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            mask_name = os.path.splitext(os.path.basename(mask_path))[0]\n",
    "            if img_name != mask_name:\n",
    "                raise ValueError(f\"Filename mismatch in {self.dataset_type} dataset: {img_name} vs {mask_name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "def __getitem__(self, idx):\n",
    "    # Read image and mask\n",
    "    img = cv2.cvtColor(cv2.imread(self.image_files[idx]), cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(self.mask_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    mask = (mask > 127).astype(np.uint8)\n",
    "\n",
    "    original_img = self.base_transform(img)\n",
    "    original_mask = T.from_numpy(mask).long()\n",
    "\n",
    "    sample = {\n",
    "        \"original_img\": original_img,\n",
    "        \"original_mask\": original_mask\n",
    "    }\n",
    "\n",
    "    if self.train_transform:\n",
    "        augmented = self.train_transform(image=img, mask=mask)\n",
    "        aug_img = augmented[\"image\"]\n",
    "        aug_mask = augmented[\"mask\"]\n",
    "\n",
    "        augmented_img = self.base_transform(aug_img)\n",
    "        augmented_mask = T.from_numpy(aug_mask).long()\n",
    "\n",
    "        sample[\"augmented_img\"] = augmented_img\n",
    "        sample[\"augmented_mask\"] = augmented_mask\n",
    "    else:\n",
    "        sample[\"augmented_img\"] = original_img\n",
    "        sample[\"augmented_mask\"] = original_mask\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "# ---------------------- DataLoaders -----------------------\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, train_transform, base_transform, \"Training\")\n",
    "val_dataset = SegmentationDataset(validation_images, validation_masks, train_transform, base_transform, \"Validation\")\n",
    "test_dataset = SegmentationDataset(test_images, test_masks, train_transform, base_transform, \"Testing\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e295ec20-319a-4744-a78d-59878f76264f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n",
      "Epoch 1 [Train]:   0%|                                                                         | 0/200 [00:00<?, ?it/s]D:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "                                                                                                                       "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    134\u001b[39m num_epochs = \u001b[32m50\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     train_loss, train_metrics = \u001b[43mTrainUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     val_loss, val_metrics = ValidateUNet(model, val_dataloader, loss_fn)\n\u001b[32m    139\u001b[39m     T.cuda.empty_cache()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mTrainUNet\u001b[39m\u001b[34m(model, dataloader, loss_fn, optimizer, epoch)\u001b[39m\n\u001b[32m     92\u001b[39m optimizer.zero_grad()\n\u001b[32m     93\u001b[39m outputs = model(inputs)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m loss.backward()\n\u001b[32m     96\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mFocalTverskyLoss.forward\u001b[39m\u001b[34m(self, preds, targets)\u001b[39m\n\u001b[32m     35\u001b[39m probs = preds\n\u001b[32m     36\u001b[39m dims = (\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m TP = \u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m FP = T.sum(probs * (\u001b[32m1\u001b[39m - targets_one_hot), dims)\n\u001b[32m     39\u001b[39m FN = T.sum((\u001b[32m1\u001b[39m - probs) * targets_one_hot, dims)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Model -----------------------\n",
    "CUSTOM_SAVE_ROOT = Path(r\"D:\\AAU Internship\\Code\\UNet-Models\")\n",
    "os.makedirs(CUSTOM_SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder=\"efficientnet-b7\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    encoder_depth=4,\n",
    "    decoder_use_batchnorm='inplace',\n",
    "#    decoder_attention_type='scse',\n",
    "    decoder_channels=[256, 128, 64, 32],\n",
    "    in_channels=3,\n",
    "    classes=2,\n",
    "    activation=None,\n",
    "    center=True,\n",
    ").to(device)\n",
    "\n",
    "# ---------------------- Loss Function -----------------------\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3, gamma=0.75, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def update_hyperparams_by_epoch(self, epoch):\n",
    "        steps = epoch // 5\n",
    "        self.alpha = max(0.4, 0.7 - 0.03*steps)\n",
    "        self.beta = 1 - self.alpha\n",
    "        self.gamma = min(1.5, 0.5 + 0.1*steps)\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=preds.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        probs = preds\n",
    "        dims = (0, 2, 3)\n",
    "        TP = T.sum(probs * targets_one_hot, dims)\n",
    "        FP = T.sum(probs * (1 - targets_one_hot), dims)\n",
    "        FN = T.sum((1 - probs) * targets_one_hot, dims)\n",
    "        Tversky = (TP + self.smooth) / (TP + self.alpha * FP + self.beta * FN + self.smooth)\n",
    "        return T.mean((1 - Tversky) ** self.gamma)\n",
    "\n",
    "loss_fn = FocalTverskyLoss().to(device)\n",
    "\n",
    "# ---------------------- Metrics -----------------------\n",
    "def compute_metrics(preds, targets):\n",
    "    with T.no_grad():\n",
    "        pred_labels = T.argmax(preds, dim=1).cpu().numpy().flatten()\n",
    "        targets = targets.cpu().numpy().flatten()\n",
    "        ious = []\n",
    "        for cls in [0, 1]:\n",
    "            intersection = ((pred_labels == cls) & (targets == cls)).sum()\n",
    "            union = ((pred_labels == cls) | (targets == cls)).sum()\n",
    "            ious.append(intersection / (union + 1e-6))\n",
    "        class_acc = []\n",
    "        for cls in [0, 1]:\n",
    "            mask = (targets == cls)\n",
    "            if mask.sum() > 0:\n",
    "                class_acc.append((pred_labels[mask] == cls).mean())\n",
    "        mPA = np.mean(class_acc) * 100\n",
    "        cm = confusion_matrix(targets, pred_labels)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "        return {\n",
    "            \"Accuracy\": 100 * accuracy_score(targets, pred_labels),\n",
    "            \"mPA\": mPA,\n",
    "            \"Crop IoU\": 100 * ious[1],\n",
    "            \"mIoU\": 100 * np.mean(ious),\n",
    "            \"Precision\": 100 * precision_score(targets, pred_labels, zero_division=0),\n",
    "            \"Recall\": 100 * recall_score(targets, pred_labels, zero_division=0),\n",
    "            \"F1-Score\": 100 * f1_score(targets, pred_labels, zero_division=0),\n",
    "            \"FNR\": 100 * (FN / (FN + TP + 1e-6))\n",
    "        }\n",
    "\n",
    "# ---------------------- Training Setup -----------------------\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "MODEL_PATHS = {name: CUSTOM_SAVE_ROOT / f\"best_{name.replace(' ', '')}_model.pth\" for name in [\n",
    "    \"mPA\", \"mIoU\", \"Crop IoU\", \"Accuracy\", \"F1-Score\", \"Precision\", \"Recall\", \"FNR\"\n",
    "]}\n",
    "best_metrics = {k: {\"value\": -1 if k != \"FNR\" else float('inf'), \"path\": v} for k, v in MODEL_PATHS.items()}\n",
    "\n",
    "# ---------------------- Training & Validation -----------------------\n",
    "def TrainUNet(model, dataloader, loss_fn, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    loss_fn.update_hyperparams_by_epoch(epoch)\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for batch in loop:\n",
    "        inputs = batch['augmented_img'].to(device)\n",
    "        targets = batch['augmented_mask'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        all_preds.append(outputs.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    metrics = compute_metrics(T.cat(all_preds), T.cat(all_targets))\n",
    "\n",
    "    T.cuda.empty_cache()\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "def ValidateUNet(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    all_preds, all_targets = []\n",
    "    loop = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "\n",
    "    with T.no_grad():\n",
    "        for batch in loop:\n",
    "            inputs = batch['original_img'].to(device)\n",
    "            targets = batch['original_mask'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            all_preds.append(outputs.detach().cpu())\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    metrics = compute_metrics(T.cat(all_preds), T.cat(all_targets))\n",
    "    \n",
    "    T.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss, metrics\n",
    "\n",
    "# ---------------------- Main Training -----------------------\n",
    "num_epochs = 50\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_metrics = TrainUNet(model, train_dataloader, loss_fn, optimizer, epoch)\n",
    "    val_loss, val_metrics = ValidateUNet(model, val_dataloader, loss_fn)\n",
    "\n",
    "    T.cuda.empty_cache()\n",
    "\n",
    "    for metric_name in best_metrics.keys():\n",
    "        current_value = val_metrics[metric_name]\n",
    "        is_better = current_value > best_metrics[metric_name][\"value\"] if metric_name != \"FNR\" else current_value < best_metrics[metric_name][\"value\"]\n",
    "        if is_better:\n",
    "            best_metrics[metric_name][\"value\"] = current_value\n",
    "            T.save(model.state_dict(), str(best_metrics[metric_name][\"path\"]))\n",
    "            print(f\"‚úÖ New best {metric_name}: {current_value:.2f}% | Saved to: {best_metrics[metric_name]['path']}\")\n",
    "\n",
    "    print(f\"\\nüìä Epoch {epoch} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    for k, v in val_metrics.items():\n",
    "        print(f\"{k}: {v:.2f}%\")\n",
    "\n",
    "\n",
    "# ---------------------- Final Report -----------------------\n",
    "print(\"\\nüéØ === Best Models Summary ===\")\n",
    "for metric_name, data in best_metrics.items():\n",
    "    print(f\"{metric_name}: {data['value']:.2f}% ‚Üí {data['path']}\")\n",
    "\n",
    "# ---------------------- Testing -----------------------\n",
    "print(\"\\nüß™ === Testing Saved Models ===\")\n",
    "for metric_name, data in tqdm(best_metrics.items(), desc=\"Testing Models\"):\n",
    "    model.load_state_dict(T.load(str(data[\"path\"])))\n",
    "    test_loss, test_metrics = ValidateUNet(model, test_dataloader, loss_fn)\n",
    "    print(f\"\\nüìå {metric_name} Model Test Results:\")\n",
    "    for k, v in test_metrics.items():\n",
    "        print(f\"{k}: {v:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34eee5-3d9d-4e60-b4b4-09bdc2f69c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Allocated memory: {T.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached memory: {T.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81ad32-8d35-4c48-9e50-efd1d30b2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a23e61-bba2-4e65-ab37-e0f7a268b6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(T.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432ed0e-b7c4-46a2-a7c7-32b39a53d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[MEMORY] Allocated: {T.cuda.memory_allocated() / 1024**2:.2f} MB | Reserved: {T.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a266528-ef68-4aeb-b34b-5391af902e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
