{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550fe726-542f-47d2-ab73-2905ae98b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torchvision as TV\n",
    "import torchaudio as TA\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm as tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import segmentation_models_pytorch as smp\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536f24c4-31da-4781-8b6f-03a75276059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if T.cuda.is_available():\n",
    "    device=T.device(\"cuda\")\n",
    "else:\n",
    "    device=T.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f30aa41-32ab-4d82-8665-74505221a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Paths ----------\n",
    "train_images = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\train_new\"\n",
    "train_masks = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\trainlabel_new\"\n",
    "validation_images = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\validation_new\"\n",
    "validation_masks = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\validationlabel_new\"\n",
    "test_images = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\test_new\"\n",
    "test_masks = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\\testlabel_new\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e07beed-1b72-458b-b846-23816cd8bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "Verifying Training File Pairs üîç: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:00<00:00, 72726.24it/s]\n",
      "Verifying Validation File Pairs üîç: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 58652.27it/s]\n",
      "Verifying Testing File Pairs üîç: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [00:00<00:00, 66657.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Augmentations -----------------------\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.ElasticTransform(p=0.5),\n",
    "    A.D4(p=1),\n",
    "    A.ISONoise(\n",
    "        color_shift=[0.01, 0.05],\n",
    "        intensity=[0.1, 0.5],\n",
    "        p=0.5\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.2], contrast_limit=[-0.2, 0.2], brightness_by_max=True, ensure_safe_range=False, p=0.5),\n",
    "    A.ElasticTransform(\n",
    "        alpha=300,\n",
    "        sigma=10,\n",
    "        interpolation=cv2.INTER_NEAREST,\n",
    "        approximate=False,\n",
    "        same_dxdy=True,\n",
    "        mask_interpolation=cv2.INTER_NEAREST,\n",
    "        noise_distribution=\"gaussian\",\n",
    "        keypoint_remapping_method=\"mask\",\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        fill=0,\n",
    "        fill_mask=0\n",
    "    ),\n",
    "])\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ---------------------- Dataset Class -----------------------\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, train_transform=None, base_transform=None, dataset_type=\"Unknown\"):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.train_transform = train_transform\n",
    "        self.base_transform = base_transform\n",
    "        self.dataset_type = dataset_type\n",
    "        self.image_files = sorted(glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "        self.mask_files = sorted(glob(os.path.join(mask_dir, \"*.png\")))\n",
    "        self._verify_file_pairs()\n",
    "        \n",
    "    def _verify_file_pairs(self):\n",
    "        if len(self.image_files) != len(self.mask_files):\n",
    "            raise ValueError(f\"Mismatched counts in {self.dataset_type} dataset: {len(self.image_files)} images vs {len(self.mask_files)} masks\")\n",
    "            \n",
    "        for img_path, mask_path in tqdm(zip(self.image_files, self.mask_files), total=len(self.image_files), desc=f\"Verifying {self.dataset_type} File Pairs üîç\"):\n",
    "            img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            mask_name = os.path.splitext(os.path.basename(mask_path))[0]\n",
    "            if img_name != mask_name:\n",
    "                raise ValueError(f\"Filename mismatch in {self.dataset_type} dataset: {img_name} vs {mask_name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.image_files[idx]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        original_img = self.base_transform(T.from_numpy(img).permute(2, 0, 1).float()).to(device)\n",
    "        original_mask = T.from_numpy(mask).long().to(device)  # Convert mask to tensor directly\n",
    "        \n",
    "        if self.train_transform:\n",
    "            augmented = self.train_transform(image=img, mask=mask)\n",
    "            aug_img = augmented['image']\n",
    "            aug_mask = augmented['mask']\n",
    "            aug_img = self.base_transform(T.from_numpy(aug_img).permute(2, 0, 1).float()).to(device)\n",
    "            aug_mask = T.from_numpy(aug_mask).long().to(device)\n",
    "            \n",
    "            return {\n",
    "                'original_img': original_img,\n",
    "                'original_mask': original_mask,\n",
    "                'augmented_img': aug_img,\n",
    "                'augmented_mask': aug_mask\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'original_img': original_img,\n",
    "                'original_mask': original_mask\n",
    "            }\n",
    "\n",
    "# ---------------------- Datasets & DataLoaders -----------------------\n",
    "train_dataset = SegmentationDataset(\n",
    "    train_images, \n",
    "    train_masks, \n",
    "    train_transform=train_transform,\n",
    "    base_transform=base_transform,\n",
    "    dataset_type=\"Training\"\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    validation_images,\n",
    "    validation_masks,\n",
    "    train_transform=train_transform,\n",
    "    base_transform=base_transform,\n",
    "    dataset_type=\"Validation\"\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_images,\n",
    "    test_masks,\n",
    "    train_transform=train_transform,\n",
    "    base_transform=base_transform,\n",
    "    dataset_type=\"Testing\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295ec20-319a-4744-a78d-59878f76264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# ====================== CONFIGURATION ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CUSTOM_SAVE_ROOT = Path(r\"D:\\AAU Internship\\Code\\UNet-Models\")\n",
    "os.makedirs(CUSTOM_SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "# ====================== MODEL DEFINITION ======================\n",
    "model = smp.Unet(\n",
    "    encoder=\"efficientnet-b7\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    encoder_depth=4,\n",
    "    decoder_use_batchnorm='inplace',\n",
    "    decoder_attention_type='scse',\n",
    "    decoder_channels=[256, 128, 64, 32],\n",
    "    in_channels=3,\n",
    "    classes=2,\n",
    "    activation=\"softmax\",\n",
    "    center=True,\n",
    ").to(device)\n",
    "\n",
    "# ====================== LOSS FUNCTION ======================\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3, gamma=0.75, smooth=1e-6):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def update_hyperparams_by_epoch(self, epoch):\n",
    "        steps = epoch // 5\n",
    "        self.alpha = max(0.4, 0.7 - 0.03*steps)\n",
    "        self.beta = 1 - self.alpha\n",
    "        self.gamma = min(1.5, 0.5 + 0.1*steps)\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=preds.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        probs = preds\n",
    "        dims = (0, 2, 3)\n",
    "        TP = torch.sum(probs * targets_one_hot, dims)\n",
    "        FP = torch.sum(probs * (1 - targets_one_hot), dims)\n",
    "        FN = torch.sum((1 - probs) * targets_one_hot, dims)\n",
    "        Tversky = (TP + self.smooth) / (TP + self.alpha * FP + self.beta * FN + self.smooth)\n",
    "        return torch.mean((1 - Tversky) ** self.gamma)\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_fn = FocalTverskyLoss().to(device)\n",
    "\n",
    "# ====================== EVALUATION METRICS ======================\n",
    "def compute_metrics(preds, targets):\n",
    "    with torch.no_grad():\n",
    "        pred_labels = torch.argmax(preds, dim=1).cpu().numpy().flatten()\n",
    "        targets = targets.cpu().numpy().flatten()\n",
    "        \n",
    "        # Calculate IoUs\n",
    "        ious = []\n",
    "        for cls in [0, 1]:\n",
    "            intersection = ((pred_labels == cls) & (targets == cls)).sum()\n",
    "            union = ((pred_labels == cls) | (targets == cls)).sum()\n",
    "            ious.append(intersection / (union + 1e-6))\n",
    "        \n",
    "        # Calculate mPA\n",
    "        class_acc = []\n",
    "        for cls in [0, 1]:\n",
    "            mask = (targets == cls)\n",
    "            if mask.sum() > 0:\n",
    "                class_acc.append((pred_labels[mask] == cls).mean())\n",
    "        mPA = np.mean(class_acc) * 100\n",
    "        \n",
    "        # Other metrics\n",
    "        cm = confusion_matrix(targets, pred_labels)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "        \n",
    "        return {\n",
    "            \"Accuracy\": 100 * accuracy_score(targets, pred_labels),\n",
    "            \"mPA\": mPA,\n",
    "            \"Crop IoU\": 100 * ious[1],\n",
    "            \"mIoU\": 100 * np.mean(ious),\n",
    "            \"Precision\": 100 * precision_score(targets, pred_labels, zero_division=0),\n",
    "            \"Recall\": 100 * recall_score(targets, pred_labels, zero_division=0),\n",
    "            \"F1-Score\": 100 * f1_score(targets, pred_labels, zero_division=0),\n",
    "            \"FNR\": 100 * (FN / (FN + TP + 1e-6))\n",
    "        }\n",
    "\n",
    "# ====================== TRAINING SETUP ======================\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Define custom model save paths\n",
    "MODEL_PATHS = {\n",
    "    \"mPA\": CUSTOM_SAVE_ROOT / \"best_mPA_model.pth\",\n",
    "    \"mIoU\": CUSTOM_SAVE_ROOT / \"best_mIoU_model.pth\",\n",
    "    \"Crop IoU\": CUSTOM_SAVE_ROOT / \"best_CropIoU_model.pth\",\n",
    "    \"Accuracy\": CUSTOM_SAVE_ROOT / \"best_Accuracy_model.pth\",\n",
    "    \"F1-Score\": CUSTOM_SAVE_ROOT / \"best_F1_model.pth\",\n",
    "    \"Precision\": CUSTOM_SAVE_ROOT / \"best_Precision_model.pth\",\n",
    "    \"Recall\": CUSTOM_SAVE_ROOT / \"best_Recall_model.pth\",\n",
    "    \"FNR\": CUSTOM_SAVE_ROOT / \"best_FNR_model.pth\"\n",
    "}\n",
    "\n",
    "# Initialize best metrics tracking\n",
    "best_metrics = {\n",
    "    \"mPA\": {\"value\": -1, \"path\": MODEL_PATHS[\"mPA\"]},\n",
    "    \"mIoU\": {\"value\": -1, \"path\": MODEL_PATHS[\"mIoU\"]},\n",
    "    \"Crop IoU\": {\"value\": -1, \"path\": MODEL_PATHS[\"Crop IoU\"]},\n",
    "    \"Accuracy\": {\"value\": -1, \"path\": MODEL_PATHS[\"Accuracy\"]},\n",
    "    \"F1-Score\": {\"value\": -1, \"path\": MODEL_PATHS[\"F1-Score\"]},\n",
    "    \"Precision\": {\"value\": -1, \"path\": MODEL_PATHS[\"Precision\"]},\n",
    "    \"Recall\": {\"value\": -1, \"path\": MODEL_PATHS[\"Recall\"]},\n",
    "    \"FNR\": {\"value\": float('inf'), \"path\": MODEL_PATHS[\"FNR\"]}\n",
    "}\n",
    "\n",
    "# ====================== TRAINING LOOPS ======================\n",
    "def TrainUNet(model, dataloader, loss_fn, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    loss_fn.update_hyperparams_by_epoch(epoch)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['augmented_img'].to(device)\n",
    "        targets = batch['augmented_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_preds.append(outputs.detach())\n",
    "        all_targets.append(targets.detach())\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    metrics = compute_metrics(all_preds, all_targets)\n",
    "    return avg_loss, metrics\n",
    "\n",
    "def ValidateUNet(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['original_img'].to(device)\n",
    "            targets = batch['original_mask'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            all_preds.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    metrics = compute_metrics(all_preds, all_targets)\n",
    "    return avg_loss, metrics\n",
    "\n",
    "# ====================== MAIN TRAINING ======================\n",
    "num_epochs = 50\n",
    "\n",
    "# Make sure these dataloaders are defined before this point\n",
    "# train_dataloader = ...\n",
    "# val_loader = ...\n",
    "# test_loader = ...\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_metrics = TrainUNet(model, train_dataloader, loss_fn, optimizer, epoch)\n",
    "    val_loss, val_metrics = ValidateUNet(model, val_loader, loss_fn)\n",
    "    \n",
    "    # Update best models\n",
    "    for metric_name in best_metrics.keys():\n",
    "        current_value = val_metrics[metric_name]\n",
    "        is_better = (current_value > best_metrics[metric_name][\"value\"]) if metric_name != \"FNR\" else (current_value < best_metrics[metric_name][\"value\"])\n",
    "        \n",
    "        if is_better:\n",
    "            best_metrics[metric_name][\"value\"] = current_value\n",
    "            torch.save(model.state_dict(), str(best_metrics[metric_name][\"path\"]))\n",
    "            print(f\"New best {metric_name}: {current_value:.2f}% | Saved to: {best_metrics[metric_name]['path']}\")\n",
    "\n",
    "    # Epoch summary\n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    for k, v in val_metrics.items():\n",
    "        print(f\"{k}: {v:.2f}%\")\n",
    "\n",
    "# ====================== FINAL REPORT ======================\n",
    "print(\"\\n=== Best Models Saved ===\")\n",
    "for metric_name, data in best_metrics.items():\n",
    "    print(f\"{metric_name}: {data['value']:.2f}%\")\n",
    "    print(f\"Location: {data['path']}\\n\")\n",
    "\n",
    "# ====================== TESTING ======================\n",
    "print(\"\\n=== Testing Best Models ===\")\n",
    "for metric_name, data in best_metrics.items():\n",
    "    model.load_state_dict(torch.load(str(data[\"path\"])))\n",
    "    test_loss, test_metrics = ValidateUNet(model, test_loader, loss_fn)\n",
    "    print(f\"\\n{metric_name} Model Test Results:\")\n",
    "    for k, v in test_metrics.items():\n",
    "        print(f\"{k}: {v:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34eee5-3d9d-4e60-b4b4-09bdc2f69c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
