{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b263a116-ec67-45df-ab0f-db7f00974397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c942ca-872a-48d2-95cb-f8f9a042abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\"\n",
    "DEVICE = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4478d637-ac91-4999-865a-debf790dc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropWeedDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.image_paths = glob(os.path.join(folder_path, \"*.jpg\"))\n",
    "        self.image_paths += glob(os.path.join(folder_path, \"*.png\"))\n",
    "\n",
    "        def natural_key(string):\n",
    "            return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string)]\n",
    "\n",
    "        self.image_paths.sort(key=natural_key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img_tensor = T.from_numpy(img).permute(2, 0, 1).contiguous()\n",
    "\n",
    "        return img_tensor.to(DEVICE), img_path\n",
    "\n",
    "def get_loader(folder_path, batch_size=4):\n",
    "    dataset = CropWeedDataset(folder_path)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return loader\n",
    "\n",
    "train_path = os.path.join(BASE_PATH, \"train_new\")\n",
    "val_path   = os.path.join(BASE_PATH, \"validation_new\")\n",
    "test_path  = os.path.join(BASE_PATH, \"test_new\")\n",
    "train_loader = get_loader(train_path)\n",
    "val_loader   = get_loader(val_path)\n",
    "test_loader  = get_loader(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421d5508-24cf-4237-88bf-fab96a79c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fused_contours(batch_imgs):\n",
    "    T.cuda.empty_cache()\n",
    "    fused_maps = []\n",
    "    batch_imgs_np = batch_imgs.detach().cpu().numpy()\n",
    "\n",
    "    for img in batch_imgs_np:\n",
    "        img_rgb = np.transpose(img, (1, 2, 0)) * 255.0\n",
    "        img_rgb = img_rgb.astype(np.uint8)\n",
    "        gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Sobel\n",
    "        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        sobel = cv2.magnitude(sobelx, sobely)\n",
    "        sobel = np.clip(sobel / sobel.max(), 0, 1)\n",
    "\n",
    "        # Canny\n",
    "        canny = cv2.Canny(gray, 100, 200) / 255.0\n",
    "\n",
    "        # Fuse Sobel + Canny\n",
    "        fused = np.maximum(sobel, canny)\n",
    "\n",
    "        fused_tensor = T.tensor(fused, dtype=T.float32).unsqueeze(0)\n",
    "        fused_maps.append(fused_tensor)\n",
    "\n",
    "    fused_batch = T.stack(fused_maps).to(batch_imgs.device)\n",
    "    T.cuda.empty_cache()\n",
    "    return fused_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d470d62-5ad3-49b1-b78b-cd89454eefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRNetW32FeatureExtractor(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Load pretrained HRNet-W32 from timm\n",
    "        self.backbone = timm.create_model(\n",
    "            \"hrnet_w32\", \n",
    "            pretrained=True, \n",
    "            features_only=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # HRNet returns multi-scale outputs (C1, C2, C3, C4)\n",
    "        # We'll use the highest-resolution one: index 0\n",
    "        self.selected_feature_idx = 0\n",
    "\n",
    "        # Freeze the backbone\n",
    "        self.backbone.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        with T.no_grad():\n",
    "            features = self.backbone(x)  # List of [B, C, H', W']\n",
    "            feature_map = features[self.selected_feature_idx]  # [B, 32, 96, 128]\n",
    "\n",
    "            # Upsample to match original input size (384Ã—512)\n",
    "            feature_map_upsampled = F.interpolate(\n",
    "                feature_map,\n",
    "                size=(384, 512),\n",
    "                mode='bicubic',  # Better edge preservation\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            return feature_map_upsampled  # [B, 32, 384, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b8f183-0e30-4888-8726-690ac18732ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContourFeatureFusion(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_fusion = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, cnn_features, edge_maps):\n",
    "        # cnn_features: [B, C1, H, W]\n",
    "        # edge_maps:    [B, 1, H, W]\n",
    "        x = T.cat([cnn_features, edge_maps], dim=1)  # [B, C1+1, H, W]\n",
    "        fused = self.conv_fusion(x)\n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05c3d4-1019-409f-bd44-25d579a5869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_kmeans(fused_features, n_clusters=2):\n",
    "    masks = []\n",
    "    fused_np = fused_features.detach().cpu().numpy()\n",
    "\n",
    "    for feat in tqdm(fused_np, desc=\"ðŸ§  Running K-Means on batch\"):\n",
    "        C, H, W = feat.shape\n",
    "        flat_feat = feat.reshape(C, -1).T\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "        labels = kmeans.fit_predict(flat_feat)\n",
    "\n",
    "        mask = labels.reshape(H, W)\n",
    "        masks.append(mask.astype(np.uint8))\n",
    "\n",
    "    return masks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
