{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1870a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "from skimage.filters import threshold_multiotsu, threshold_otsu\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce338520-9012-4bd1-9648-20b57e266a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# üìÅ Set your base image directory\n",
    "BASE_PATH = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\"\n",
    "DEVICE = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {DEVICE}\")\n",
    "\n",
    "# üì¶ Dataset class\n",
    "class VegetationDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.image_paths = sorted(\n",
    "            glob(os.path.join(folder_path, \"*.jpg\")) + glob(os.path.join(folder_path, \"*.png\")),\n",
    "            key=lambda x: [int(t) if t.isdigit() else t for t in re.split(r'(\\d+)', x)]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path) \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "        img = cv2.resize(img, (512, 384), interpolation=cv2.INTER_AREA)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img_tensor = T.from_numpy(img).permute(2, 0, 1)\n",
    "\n",
    "        return img_tensor.to(DEVICE), img_path\n",
    "\n",
    "def get_loader(folder_path, batch_size=4):\n",
    "    dataset = VegetationDataset(folder_path)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return loader\n",
    "\n",
    "train_path = os.path.join(BASE_PATH, \"train\")\n",
    "val_path   = os.path.join(BASE_PATH, \"validation\")\n",
    "test_path  = os.path.join(BASE_PATH, \"test\")\n",
    "train_loader = get_loader(train_path)\n",
    "val_loader   = get_loader(val_path)\n",
    "test_loader  = get_loader(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0dbb6c-19c3-4a54-92ee-de2535a59b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vegetation_indices(batch_imgs):\n",
    "    \"\"\"\n",
    "    Compute 6 vegetation indices from a batch of RGB images.\n",
    "    Input:  batch_imgs [B, 3, H, W], float32 in range [0, 1]\n",
    "    Output: indices_tensor [B, 6, H, W]\n",
    "    \"\"\"\n",
    "    def normalize_index(idx):\n",
    "        idx_min = idx.min(dim=-1, keepdim=True)[0].min(dim=-2, keepdim=True)[0]\n",
    "        idx_max = idx.max(dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)[0]\n",
    "        \n",
    "        return (idx - idx_min) / (idx_max - idx_min + 1e-6)\n",
    "\n",
    "    def enhance_contrast_clahe(fused_tensor):\n",
    "        enhanced = []\n",
    "        fused_np = fused_tensor.squeeze(1).cpu().numpy()  # [B, H, W]\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    \n",
    "        for img in fused_np:\n",
    "            img_uint8 = np.uint8(img * 255)\n",
    "            img_eq = clahe.apply(img_uint8)\n",
    "            enhanced.append(img_eq / 255.0)\n",
    "            \n",
    "    R = batch_imgs[:, 0, :, :]\n",
    "    G = batch_imgs[:, 1, :, :]\n",
    "    B = batch_imgs[:, 2, :, :]\n",
    "\n",
    "    eps = 1e-6  # to prevent division by zero\n",
    "\n",
    "    # 1. ExG = 2G - R - B\n",
    "    ExG = 2 * G - R - B\n",
    "\n",
    "    # 2. ExR = 1.4R - G\n",
    "    ExR = 1.4 * R - G\n",
    "\n",
    "    # 3. CIVE = 0.441R - 0.811G + 0.385B + 18.787\n",
    "    CIVE = 0.441 * R - 0.811 * G + 0.385 * B + 18.787\n",
    "\n",
    "    # 4. VEG = G / (R^0.667 * B^0.333 + eps)\n",
    "    VEG = G / ((R**0.667) * (B**0.333) + eps)\n",
    "\n",
    "    # 5. NDI = (G - R) / (G + R + eps)\n",
    "    NDI = (G - R) / (G + R + eps)\n",
    "\n",
    "    # 6. GLI = (2G - R - B) / (2G + R + B + eps)\n",
    "    GLI = (2 * G - R - B) / (2 * G + R + B + eps)\n",
    "\n",
    "    # 7. AGRI = (G - B) / (G + B + eps)\n",
    "    AGRI = (G - B) / (G + B + eps)\n",
    "\n",
    "    # 8. VARI = (G - R) / (G + R - B + eps)\n",
    "    VARI = (G - R) / (G + R - B + eps)\n",
    "    \n",
    "    # 9. MVI = (2G - R - B) / (2G + R + B + eps) - CIVE\n",
    "    MVI = ((2 * G - R - B) / (2 * G + R + B + eps)) - CIVE\n",
    "\n",
    "    #10 BGI = (G - B) / (G + B + eps)\n",
    "    BGI = (G - B) / (G + B + eps)\n",
    "\n",
    "    #11 CIg = G / (R + eps)\n",
    "    CIg = G / (R + eps)\n",
    "\n",
    "    ExG  = normalize_index(ExG)\n",
    "    ExR  = normalize_index(ExR)\n",
    "    CIVE = normalize_index(CIVE)\n",
    "    VEG  = normalize_index(VEG)\n",
    "    NDI  = normalize_index(NDI)\n",
    "    GLI  = normalize_index(GLI)\n",
    "    AGRI = normalize_index(AGRI)\n",
    "    VARI = normalize_index(VARI)\n",
    "    MVI  = normalize_index(MVI)\n",
    "    BGI  = normalize_index(BGI)\n",
    "    CIg  = normalize_index(CIg)\n",
    "\n",
    "    fused = (\n",
    "        + 0.9 * GLI     # good for thin & yellowish-green vegetation\n",
    "        + 0.7 * VARI    # excellent under variable lighting\n",
    "        + 0.6 * NDI     # green-red contrast\n",
    "        + 0.4 * CIVE    # bright/contrast patches (limited)\n",
    "        + 0.4 * VEG     # strong for crops\n",
    "        + 0.3 * AGRI    # soil separation\n",
    "        + 0.2 * CIg     # helps crops & partial green\n",
    "        - 0.6 * ExR     # suppress reddish areas\n",
    "        - 0.5 * MVI     # suppress flat/dark patches\n",
    "        - 0.4 * BGI     # suppress bluish/purple\n",
    "        - 0.3 * R       # pipe suppressor\n",
    "    )\n",
    "\n",
    "    \n",
    "    return fused.unsqueeze(1)  # [B, 1, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd61f9c-4719-4d39-803c-06e777605169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef fit_pca_on_sample(indices_tensor, n_components=1, sample_size=700):\\n    \"\"\"\\n    indices_tensor: [B, 6, H, W]\\n    \"\"\"\\n    B, C, H, W = indices_tensor.shape\\n    reshaped = indices_tensor.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H*W, 6]\\n\\n    # Sample randomly to limit fitting time\\n    sample = reshaped[T.randperm(reshaped.shape[0])[:sample_size * H]]\\n    pca = PCA(n_components=n_components)\\n    pca.fit(sample.cpu().numpy())\\n\\n    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\\n\\n    return pca\\n\\n# 2Ô∏è‚É£ Apply PCA to full batch\\ndef apply_pca_to_batch(indices_tensor, pca):\\n    \"\"\"\\n    indices_tensor: [B, 6, H, W]\\n    Returns: [B, 1, H, W] (PCA projected grayscale)\\n    \"\"\"\\n    B, C, H, W = indices_tensor.shape\\n    reshaped = indices_tensor.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H*W, 6]\\n\\n    projected = pca.transform(reshaped.cpu().numpy())  # [B*H*W, 1]\\n    out = T.tensor(projected, dtype=T.float32).reshape(B, H, W)\\n    return out.unsqueeze(1).to(indices_tensor.device)  # [B, 1, H, W]\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def fit_pca_on_sample(indices_tensor, n_components=1, sample_size=700):\n",
    "    \"\"\"\n",
    "    indices_tensor: [B, 6, H, W]\n",
    "    \"\"\"\n",
    "    B, C, H, W = indices_tensor.shape\n",
    "    reshaped = indices_tensor.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H*W, 6]\n",
    "    \n",
    "    # Sample randomly to limit fitting time\n",
    "    sample = reshaped[T.randperm(reshaped.shape[0])[:sample_size * H]]\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(sample.cpu().numpy())\n",
    "    \n",
    "    print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "    \n",
    "    return pca\n",
    "\n",
    "# 2Ô∏è‚É£ Apply PCA to full batch\n",
    "def apply_pca_to_batch(indices_tensor, pca):\n",
    "    \"\"\"\n",
    "    indices_tensor: [B, 6, H, W]\n",
    "    Returns: [B, 1, H, W] (PCA projected grayscale)\n",
    "    \"\"\"\n",
    "    B, C, H, W = indices_tensor.shape\n",
    "    reshaped = indices_tensor.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H*W, 6]\n",
    "\n",
    "    projected = pca.transform(reshaped.cpu().numpy())  # [B*H*W, 1]\n",
    "    out = T.tensor(projected, dtype=T.float32).reshape(B, H, W)\n",
    "    return out.unsqueeze(1).to(indices_tensor.device)  # [B, 1, H, W]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1b630d-bd8a-479f-88b9-e22e698932ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pca_tensor(pca_tensor):\n",
    "    # [B, 1, H, W] ‚Üí normalize each image to [0, 1]\n",
    "    B = pca_tensor.shape[0]\n",
    "    normed = []\n",
    "\n",
    "    for i in range(B):\n",
    "        img = pca_tensor[i]\n",
    "        img_min = img.min()\n",
    "        img_max = img.max()\n",
    "        if (img_max - img_min) < 1e-5 or T.isnan(img_max - img_min):\n",
    "            # Skip normalization if image is nearly constant\n",
    "            normed_img = T.zeros_like(img)\n",
    "        else:\n",
    "            normed_img = (img - img_min) / (img_max - img_min)\n",
    "\n",
    "        normed.append(normed_img)\n",
    "\n",
    "    return T.stack(normed, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056baae3-2714-40bb-ad3d-f2c2829d1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholding(pca_tensor):\n",
    "    masks = []\n",
    "    pca_np = pca_tensor.squeeze(1).cpu().numpy()\n",
    "\n",
    "    for img in pca_np:\n",
    "        try:\n",
    "            thresholds = threshold_multiotsu(img, classes=2)\n",
    "            mask = (img > thresholds[0]).astype(np.uint8)\n",
    "        except:\n",
    "            # fallback in case of flat image\n",
    "            try:\n",
    "                thresh = threshold_otsu(img)\n",
    "                mask = (img > thresh).astype(np.uint8)\n",
    "            except:\n",
    "                mask = np.zeros_like(img, dtype=np.uint8)\n",
    "\n",
    "        masks.append(mask)\n",
    "\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b865fec1-1485-4260-9c9e-0b0ed8a114dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_masks_with_morphops(mask_list, apply_opening=True, apply_closing=True, dilate=False):\n",
    "    \"\"\"\n",
    "    Refine a list of binary masks using Morphological Operations.\n",
    "    Input: list of [H, W] uint8 masks (0 and 1)\n",
    "    Output: list of refined [H, W] masks\n",
    "    \"\"\"\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    refined = []\n",
    "\n",
    "    for mask in mask_list:\n",
    "        mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "\n",
    "        if apply_opening:\n",
    "            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        if apply_closing:\n",
    "            mask_uint8 = cv2.morphologyEx(mask_uint8, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        if dilate:\n",
    "            mask_uint8 = cv2.dilate(mask_uint8, kernel, iterations=1)\n",
    "\n",
    "        refined.append(mask_uint8 // 255)  # Normalize back to 0 or 1\n",
    "\n",
    "    return refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94656738-b32c-4b04-8200-01f1a62d1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vegetation_masks(masks, paths, save_root=\"Vegetation_Masks\"):\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "    for mask, img_path in zip(masks, paths):\n",
    "        # Extract base filename\n",
    "        base_filename = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n",
    "\n",
    "        # Determine subfolder name from input path\n",
    "        if \"train\" in img_path:\n",
    "            subfolder = \"train\"\n",
    "        elif \"validation\" in img_path:\n",
    "            subfolder = \"val\"\n",
    "        elif \"test\" in img_path:\n",
    "            subfolder = \"test\"\n",
    "        else:\n",
    "            raise ValueError(f\"‚ùå Cannot determine subfolder for: {img_path}\")\n",
    "\n",
    "        # Create subfolder path\n",
    "        save_dir = os.path.join(save_root, subfolder)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Convert mask to 0-255 grayscale\n",
    "        grayscale_mask = (mask * 255).astype(np.uint8)\n",
    "\n",
    "        # Save the mask\n",
    "        save_path = os.path.join(save_dir, base_filename)\n",
    "        cv2.imwrite(save_path, grayscale_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7547c646-68e8-445f-8808-8f8447768d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_image(img, mask, index):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f'PCA [{index}]')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title(f'Mask [{index}]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541a3e4a-e66d-4b1c-a784-dedf113c7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vegetation_masking_pipeline(dataloader, set_name):\n",
    "    \"\"\"\n",
    "    set_name: one of [\"train\", \"val\", \"test\"]\n",
    "    Returns:\n",
    "        total_images (int), total_time (float in seconds)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Starting vegetation masking for {set_name} set...\")\n",
    "    total_images = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_imgs, batch_paths in tqdm(dataloader, desc=f\"üå± Processing {set_name}\"):\n",
    "        total_images += len(batch_imgs)\n",
    "\n",
    "        # Step 1: Compute vegetation indices\n",
    "        veg_fused = compute_vegetation_indices(batch_imgs)  # [B, 1, H, W]\n",
    "        \n",
    "        # Step 2: No PCA ‚Äî normalize the fused tensor\n",
    "        veg_fused = normalize_pca_tensor(veg_fused)\n",
    "        \n",
    "        # Optional debug\n",
    "#        for i in range(min(3, veg_fused.shape[0])):\n",
    "#            print(f\"üß™ Fused range [{i}]: min={veg_fused[i].min().item():.4f}, max={veg_fused[i].max().item():.4f}\")\n",
    "        \n",
    "        # Step 3: Threshold\n",
    "        binary_masks = apply_thresholding(veg_fused)\n",
    "        \n",
    "        # Step 4: Morph ops\n",
    "#        refined_masks = refine_masks_with_morphops(binary_masks)\n",
    "#        for i, mask in enumerate(refined_masks):\n",
    "#            if np.sum(mask) == 0:\n",
    "#                print(f\"‚ö†Ô∏è Empty mask after morphOps at index {i}\")\n",
    "\n",
    "        # Step 6: Save\n",
    "        save_vegetation_masks(binary_masks, batch_paths)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    ms_per_image = (total_time / total_images) * 1000\n",
    "\n",
    "    print(f\"‚úÖ Completed {total_images} images in {total_time:.2f} sec\")\n",
    "    print(f\"‚ö° Avg processing time: {ms_per_image:.2f} ms/image\\n\")\n",
    "\n",
    "    return total_images, total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c10243e-78a0-4093-aa3f-cda909ca3bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting vegetation masking for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac77d31e3cd34ce0a287f6ac5bc0f7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üå± Processing train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed 400 images in 7.26 sec\n",
      "‚ö° Avg processing time: 18.16 ms/image\n",
      "\n",
      "\n",
      "üöÄ Starting vegetation masking for val set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c81ba885d234b38b2a6771c4fc1ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üå± Processing val:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed 88 images in 3.55 sec\n",
      "‚ö° Avg processing time: 40.29 ms/image\n",
      "\n",
      "\n",
      "üöÄ Starting vegetation masking for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9515c66a7042d68f399dbab79ad752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üå± Processing test:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed 300 images in 7.56 sec\n",
      "‚ö° Avg processing time: 25.20 ms/image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate total timing across all splits\n",
    "total_imgs_all = 0\n",
    "total_time_all = 0\n",
    "\n",
    "for loader, name in zip([train_loader, val_loader, test_loader], [\"train\", \"val\", \"test\"]):\n",
    "    imgs, time_taken = run_vegetation_masking_pipeline(loader, name)\n",
    "    total_imgs_all += imgs\n",
    "    total_time_all += time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b32138-f9f5-4b95-918b-104f17d35dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Total: 788 images processed in 18.37 seconds\n",
      "üöÄ Overall Speed: 23.31 ms/image\n"
     ]
    }
   ],
   "source": [
    "# Final performance summary\n",
    "overall_ms_per_image = (total_time_all / total_imgs_all) * 1000\n",
    "print(f\"üß† Total: {total_imgs_all} images processed in {total_time_all:.2f} seconds\")\n",
    "print(f\"üöÄ Overall Speed: {overall_ms_per_image:.2f} ms/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ce49d-cbea-42a6-9a9c-a8c105c8c13c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
