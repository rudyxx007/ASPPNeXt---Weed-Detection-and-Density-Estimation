{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5262a4-3066-4b54-b6f4-ab733ed9233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ab70a1c-1825-4e74-81b4-23fceb8604a0",
   "metadata": {},
   "source": [
    "--------------------xX ASPPNeXt Architecture Xx--------------------\n",
    "\n",
    "             RGB Input          Depth Input\n",
    "                │                   │\n",
    "       ┌────────▼────────┐ ┌────────▼────────┐\n",
    "       │ Hybrid ConvNeXt │ │ Hybrid ConvNeXt │\n",
    "       │ Block ×4 (with  │ │ Block ×4 (with  │\n",
    "       │  VWA + GhostMLP)│ │  VWA + GhostMLP)│\n",
    "       └────────┬────────┘ └────────┬────────┘\n",
    "                │                   │\n",
    "       ┌────────▼────────┐ ┌────────▼────────┐\n",
    "       │ Pre-DAAF        │ │ Pre-DAAF        │\n",
    "       │ GhostModule     │ │ GhostModule     │\n",
    "       └────────┬────────┘ └────────┬────────┘\n",
    "                │                   │\n",
    "                └──────► DAAF ◄─────┘\n",
    "                          │\n",
    "                ┌─────────▼─────────┐\n",
    "                │ Post-DAAF         │\n",
    "                │ GhostModule       │\n",
    "                └─────────┬─────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 1         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E4)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 2         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E3)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 3         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E2)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "                   Final 1×1 Conv\n",
    "                          ↓\n",
    "                     Output Map\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c133a51c-fd32-48e9-afa1-8098e6d1e257",
   "metadata": {},
   "source": [
    "--------------------xX GhostModule Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  ↓\n",
    "Primary Features: \n",
    "  - 1×1 Conv → C_mid = C_out // ratio\n",
    "  ↓\n",
    "Ghost Features:\n",
    "  - Depthwise Conv (or cheap linear ops) on C_mid\n",
    "  ↓\n",
    "Concatenate Primary + Ghost → C_out\n",
    "  ↓\n",
    "BatchNorm (optional)\n",
    "  ↓\n",
    "Output: (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9544a6f3-e3fd-4213-bf8e-4f1c86f66f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFCAttention and GhostModuleV2 as you defined them:\n",
    "class DFCAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dfc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1, bias=False),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x * self.dfc(x)\n",
    "\n",
    "class GhostModuleV2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 ratio=2, kernel_size=1, dw_size=3,\n",
    "                 stride=1, use_attn=True):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        init_ch = math.ceil(out_channels / ratio)\n",
    "        new_ch  = init_ch * (ratio - 1)\n",
    "\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, init_ch,\n",
    "                      kernel_size, stride, kernel_size//2, bias=False),\n",
    "            nn.BatchNorm2d(init_ch),\n",
    "            nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_ch, new_ch, dw_size, 1,\n",
    "                      dw_size//2, groups=init_ch, bias=False),\n",
    "            nn.BatchNorm2d(new_ch),\n",
    "            nn.Hardswish(inplace=True),\n",
    "        )\n",
    "        self.use_attn = use_attn\n",
    "        if use_attn:\n",
    "            self.attn = DFCAttention(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        out = torch.cat([x1, x2], dim=1)[:, :self.out_channels]\n",
    "        if self.use_attn:\n",
    "            out = self.attn(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c0e33b-905a-4c8c-a202-e860c4ae83a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--------------------xX Hybrid ConvNeXt Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "7×7 DepthwiseConv\n",
    "  ↓\n",
    "Permute → ℝ^(B×H×W×C)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Varying Window Attention:\n",
    "  • Query window: P×P fixed\n",
    "  • Context window: (R×P)×(R×P)\n",
    "  • MHSA with zero extra cost\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Ghost MLP:\n",
    "  • GhostConv(1×1): C→4C → GELU\n",
    "  • GhostConv(1×1): 4C→C\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "Permute back → ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632c4c0f-bf1b-4412-ae5d-1f4f0c5907a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaryingWindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, context_ratio=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.context_ratio = context_ratio\n",
    "        \n",
    "        # Projections for Q/K/V\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.to_out = nn.Linear(dim, dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        P = self.window_size\n",
    "        R = self.context_ratio\n",
    "        \n",
    "        # Reshape to (B, H, W, C)\n",
    "        x_spatial = x.view(B, H, W, C)\n",
    "        \n",
    "        # 1. Create P×P query windows\n",
    "        q_windows = x_spatial.unfold(1, P, P).unfold(2, P, P)\n",
    "        q_windows = q_windows.contiguous().view(B, -1, P*P, C)\n",
    "        \n",
    "        # 2. Create (R*P)×(R*P) context windows\n",
    "        pad = (R * P - P) // 2\n",
    "        x_pad = F.pad(x_spatial, (0, 0, pad, pad, pad, pad))\n",
    "        ctx_windows = x_pad.unfold(1, R*P, P).unfold(2, R*P, P)\n",
    "        ctx_windows = ctx_windows.contiguous().view(B, -1, (R*P)**2, C)\n",
    "        \n",
    "        # 3. Concatenate queries and context\n",
    "        seq = T.cat([q_windows, ctx_windows], dim=2)\n",
    "        \n",
    "        # 4. Compute Q, K, V\n",
    "        qkv = self.to_qkv(seq)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Split into q, k, v\n",
    "        \n",
    "        # 5. Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * (C ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # 6. Keep only query window outputs\n",
    "        out = out[:, :, :P*P, :]\n",
    "        \n",
    "        # 7. Reconstruct spatial layout\n",
    "        out = out.view(B, H//P, W//P, P, P, C)\n",
    "        out = out.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, C)\n",
    "        out = out.view(B, N, C)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class GhostMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Ghost MLP with two GhostModuleV2 layers and GELU activation.\n",
    "    Expands C→4C then projects back 4C→C.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * mlp_ratio\n",
    "        self.fc1 = GhostModuleV2(dim, hidden_dim, use_attn=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = GhostModuleV2(hidden_dim, dim, use_attn=False)\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, N, C)\n",
    "        returns: (B, N, C)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        # reshape to (B, C, H, W) for GhostModuleV2, then back\n",
    "        H = W = int(N ** 0.5)\n",
    "        x_spatial = x.view(B, H, W, C).permute(0,3,1,2)\n",
    "        x_spatial = self.act(self.fc1(x_spatial))\n",
    "        x_spatial = self.fc2(x_spatial)\n",
    "        x_flat = x_spatial.permute(0,2,3,1).view(B, N, C)\n",
    "        return x_flat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3b707c-efb0-47c6-85f2-f7818a31dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size,\n",
    "                 context_ratio=2, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 7, padding=3,\n",
    "                                groups=dim, bias=False)\n",
    "        self.norm1  = nn.LayerNorm(dim)\n",
    "        self.attn   = VaryingWindowAttention(dim, num_heads,\n",
    "                                             window_size, context_ratio)\n",
    "        self.norm2  = nn.LayerNorm(dim)\n",
    "        self.mlp    = GhostMLP(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        B,C,H,W = x.shape\n",
    "        seq = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        seq = seq + self.attn(self.norm1(seq))\n",
    "        seq = seq + self.mlp(self.norm2(seq))\n",
    "        return seq.view(B, H, W, C).permute(0,3,1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2a6e33-ef46-4fef-b486-631a450f042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, base_dim=64,\n",
    "                 num_heads=(4,8,16,32),\n",
    "                 window_sizes=(8,4,2,1),\n",
    "                 mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        # Stage 1 patch‐embedding (no further downsampling here)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, base_dim, 4, stride=4, bias=False),\n",
    "            nn.LayerNorm([base_dim, None, None])\n",
    "        )\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            dim = base_dim * (2**i)\n",
    "            # downsample before stages 2–4 (i=1,2,3)\n",
    "            down = None if i == 0 else nn.Sequential(\n",
    "                GhostModuleV2(in_channels=dim//2,\n",
    "                              out_channels=dim,\n",
    "                              ratio=2,\n",
    "                              kernel_size=2,\n",
    "                              stride=2,\n",
    "                              use_attn=True),\n",
    "                nn.LayerNorm([dim, None, None])\n",
    "            )\n",
    "            block = HybridConvNeXtBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_sizes[i],\n",
    "                context_ratio=2,\n",
    "                mlp_ratio=mlp_ratio\n",
    "            )\n",
    "            self.stages.append(nn.ModuleDict({'down': down, 'block': block}))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, in_ch, H, W)\n",
    "        x = self.stem(x)            # Stage 1 input → (B, base_dim, H/4, W/4)\n",
    "        skips = []\n",
    "        for stage in self.stages:\n",
    "            if stage['down'] is not None:\n",
    "                x = stage['down'](x) # downsample before stages 2–4\n",
    "            x = stage['block'](x)   # Hybrid block\n",
    "            skips.append(x)         # capture skip for decoder\n",
    "        # skips[0] = stage 1 output (no downsample)\n",
    "        # skips[1] = stage 2 output (1/8 input res)\n",
    "        # skips[2] = stage 3 output (1/16 input res)\n",
    "        # skips[3] = stage 4 output (1/32 input res)\n",
    "        return x, skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc73c7-de29-46bb-9c89-f6eaf2268b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure DFCAttention and GhostModuleV2 are defined in your notebook\n",
    "# from ghost_module_v2 import GhostModuleV2\n",
    "\n",
    "class PreDAAFGhostV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-DAAF bottleneck using GhostModuleV2 with DFC attention (V2).\n",
    "    Applies:\n",
    "      1) GhostModuleV2 1×1 (C→C//reduction) with stride=1\n",
    "      2) 3×3 depthwise convolution on the reduced channels\n",
    "      3) GhostModuleV2 1×1 (C//reduction→C) with stride=1\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 4, ratio: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          channels:    number of input/output channels (C)\n",
    "          reduction:   bottleneck factor (default 4 → reduces to C/4)\n",
    "          ratio:       Ghost ratio for GhostModuleV2 (default 2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        mid_channels = channels // reduction\n",
    "\n",
    "        # 1×1 Ghost down (intrinsic→mid_channels)\n",
    "        self.reduce = GhostModuleV2(\n",
    "            in_channels=channels,\n",
    "            out_channels=mid_channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "        # 3×3 depthwise convolution\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, mid_channels,\n",
    "                      kernel_size=3, stride=1, padding=1,\n",
    "                      groups=mid_channels, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.Hardswish(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 1×1 Ghost up (mid_channels→channels)\n",
    "        self.expand = GhostModuleV2(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        returns: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        x = self.reduce(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.dwconv(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.expand(x)    # → (B, C, H, W)\n",
    "        return x\n",
    "\n",
    "# Example usage in the ASPPNeXt pipeline:\n",
    "\n",
    "# Instantiate once (channels must match final encoder output channels)\n",
    "# e.g., if encoder final stage outputs C=512 feature maps:\n",
    "pre_daaf = PreDAAFGhostV2(channels=512, reduction=4, ratio=2)\n",
    "\n",
    "# In forward pass for RGB and Depth encoders:\n",
    "# f_rgb_out, rgb_skips   = rgb_encoder(rgb_input)\n",
    "# f_depth_out, depth_skips = depth_encoder(depth_input)\n",
    "# f_rgb_pre   = pre_daaf(f_rgb_out)\n",
    "# f_depth_pre = pre_daaf(f_depth_out)\n",
    "#\n",
    "# Then feed (f_rgb_pre, f_depth_pre) into your DAAF fusion block.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e060b4ec-ec2d-4178-a5a0-8fc57be6244c",
   "metadata": {},
   "source": [
    "--------------------xX DAAF Block with GhostModule Integrations Xx--------------------\n",
    "\n",
    "Inputs:\n",
    "  f_rgb_preGhost ∈ ℝ^(B×C×H×W)\n",
    "  f_depth_preGhost ∈ ℝ^(B×C×H×W)\n",
    "\n",
    "1. Local Branch (RDSCB + LIA)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ RDSCB (n∈{1,3,5,7}):               │\n",
    "   │   For each n: GhostConv(n×n)       │\n",
    "   │   LeakyReLU                        │\n",
    "   │ Concatenate → GhostConv(1×1)       │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   LIA: Conv1D pooling → GhostConv(1×1)\n",
    "\n",
    "2. Global Branch (ITB)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ Interactive Self-Attention (ISA)   │\n",
    "   │   Cross-modal MSA on (f_rgb, f_d)  │\n",
    "   │ Residual Add → LayerNorm           │\n",
    "   │ GhostConv FFN:                     │\n",
    "   │   • GhostConv(1×1): C→4C           │\n",
    "   │   • GELU                           │\n",
    "   │   • GhostConv(1×1): 4C→C           │\n",
    "   │ Residual Add                       │\n",
    "   └────────────────────────────────────┘\n",
    "\n",
    "3. Fusion and Reconstruction\n",
    "   ┌──────────────────────────────────────────┐\n",
    "   │ Concat(local, global_rgb, global_depth)  │\n",
    "   │ GhostConv(3×3) → LeakyReLU               │\n",
    "   │ (This is the “Global Fusion” Conv)       │\n",
    "   └──────────────────────────────────────────┘\n",
    "       ↓\n",
    "   Reconstruction Head:\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   Output: f_fused ∈ ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4a95455-17d8-4fa5-b1d1-66c58ed40f57",
   "metadata": {},
   "source": [
    "The decoder block at each stage of ASPPNeXt with skip-connections, GhostASPPFELAN, CoordAttention and DySample executes in exactly this order:\n",
    "\n",
    "Gather Inputs\n",
    "– If Stage 1: take the Post-DAAF fused feature map\n",
    "– Otherwise: take the upsampled output from the previous decoder stage\n",
    "– Also fetch the skip feature from the corresponding encoder block (E₄→Stage 1, E₃→Stage 2, E₂→Stage 3)\n",
    "\n",
    "Fuse Skip Connection\n",
    "– Concatenate along channels:\n",
    "input ⊕ skip → F₀ (shape: B×(2C)×H×W)\n",
    "\n",
    "GhostASPPFELAN\n",
    "a. Branch 1: GhostConv 1×1 → LeakyReLU\n",
    "b. Branch 2: GhostConv 3×3, dilation=2 → LeakyReLU\n",
    "c. Branch 3: GhostConv 3×3, dilation=4 → LeakyReLU\n",
    "d. Concatenate branches (B×3C_out×H×W) → GhostConv 1×1 fusion → LeakyReLU\n",
    "→ Output F₁ (B×C_out×H×W)\n",
    "\n",
    "CoordAttention\n",
    "a. Pool along H → z_h (B×C×1×W)\n",
    "b. Pool along W → z_w (B×C×H×1)\n",
    "c. Concat(z_h, z_w) → GhostConv(1×1,C→C/r) → LeakyReLU → split into t_h, t_w\n",
    "d. t_h → GhostConv(1×1,C/r→C) → sigmoid → attn_h (B×C×1×W)\n",
    "e. t_w → GhostConv(1×1,C/r→C) → sigmoid → attn_w (B×C×H×1)\n",
    "f. Multiply: F₂ = F₁ × attn_h × attn_w\n",
    "\n",
    "DySample Upsampling (×2)\n",
    "– Optionally pre-process with GhostConv(1×1)\n",
    "– Apply content-aware DySample to F₂ → F₃ (B×C×2H×2W)\n",
    "\n",
    "Pass to Next Stage or Final\n",
    "– F₃ is fed as input to the next decoder stage (or into the final 1×1 conv at the end of Stage 3)\n",
    "\n",
    "Summary of per-stage flow\n",
    "Input + Skip → GhostASPPFELAN → CoordAttention → DySample → Next stage."
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fb663-315e-4b3a-90e6-ed035184a557",
   "metadata": {},
   "source": [
    "--------------------xX Ghost ASPPFELAN Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C_in×H×W)\n",
    "  ↓\n",
    "Branch 1: GhostConv(1×1) → LeakyReLU\n",
    "Branch 2: GhostConv(3×3, dilation=2) → LeakyReLU\n",
    "Branch 3: GhostConv(3×3, dilation=4) → LeakyReLU\n",
    "  ↓\n",
    "Concat(branch1,2,3) ∈ ℝ^(B×3C_out×H×W)\n",
    "  ↓\n",
    "GhostConv(1×1) → LeakyReLU\n",
    "  ↓\n",
    "Output: Y ∈ ℝ^(B×C_out×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d717db5-a2b0-47fd-91e4-3f678caedb7e",
   "metadata": {},
   "source": [
    "--------------------xX CoordAttention Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Pool along H: z_h ∈ ℝ^(B×C×1×W)\n",
    "2. Pool along W: z_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Concat(z_h, z_w) → GhostConv(1×1, C→C/r) → LeakyReLU\n",
    "  ↓\n",
    "Split into t_h, t_w channels\n",
    "  ↓\n",
    "t_h → GhostConv(1×1, C/r→C) → sigmoid → attn_h ∈ ℝ^(B×C×1×W)\n",
    "t_w → GhostConv(1×1, C/r→C) → sigmoid → attn_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Output: X' = X × attn_h × attn_w\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "813b60a6-67c5-461d-8120-201c989d29eb",
   "metadata": {},
   "source": [
    "--------------------xX DySample Upsampling Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Feature Transformation: GhostConv(1×1)  # Optional\n",
    "2. DySample Operation:\n",
    "   - Dynamic kernel prediction based on local context\n",
    "   - Content-aware upsampling (×2 scale)\n",
    "  ↓\n",
    "Output: X_up ∈ ℝ^(B×C×2H×2W)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
