{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aea3a0-ce64-45ec-ac09-1cfef823c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 🔧 1. Ghost MLP Block (ConvNeXt-style)\n",
    "# -----------------------------\n",
    "class GhostMLPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, expansion_ratio=4):\n",
    "        super(GhostMLPBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expansion_ratio\n",
    "        self.fc1 = GhostModule(in_channels, hidden_dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = GhostModule(hidden_dim, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 🔧 2. Modified ConvNeXt-Attention Block\n",
    "# -----------------------------\n",
    "class HybridConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, channels, attention_module):\n",
    "        super(HybridConvNeXtBlock, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size=7, padding=3, groups=channels)  # Positional encoding\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.attn = attention_module(channels)  # Could be MHSA, Axial, etc.\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.mlp = GhostMLPBlock(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = x + residual.permute(0, 2, 3, 1)\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x.permute(0, 3, 1, 2))\n",
    "        x = x.permute(0, 2, 3, 1) + residual\n",
    "        return x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔧 3. Ghost CoordAttention Block\n",
    "# -----------------------------\n",
    "class GhostCoordAttention(nn.Module):\n",
    "    def __init__(self, inp, reduction=32):\n",
    "        super(GhostCoordAttention, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mid = max(8, inp // reduction)\n",
    "        self.conv1 = GhostModule(inp, mid, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(mid)\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv_h = GhostModule(mid, inp, kernel_size=1)\n",
    "        self.conv_w = GhostModule(mid, inp, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n, c, h, w = x.size()\n",
    "\n",
    "        x_h = self.pool_h(x).permute(0, 1, 3, 2)\n",
    "        x_w = self.pool_w(x)\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y)\n",
    "\n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_h = self.conv_h(x_h.permute(0, 1, 3, 2))\n",
    "        x_w = self.conv_w(x_w)\n",
    "\n",
    "        out = identity * torch.sigmoid(x_h) * torch.sigmoid(x_w)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# 🔧 4. Ghost ASPPFELAN Block (Simplified)\n",
    "# -----------------------------\n",
    "class GhostASPPFELAN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GhostASPPFELAN, self).__init__()\n",
    "        self.branch1 = GhostModule(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch2 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        self.branch3 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=4, padding=4)\n",
    "        self.fuse = GhostModule(out_channels * 3, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        x = torch.cat([b1, b2, b3], dim=1)\n",
    "        x = self.fuse(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 🏗️ Full Architecture (Pseudo)\n",
    "# -----------------------------\n",
    "class ASPPNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ASPPNeXt, self).__init__()\n",
    "        self.encoder_stages = nn.ModuleList([\n",
    "            HybridConvNeXtBlock(96, attention_module),\n",
    "            HybridConvNeXtBlock(192, attention_module),\n",
    "            HybridConvNeXtBlock(384, attention_module),\n",
    "            HybridConvNeXtBlock(768, attention_module)\n",
    "        ])\n",
    "\n",
    "        self.bottleneck = DAAFModule(768)  # Assume DAAFModule is implemented\n",
    "\n",
    "        self.decoder_stages = nn.ModuleList([\n",
    "            GhostASPPFELAN(768, 384),\n",
    "            GhostASPPFELAN(384, 192),\n",
    "            GhostASPPFELAN(192, 96)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList([\n",
    "            GhostCoordAttention(384),\n",
    "            GhostCoordAttention(192),\n",
    "            GhostCoordAttention(96)\n",
    "        ])\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(96, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_feats = []\n",
    "        for stage in self.encoder_stages:\n",
    "            x = stage(x)\n",
    "            enc_feats.append(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i in range(3):\n",
    "            x = self.upsample(x)\n",
    "            x = x + enc_feats[-(i+2)]  # Skip connection\n",
    "            x = self.decoder_stages[i](x)\n",
    "            x = self.attentions[i](x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Note: GhostModule and DAAFModule need to be implemented or imported\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c133a51c-fd32-48e9-afa1-8098e6d1e257",
   "metadata": {},
   "source": [
    "--------------------xX GhostModule Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  ↓\n",
    "Primary Features: \n",
    "  - 1×1 Conv → C_mid = C_out // ratio\n",
    "  ↓\n",
    "Ghost Features:\n",
    "  - Depthwise Conv (or cheap linear ops) on C_mid\n",
    "  ↓\n",
    "Concatenate Primary + Ghost → C_out\n",
    "  ↓\n",
    "BatchNorm (optional)\n",
    "  ↓\n",
    "Output: (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c0e33b-905a-4c8c-a202-e860c4ae83a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--------------------xX Modified ConvNeXt Block with Self-Attention + Ghost MLP Xx--------------------\n",
    "\n",
    "Input: (B, C, H, W)\n",
    "  ↓\n",
    "7×7 Depthwise Convolution (Positional Info)\n",
    "  ↓\n",
    "Permute to (B, H, W, C)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Self-Attention Module (e.g. MHSA / Axial Attention)\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Ghost MLP:\n",
    "    - GhostConv(1×1, C → 4C)\n",
    "    - GELU\n",
    "    - GhostConv(1×1, 4C → C)\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "Permute back to (B, C, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fb663-315e-4b3a-90e6-ed035184a557",
   "metadata": {},
   "source": [
    "--------------------xX Ghost ASPPFELAN Decoder Block Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  ↓\n",
    "Branch 1: GhostConv (1×1)\n",
    "Branch 2: GhostConv (3×3, dilation=2)\n",
    "Branch 3: GhostConv (3×3, dilation=4)\n",
    "  ↓\n",
    "Concatenate all branches → (B, 3×C_out, H, W)\n",
    "  ↓\n",
    "GhostConv (1×1) for fusion → (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ab70a1c-1825-4e74-81b4-23fceb8604a0",
   "metadata": {},
   "source": [
    "--------------------xX Full Architecture – ASPPNeXt Xx--------------------\n",
    "\n",
    "                    RGB Input        Depth Input\n",
    "                         │                │\n",
    "       ┌─────────────────┘                └─────────────────┐\n",
    "       │                                                    │\n",
    " ┌────▼────┐                                          ┌─────▼─────┐\n",
    " │ Encoder │ ← Hybrid ConvNeXt Blocks (x4)            │ Encoder   │ ← Hybrid ConvNeXt Blocks (x4)\n",
    " │ (RGB)   │                                          │ (Depth)   │\n",
    " └────┬────┘                                          └─────┬─────┘\n",
    "      │                                                       │\n",
    "      └───────────────┬─────[ Feature Fusion ]─────┬──────────┘\n",
    "                      ▼                           ▼\n",
    "                  ┌────────────────────────────────────┐\n",
    "                  │      DAAF (RGB ⊕ Depth Fusion)    │\n",
    "                  └────────────────────────────────────┘\n",
    "                                    ↓\n",
    "            ┌────────────────────────────────────────────┐\n",
    "            │ [Optional] GhostModule Pre/Post Bottleneck │\n",
    "            └────────────────────────────────────────────┘\n",
    "                                    ↓\n",
    "            ┌───────────── Decoder Stage 1 ──────────────┐\n",
    "            │ Ghost ASPPFELAN + CoordAttention + Upsample│\n",
    "            └────────────────────────────────────────────┘\n",
    "                                    ↓\n",
    "            ┌───────────── Decoder Stage 2 ──────────────┐\n",
    "            │ Ghost ASPPFELAN + CoordAttention + Upsample│\n",
    "            └────────────────────────────────────────────┘\n",
    "                                    ↓\n",
    "            ┌───────────── Decoder Stage 3 ──────────────┐\n",
    "            │ Ghost ASPPFELAN + CoordAttention + Upsample│\n",
    "            └────────────────────────────────────────────┘\n",
    "                                    ↓\n",
    "                      Final 1×1 Conv → Output Map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
