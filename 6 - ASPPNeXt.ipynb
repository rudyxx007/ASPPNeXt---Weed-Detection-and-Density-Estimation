{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7ab70a1c-1825-4e74-81b4-23fceb8604a0",
   "metadata": {},
   "source": [
    "--------------------xX ASPPNeXt Architecture Xx--------------------\n",
    "\n",
    "             RGB Input          Depth Input\n",
    "                │                   │\n",
    "       ┌────────▼────────┐ ┌────────▼────────┐\n",
    "       │ Hybrid ConvNeXt │ │ Hybrid ConvNeXt │\n",
    "       │ Block ×4 (with  │ │ Block ×4 (with  │\n",
    "       │  VWA + GhostMLP)│ │  VWA + GhostMLP)│\n",
    "       └────────┬────────┘ └────────┬────────┘\n",
    "                │                   │\n",
    "       ┌────────▼────────┐ ┌────────▼────────┐\n",
    "       │ Pre-DAAF        │ │ Pre-DAAF        │\n",
    "       │ GhostModule     │ │ GhostModule     │\n",
    "       └────────┬────────┘ └────────┬────────┘\n",
    "                │                   │\n",
    "                └──────► DAAF ◄─────┘\n",
    "                          │\n",
    "                ┌─────────▼─────────┐\n",
    "                │ Post-DAAF         │\n",
    "                │ GhostModule       │\n",
    "                └─────────┬─────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 1         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E4)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 2         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E3)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 3         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E2)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "                   Final 1×1 Conv\n",
    "                          ↓\n",
    "                     Output Map\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c133a51c-fd32-48e9-afa1-8098e6d1e257",
   "metadata": {},
   "source": [
    "--------------------xX GhostModule Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  ↓\n",
    "Primary Features: \n",
    "  - 1×1 Conv → C_mid = C_out // ratio\n",
    "  ↓\n",
    "Ghost Features:\n",
    "  - Depthwise Conv (or cheap linear ops) on C_mid\n",
    "  ↓\n",
    "Concatenate Primary + Ghost → C_out\n",
    "  ↓\n",
    "BatchNorm (optional)\n",
    "  ↓\n",
    "Output: (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c0e33b-905a-4c8c-a202-e860c4ae83a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--------------------xX Hybrid ConvNeXt Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "7×7 DepthwiseConv\n",
    "  ↓\n",
    "Permute → ℝ^(B×H×W×C)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Varying Window Attention:\n",
    "  • Query window: P×P fixed\n",
    "  • Context window: (R×P)×(R×P)\n",
    "  • MHSA with zero extra cost\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Ghost MLP:\n",
    "  • GhostConv(1×1): C→4C → GELU\n",
    "  • GhostConv(1×1): 4C→C\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "Permute back → ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e060b4ec-ec2d-4178-a5a0-8fc57be6244c",
   "metadata": {},
   "source": [
    "--------------------xX DAAF Block with GhostModule Integrations Xx--------------------\n",
    "\n",
    "Inputs:\n",
    "  f_rgb_preGhost ∈ ℝ^(B×C×H×W)\n",
    "  f_depth_preGhost ∈ ℝ^(B×C×H×W)\n",
    "\n",
    "1. Local Branch (RDSCB + LIA)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ RDSCB (n∈{1,3,5,7}):               │\n",
    "   │   For each n: GhostConv(n×n)       │\n",
    "   │   LeakyReLU                        │\n",
    "   │ Concatenate → GhostConv(1×1)       │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   LIA: Conv1D pooling → GhostConv(1×1)\n",
    "\n",
    "2. Global Branch (ITB)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ Interactive Self-Attention (ISA)   │\n",
    "   │   Cross-modal MSA on (f_rgb, f_d)  │\n",
    "   │ Residual Add → LayerNorm           │\n",
    "   │ GhostConv FFN:                     │\n",
    "   │   • GhostConv(1×1): C→4C           │\n",
    "   │   • GELU                           │\n",
    "   │   • GhostConv(1×1): 4C→C           │\n",
    "   │ Residual Add                       │\n",
    "   └────────────────────────────────────┘\n",
    "\n",
    "3. Fusion and Reconstruction\n",
    "   ┌──────────────────────────────────────────┐\n",
    "   │ Concat(local, global_rgb, global_depth)  │\n",
    "   │ GhostConv(3×3) → LeakyReLU               │\n",
    "   │ (This is the “Global Fusion” Conv)       │\n",
    "   └──────────────────────────────────────────┘\n",
    "       ↓\n",
    "   Reconstruction Head:\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   Output: f_fused ∈ ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3f1800c-6e89-4f1a-8a7a-c31a7ce7beae",
   "metadata": {},
   "source": [
    "The decoder block at each stage of ASPPNeXt with skip-connections, GhostASPPFELAN, CoordAttention and DySample executes in exactly this order:\n",
    "\n",
    "Gather Inputs\n",
    "– If Stage 1: take the Post-DAAF fused feature map\n",
    "– Otherwise: take the upsampled output from the previous decoder stage\n",
    "– Also fetch the skip feature from the corresponding encoder block (E₄→Stage 1, E₃→Stage 2, E₂→Stage 3)\n",
    "\n",
    "Fuse Skip Connection\n",
    "– Concatenate along channels:\n",
    "input ⊕ skip → F₀ (shape: B×(2C)×H×W)\n",
    "\n",
    "GhostASPPFELAN\n",
    "a. Branch 1: GhostConv 1×1 → LeakyReLU\n",
    "b. Branch 2: GhostConv 3×3, dilation=2 → LeakyReLU\n",
    "c. Branch 3: GhostConv 3×3, dilation=4 → LeakyReLU\n",
    "d. Concatenate branches (B×3C_out×H×W) → GhostConv 1×1 fusion → LeakyReLU\n",
    "→ Output F₁ (B×C_out×H×W)\n",
    "\n",
    "CoordAttention\n",
    "a. Pool along H → z_h (B×C×1×W)\n",
    "b. Pool along W → z_w (B×C×H×1)\n",
    "c. Concat(z_h, z_w) → GhostConv(1×1,C→C/r) → LeakyReLU → split into t_h, t_w\n",
    "d. t_h → GhostConv(1×1,C/r→C) → sigmoid → attn_h (B×C×1×W)\n",
    "e. t_w → GhostConv(1×1,C/r→C) → sigmoid → attn_w (B×C×H×1)\n",
    "f. Multiply: F₂ = F₁ × attn_h × attn_w\n",
    "\n",
    "DySample Upsampling (×2)\n",
    "– Optionally pre-process with GhostConv(1×1)\n",
    "– Apply content-aware DySample to F₂ → F₃ (B×C×2H×2W)\n",
    "\n",
    "Pass to Next Stage or Final\n",
    "– F₃ is fed as input to the next decoder stage (or into the final 1×1 conv at the end of Stage 3)\n",
    "\n",
    "Summary of per-stage flow\n",
    "Input + Skip → GhostASPPFELAN → CoordAttention → DySample → Next stage."
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fb663-315e-4b3a-90e6-ed035184a557",
   "metadata": {},
   "source": [
    "--------------------xX Ghost ASPPFELAN Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C_in×H×W)\n",
    "  ↓\n",
    "Branch 1: GhostConv(1×1) → LeakyReLU\n",
    "Branch 2: GhostConv(3×3, dilation=2) → LeakyReLU\n",
    "Branch 3: GhostConv(3×3, dilation=4) → LeakyReLU\n",
    "  ↓\n",
    "Concat(branch1,2,3) ∈ ℝ^(B×3C_out×H×W)\n",
    "  ↓\n",
    "GhostConv(1×1) → LeakyReLU\n",
    "  ↓\n",
    "Output: Y ∈ ℝ^(B×C_out×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d717db5-a2b0-47fd-91e4-3f678caedb7e",
   "metadata": {},
   "source": [
    "--------------------xX CoordAttention Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Pool along H: z_h ∈ ℝ^(B×C×1×W)\n",
    "2. Pool along W: z_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Concat(z_h, z_w) → GhostConv(1×1, C→C/r) → LeakyReLU\n",
    "  ↓\n",
    "Split into t_h, t_w channels\n",
    "  ↓\n",
    "t_h → GhostConv(1×1, C/r→C) → sigmoid → attn_h ∈ ℝ^(B×C×1×W)\n",
    "t_w → GhostConv(1×1, C/r→C) → sigmoid → attn_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Output: X' = X × attn_h × attn_w\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "813b60a6-67c5-461d-8120-201c989d29eb",
   "metadata": {},
   "source": [
    "--------------------xX DySample Upsampling Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Feature Transformation: GhostConv(1×1)  # Optional\n",
    "2. DySample Operation:\n",
    "   - Dynamic kernel prediction based on local context\n",
    "   - Content-aware upsampling (×2 scale)\n",
    "  ↓\n",
    "Output: X_up ∈ ℝ^(B×C×2H×2W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11102300-cb45-4ee8-b15e-e0d75bc3470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e19c5-263b-4b77-9a24-342633cc4a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60399c-f18f-4b4b-b778-ce66925dcfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9616ca-6247-4716-9c14-503f8dda5e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456c00e-717f-494e-859d-4fe8a1af39dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5262a4-3066-4b54-b6f4-ab733ed9233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ed60fa-3ff2-4518-8337-9503d72529cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e915004-b960-416b-82c0-9f4f880eb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = os.path.join(os.getcwd(), \"ASPPNeXt Models\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9544a6f3-e3fd-4213-bf8e-4f1c86f66f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFCAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dfc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1, bias=False),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()  # Remove .add_(1e-6)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention = self.dfc(x) + 1e-6\n",
    "        return x * attention\n",
    "\n",
    "class GhostModuleV2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 ratio=2, kernel_size=1, dw_size=3,\n",
    "                 stride=1, use_attn=True):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        init_ch = math.ceil(out_channels / ratio)\n",
    "        new_ch = init_ch * (ratio - 1)\n",
    "\n",
    "        # Primary convolution with dynamic padding\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, init_ch,\n",
    "                     kernel_size, stride,\n",
    "                     (kernel_size - 1) // 2,  # More precise padding\n",
    "                     bias=False),\n",
    "            nn.BatchNorm2d(init_ch),\n",
    "            nn.Hardswish(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Cheap operation with dynamic padding\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_ch, new_ch, dw_size, 1,\n",
    "                     (dw_size - 1) // 2,  # More precise padding\n",
    "                     groups=init_ch, bias=False),\n",
    "            nn.BatchNorm2d(new_ch),\n",
    "            nn.Hardswish(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Attention with stride warning\n",
    "        self.use_attn = use_attn\n",
    "        if use_attn:\n",
    "#            if stride > 1:\n",
    "#                print(f\"Warning: Stride {stride}>1 may reduce attention effectiveness\")\n",
    "            self.attn = DFCAttention(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        out = T.cat([x1, x2], dim=1)[:, :self.out_channels]\n",
    "        return self.attn(out) if self.use_attn else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632c4c0f-bf1b-4412-ae5d-1f4f0c5907a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaryingWindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, context_ratio=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.context_ratio = context_ratio\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        P = self.window_size\n",
    "        R = self.context_ratio\n",
    "\n",
    "        x_spatial = x.view(B, H, W, C)\n",
    "\n",
    "        q_windows = x_spatial.unfold(1, P, P).unfold(2, P, P)\n",
    "        q_windows = q_windows.contiguous().view(B, -1, P * P, C)\n",
    "\n",
    "        pad = (R * P - P) // 2\n",
    "        x_pad = F.pad(x_spatial, (0, 0, pad, pad, pad, pad))\n",
    "        ctx_windows = x_pad.unfold(1, R * P, P).unfold(2, R * P, P)\n",
    "        ctx_windows = ctx_windows.contiguous().view(B, -1, (R * P) ** 2, C)\n",
    "\n",
    "        if q_windows.size(1) != ctx_windows.size(1):\n",
    "            raise ValueError(f\"Query and context windows mismatch: {q_windows.shape[1]} vs {ctx_windows.shape[1]}\")\n",
    "\n",
    "        seq = T.cat([q_windows, ctx_windows], dim=2)\n",
    "\n",
    "        qkv = self.to_qkv(seq)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * (C ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v\n",
    "\n",
    "        out = out[:, :, :P * P, :]\n",
    "        out = out.view(B, H // P, W // P, P, P, C)\n",
    "        out = out.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, C)\n",
    "        out = out.view(B, N, C)\n",
    "\n",
    "        return self.to_out(out)\n",
    "        \n",
    "\n",
    "class GhostMLP(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * mlp_ratio\n",
    "        self.fc1 = GhostModuleV2(dim, hidden_dim, use_attn=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = GhostModuleV2(hidden_dim, dim, use_attn=False)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x_spatial = x.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        x_spatial = self.act(self.fc1(x_spatial))\n",
    "        x_spatial = self.fc2(x_spatial)\n",
    "        x_flat = x_spatial.permute(0, 2, 3, 1).view(B, N, C)\n",
    "        return x_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5598e7b5-a4eb-40a7-910c-f35829c39a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Channel-wise LayerNorm for 4D tensors (B, C, H, W), ConvNeXt style.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(num_channels, eps=eps)\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W) → (B, H, W, C)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.norm(x)\n",
    "        # → (B, C, H, W)\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e3b707c-efb0-47c6-85f2-f7818a31dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size,\n",
    "                 context_ratio=2, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 7, padding=3,\n",
    "                                groups=dim, bias=False)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = VaryingWindowAttention(dim, num_heads,\n",
    "                                           window_size, context_ratio)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = GhostMLP(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        seq = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        seq = seq + self.attn(self.norm1(seq), H, W)\n",
    "        seq = seq + self.mlp(self.norm2(seq), H, W)\n",
    "        return seq.view(B, H, W, C).permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2a6e33-ef46-4fef-b486-631a450f042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ASPPNeXtEncoder with patch embedding, hierarchical Ghost blocks, and LayerNorm2d.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, base_dim=64,\n",
    "                 num_heads=(4, 8, 16, 32),\n",
    "                 window_sizes=(16, 8, 4, 2),  # ✅ Updated here\n",
    "                 mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Stem layer (Patch Embedding)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, base_dim, kernel_size=4, stride=4, bias=False),  # Down to 1/4\n",
    "            LayerNorm2d(base_dim)\n",
    "        )\n",
    "        \n",
    "        # 2. Four stages of encoder\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            dim = base_dim * (2 ** i)\n",
    "            down = None\n",
    "            \n",
    "            # Downsample before stages 2-4\n",
    "            if i > 0:\n",
    "                down = nn.Sequential(\n",
    "                    GhostModuleV2(\n",
    "                        in_channels=dim // 2,\n",
    "                        out_channels=dim,\n",
    "                        ratio=2,\n",
    "                        kernel_size=2,\n",
    "                        stride=2,\n",
    "                        use_attn=True\n",
    "                    ),\n",
    "                    LayerNorm2d(dim)\n",
    "                )\n",
    "                \n",
    "            block = HybridConvNeXtBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_sizes[i],  # ✅ Applies updated values\n",
    "                context_ratio=2,\n",
    "                mlp_ratio=mlp_ratio\n",
    "            )\n",
    "            \n",
    "            self.stages.append(nn.ModuleDict({'down': down, 'block': block}))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.stem(x)  # → (B, base_dim, H/4, W/4)\n",
    "        skips = []\n",
    "        for stage in self.stages:\n",
    "            if stage['down'] is not None:\n",
    "                x = stage['down'](x)\n",
    "            x = stage['block'](x)\n",
    "            skips.append(x)\n",
    "        return x, skips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbc73c7-de29-46bb-9c89-f6eaf2268b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreDAAFGhostV2(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 4, ratio: int = 2):\n",
    "        super().__init__()\n",
    "        mid_channels = channels // reduction\n",
    "\n",
    "        # 1. Reduce channels (C → C//reduction)\n",
    "        self.reduce = GhostModuleV2(\n",
    "            in_channels=channels,\n",
    "            out_channels=mid_channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "        # 2. Lightweight depthwise conv\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, mid_channels,\n",
    "                      kernel_size=3, stride=1, padding=1,\n",
    "                      groups=mid_channels, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.Hardswish(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 3. Expand back to original channels (C//reduction → C)\n",
    "        self.expand = GhostModuleV2(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        Input:  (B, C, H, W)\n",
    "        Output: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        x = self.reduce(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.expand(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705ae152-145e-4301-83d9-2480661cea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDSCBLocal(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Depthwise Separable Convolutions Branch (local branch)\n",
    "    Applies GhostModuleV2 with kernel sizes 1,3,5,7, then fuses.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        # Four multi‐scale GhostConv branches\n",
    "        self.conv1 = GhostModuleV2(channels, channels, kernel_size=1, use_attn=False)\n",
    "        self.conv3 = GhostModuleV2(channels, channels, kernel_size=3, use_attn=False)\n",
    "        self.conv5 = GhostModuleV2(channels, channels, kernel_size=5, use_attn=False)\n",
    "        self.conv7 = GhostModuleV2(channels, channels, kernel_size=7, use_attn=False)\n",
    "        # Fusion 1×1 GhostConv\n",
    "        self.fuse = GhostModuleV2(channels * 4, channels, kernel_size=1, use_attn=False)\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        # x: (B, C, H, W)\n",
    "        b1 = self.act(self.conv1(x))\n",
    "        b3 = self.act(self.conv3(x))\n",
    "        b5 = self.act(self.conv5(x))\n",
    "        b7 = self.act(self.conv7(x))\n",
    "        # Concatenate multi‐scale features\n",
    "        cat = T.cat([b1, b3, b5, b7], dim=1)  # (B, 4C, H, W)\n",
    "        # Fuse back to C channels\n",
    "        out = self.act(self.fuse(cat))            # (B, C, H, W)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LIA(nn.Module):\n",
    "    \"\"\"\n",
    "    Local Interaction Attention: cross-modal local feature fusion mechanism\n",
    "    as conventionally used in DAAF. Uses both spatial average pooling and \n",
    "    standard deviation pooling to capture local contrast and variance.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        # Global pooling operations\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.std_pool = lambda x: T.sqrt(T.var(x, dim=(2,3), keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Learnable coefficients for combining avg and std features\n",
    "        self.alpha = nn.Parameter(T.ones(1))\n",
    "        self.beta = nn.Parameter(T.ones(1))\n",
    "        \n",
    "        # MLP for generating attention weights\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // 4, kernel_size=1, bias=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(channels // 4, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Final refinement convolution - note: use_attn=False as discussed\n",
    "        self.refine = GhostModuleV2(channels, channels, kernel_size=3, use_attn=False)\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W) - concatenated or summed local features from RGB+Depth\n",
    "        returns: (B, C, H, W) - refined local features with attention\n",
    "        \"\"\"\n",
    "        # Generate modality-aware saliency maps\n",
    "        avg_feat = self.avg_pool(x)      # (B, C, 1, 1)\n",
    "        std_feat = self.std_pool(x)      # (B, C, 1, 1)\n",
    "        \n",
    "        # Combine avg and std with learnable coefficients\n",
    "        combined_feat = self.alpha * avg_feat + self.beta * std_feat\n",
    "        \n",
    "        # Generate attention weights\n",
    "        attention = self.mlp(combined_feat)  # (B, C, 1, 1)\n",
    "        \n",
    "        # Apply attention and refine\n",
    "        attended = x * attention\n",
    "        refined = self.act(self.refine(attended))\n",
    "        \n",
    "        return refined\n",
    "\n",
    "\n",
    "class InteractiveTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Interactive Transformer Block (ITB): Cross‐modal interactive self‐attention + Ghost MLP FFN.\n",
    "    Produces two outputs: global features for RGB and Depth.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Layer normalization before attention\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        \n",
    "        # Cross-modal multi-head attention\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=channels,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Layer normalization after residual connection\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        \n",
    "        # Ghost-based FFN: C→4C→C with GELU activation\n",
    "        # Note: use_attn=False to avoid redundant attention in DAAF context\n",
    "        self.ffn = nn.ModuleDict({\n",
    "            'expand': GhostModuleV2(channels, channels * 4, kernel_size=1, use_attn=False),\n",
    "            'contract': GhostModuleV2(channels * 4, channels, kernel_size=1, use_attn=False)\n",
    "        })\n",
    "        self.ffn_act = nn.GELU()\n",
    "        self.ffn_dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def _apply_ffn(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"Apply Ghost-based FFN to sequence data\"\"\"\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        \n",
    "        # Reshape to 4D for GhostModuleV2\n",
    "        x_4d = x.view(B, H, W, C).permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        \n",
    "        # Apply FFN\n",
    "        x_4d = self.ffn['expand'](x_4d)\n",
    "        x_4d = self.ffn_act(x_4d)\n",
    "        x_4d = self.ffn_dropout(x_4d)\n",
    "        x_4d = self.ffn['contract'](x_4d)\n",
    "        \n",
    "        # Reshape back to sequence\n",
    "        x_out = x_4d.permute(0, 2, 3, 1).view(B, N, C)\n",
    "        return x_out\n",
    "\n",
    "    def forward(self, f_rgb: T.Tensor, f_depth: T.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        f_rgb, f_depth: (B, C, H, W)\n",
    "        returns: (g_rgb, g_depth) each (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = f_rgb.shape\n",
    "        N = H * W\n",
    "        \n",
    "        # Flatten spatial dimensions for attention\n",
    "        rgb_seq = f_rgb.permute(0, 2, 3, 1).view(B, N, C)      # (B, N, C)\n",
    "        depth_seq = f_depth.permute(0, 2, 3, 1).view(B, N, C)  # (B, N, C)\n",
    "        \n",
    "        # Normalize before attention\n",
    "        rgb_norm = self.norm1(rgb_seq)\n",
    "        depth_norm = self.norm1(depth_seq)\n",
    "        \n",
    "        # Cross-modal attention: RGB queries attend to Depth keys/values\n",
    "        g_rgb_seq, _ = self.attn(\n",
    "            query=rgb_norm,\n",
    "            key=depth_norm,\n",
    "            value=depth_norm\n",
    "        )\n",
    "        \n",
    "        # Cross-modal attention: Depth queries attend to RGB keys/values  \n",
    "        g_depth_seq, _ = self.attn(\n",
    "            query=depth_norm,\n",
    "            key=rgb_norm,\n",
    "            value=rgb_norm\n",
    "        )\n",
    "        \n",
    "        # First residual connection + normalization\n",
    "        g_rgb_seq = self.norm2(rgb_seq + g_rgb_seq)\n",
    "        g_depth_seq = self.norm2(depth_seq + g_depth_seq)\n",
    "        \n",
    "        # FFN with second residual connection\n",
    "        g_rgb_seq = g_rgb_seq + self._apply_ffn(g_rgb_seq)\n",
    "        g_depth_seq = g_depth_seq + self._apply_ffn(g_depth_seq)\n",
    "        \n",
    "        # Reshape back to spatial format\n",
    "        g_rgb = g_rgb_seq.view(B, H, W, C).permute(0, 3, 1, 2)     # (B, C, H, W)\n",
    "        g_depth = g_depth_seq.view(B, H, W, C).permute(0, 3, 1, 2) # (B, C, H, W)\n",
    "        \n",
    "        return g_rgb, g_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfa22efc-0c9d-4d73-9f6e-41edc8021df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAAFBlock(nn.Module):\n",
    "    def __init__(self, channels: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        # 1. Local branch per modality\n",
    "        self.local_branch = RDSCBLocal(channels)\n",
    "        # Fuse local RGB/depth outputs back to C channels\n",
    "        self.local_fuse = GhostModuleV2(\n",
    "            in_channels=channels * 2,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.lia = LIA(channels)\n",
    "\n",
    "        # 2. Global branch\n",
    "        self.itb = InteractiveTransformerBlock(channels, num_heads)\n",
    "\n",
    "        # 3. Global fusion conv: (local + global_rgb + global_depth) → channels\n",
    "        self.global_fuse = GhostModuleV2(\n",
    "            in_channels=channels * 3,\n",
    "            out_channels=channels,\n",
    "            kernel_size=3,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        # 4. Reconstruction head: three cascaded GhostConv(3×3)\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            GhostModuleV2(channels, channels, kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            GhostModuleV2(channels, channels, kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            GhostModuleV2(channels, channels, kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                f_rgb_pre: T.Tensor,\n",
    "                f_depth_pre: T.Tensor) -> T.Tensor:\n",
    "        # 1. Local branch on each modality\n",
    "        lr = self.lia(self.local_branch(f_rgb_pre))\n",
    "        ld = self.lia(self.local_branch(f_depth_pre))\n",
    "        # Fuse local RGB + Depth features\n",
    "        local = T.cat([lr, ld], dim=1)   # (B, 2C, H, W)\n",
    "        local = self.act(self.local_fuse(local))  # (B, C, H, W)\n",
    "\n",
    "        # 2. Global interactive attention\n",
    "        g_rgb, g_depth = self.itb(f_rgb_pre, f_depth_pre)\n",
    "\n",
    "        # 3. Global fusion\n",
    "        cat = T.cat([local, g_rgb, g_depth], dim=1)  # (B, 3C, H, W)\n",
    "        fused = self.act(self.global_fuse(cat))         # (B, C, H, W)\n",
    "\n",
    "        # 4. Reconstruction head\n",
    "        out = self.reconstruction(fused)  # (B, C, H, W)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30dfaca2-2c3a-4569-902c-fb80cd145616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostDAAFGhostV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-DAAF GhostModule bottleneck using GhostModuleV2 with DFC attention.\n",
    "    Mirrors the Pre-DAAF structure:\n",
    "      1) GhostModuleV2 1×1 (C→C//reduction)\n",
    "      2) 3×3 depthwise conv\n",
    "      3) GhostModuleV2 1×1 (C//reduction→C)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 4, ratio: int = 2):\n",
    "        super().__init__()\n",
    "        mid_ch = channels // reduction\n",
    "\n",
    "        # 1×1 GhostModuleV2 reduce channels\n",
    "        self.reduce = GhostModuleV2(\n",
    "            in_channels=channels,\n",
    "            out_channels=mid_ch,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "        # 3×3 depthwise conv on reduced features\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                mid_ch, mid_ch,\n",
    "                kernel_size=3, stride=1,\n",
    "                padding=1, groups=mid_ch,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_ch),\n",
    "            nn.Hardswish(inplace=True)\n",
    "        )\n",
    "        # 1×1 GhostModuleV2 expand back to original channels\n",
    "        self.expand = GhostModuleV2(\n",
    "            in_channels=mid_ch,\n",
    "            out_channels=channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W) — fused feature map from DAAFBlock\n",
    "        returns: (B, C, H, W) — channel-refined output\n",
    "        \"\"\"\n",
    "        x = self.reduce(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.dwconv(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.expand(x)    # → (B, C, H, W)\n",
    "        return x\n",
    "\n",
    "# Example integration:\n",
    "\n",
    "# After you obtain `out` from your DAAFBlock:\n",
    "# daaf = DAAFBlock(channels=..., num_heads=...)\n",
    "# fused = daaf(f_rgb_pre, f_depth_pre)\n",
    "#\n",
    "# post_daaf = PostDAAFGhostV2(channels=fused.shape[1], reduction=4, ratio=2)\n",
    "# refined = post_daaf(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce306b0-953d-46ec-b9f6-9efc54af5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedGhostBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    GhostModuleV2 with configurable dilation in the depthwise (cheap) path.\n",
    "    Keeps the model lightweight for edge deployment.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, use_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = GhostModuleV2(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            ratio=2,\n",
    "            use_attn=use_attn\n",
    "        )\n",
    "\n",
    "        # Set dilation and padding for the depthwise part (cheap_operation[0])\n",
    "        self.block.cheap_operation[0].dilation = (dilation, dilation)\n",
    "        self.block.cheap_operation[0].padding = (dilation, dilation)\n",
    "\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.block(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07eebee2-ff13-402c-8885-b0d222c474bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GhostASPPFELAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Ghost ASPPFELAN Block (dilation-based):\n",
    "      - Branch 1: GhostConv(3×3, dilation=1) → LeakyReLU\n",
    "      - Branch 2: GhostConv(3×3, dilation=3) → LeakyReLU\n",
    "      - Branch 3: GhostConv(3×3, dilation=5) → LeakyReLU\n",
    "      - Branch 4: GhostConv(3×3, dilation=7) → LeakyReLU\n",
    "      - Concatenate → GhostConv(1×1) → LeakyReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Helper to create branch with specific dilation\n",
    "        def make_branch(dilation):\n",
    "            branch = GhostModuleV2(in_channels, out_channels, kernel_size=3, use_attn=False)\n",
    "            # Apply dilation and padding to depthwise (cheap) path\n",
    "            branch.cheap_operation[0].dilation = (dilation, dilation)\n",
    "            branch.cheap_operation[0].padding = (dilation, dilation)\n",
    "            return nn.Sequential(branch, nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.branch1 = make_branch(1)\n",
    "        self.branch2 = make_branch(3)\n",
    "        self.branch3 = make_branch(5)\n",
    "        self.branch4 = make_branch(7)\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            GhostModuleV2(out_channels * 4, out_channels, kernel_size=1, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        b4 = self.branch4(x)\n",
    "\n",
    "        cat = T.cat([b1, b2, b3, b4], dim=1)  # (B, 4*out_ch, H, W)\n",
    "        out = self.fuse(cat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7971a21-44bd-4f5e-a4c8-7f2d825618d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoordAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Coordinate Attention block using GhostModuleV2 instead of standard 1x1 convs.\n",
    "    Applies separate attention along height and width axes.\n",
    "    Input:  (B, C, H, W)\n",
    "    Output: (B, C, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 4):\n",
    "        super().__init__()\n",
    "        mid_ch = max(8, channels // reduction)  # Prevent too small intermediate channels\n",
    "\n",
    "        # Shared intermediate transformation\n",
    "        self.conv1 = GhostModuleV2(\n",
    "            in_channels=channels,\n",
    "            out_channels=mid_ch,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        # Independent convs to recover attention maps\n",
    "        self.conv_h = GhostModuleV2(\n",
    "            in_channels=mid_ch,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.conv_w = GhostModuleV2(\n",
    "            in_channels=mid_ch,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Pool height and width separately\n",
    "        z_h = F.adaptive_avg_pool2d(x, (H, 1))         # (B, C, H, 1)\n",
    "        z_w = F.adaptive_avg_pool2d(x, (1, W))         # (B, C, 1, W)\n",
    "        z_w = z_w.permute(0, 1, 3, 2)                  # (B, C, W, 1)\n",
    "\n",
    "        # Concatenate along spatial dimension\n",
    "        z = T.cat([z_h, z_w], dim=2)                   # (B, C, H+W, 1)\n",
    "\n",
    "        # Pass through shared conv\n",
    "        z = self.conv1(z)                              # (B, mid_ch, H+W, 1)\n",
    "        z = self.act(z)\n",
    "\n",
    "        # Split and apply separate attention along H and W\n",
    "        z_h, z_w = T.split(z, [H, W], dim=2)           # z_h: (B, mid_ch, H, 1), z_w: (B, mid_ch, W, 1)\n",
    "        a_h = self.sigmoid(self.conv_h(z_h))           # (B, C, H, 1)\n",
    "        a_w = self.sigmoid(self.conv_w(z_w.permute(0, 1, 3, 2)))  # (B, C, 1, W)\n",
    "\n",
    "        # Apply attention maps\n",
    "        out = x * a_h * a_w                            # (B, C, H, W)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4414ae19-8da8-4538-b885-dc656341d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DySample(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic content-aware upsampling (×scale) via point sampling.\n",
    "    Now optionally uses GhostModuleV2 or standard Conv2d for feature transform.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 scale: int = 2,\n",
    "                 use_feat_transform: bool = False,\n",
    "                 use_ghost: bool = True):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.use_feat_transform = use_feat_transform\n",
    "\n",
    "        # Optional feature transform layer\n",
    "        if self.use_feat_transform:\n",
    "            if use_ghost:\n",
    "                self.transform = GhostModuleV2(\n",
    "                    in_channels, in_channels,\n",
    "                    kernel_size=1, use_attn=False\n",
    "                )\n",
    "            else:\n",
    "                self.transform = nn.Conv2d(\n",
    "                    in_channels, in_channels,\n",
    "                    kernel_size=1, stride=1, padding=0, bias=False\n",
    "                )\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "        # Offset prediction layer\n",
    "        self.offset_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            2 * scale * scale,\n",
    "            kernel_size=1,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Optional transform\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        # Bilinear upsample\n",
    "        H2, W2 = H * self.scale, W * self.scale\n",
    "        x_interp = F.interpolate(\n",
    "            x, size=(H2, W2),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        # Predict offsets\n",
    "        offsets = self.offset_conv(x)\n",
    "\n",
    "        # Reshape and permute offsets\n",
    "        offsets = offsets.view(B, 2, self.scale, self.scale, H, W)\n",
    "        offsets = offsets.permute(0, 1, 4, 2, 5, 3)\n",
    "        offsets = offsets.reshape(B, 2, H2, W2)\n",
    "\n",
    "        # Build base grid\n",
    "        device = x.device\n",
    "        ys = T.linspace(-1, 1, H2, device=device)\n",
    "        xs = T.linspace(-1, 1, W2, device=device)\n",
    "        grid_y, grid_x = T.meshgrid(ys, xs, indexing='ij')\n",
    "        base_grid = T.stack((grid_x, grid_y), dim=-1).unsqueeze(0).expand(B, -1, -1, -1)\n",
    "\n",
    "        # Normalize offsets\n",
    "        norm_factor = T.tensor(\n",
    "            [2.0 / max(W2 - 1, 1), 2.0 / max(H2 - 1, 1)],\n",
    "            device=device\n",
    "        ).view(1, 2, 1, 1)\n",
    "        offsets_norm = offsets * norm_factor\n",
    "\n",
    "        # Apply sampling\n",
    "        sampling_grid = base_grid + offsets_norm.permute(0, 2, 3, 1)\n",
    "        x_up = F.grid_sample(\n",
    "            x_interp, sampling_grid,\n",
    "            mode='bilinear',\n",
    "            padding_mode='border',\n",
    "            align_corners=True\n",
    "        )\n",
    "\n",
    "        return x_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c92aa391-8453-4ca1-86d9-488b5db790da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_channels: int,\n",
    "                 skip_channels: list,\n",
    "                 decoder_channels: list,\n",
    "                 coord_reduction: int = 4,\n",
    "                 use_feat_transform: bool = False):\n",
    "        super().__init__()\n",
    "        assert len(skip_channels) == 3 and len(decoder_channels) == 3\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        in_ch = feature_channels  # Start with E4 output (e.g., 512 or 1024)\n",
    "\n",
    "        for i in range(3):\n",
    "            skip_ch = skip_channels[i]  # Use in natural E4→E3→E2 order\n",
    "            out_ch = decoder_channels[i]\n",
    "            concat_ch = in_ch + skip_ch\n",
    "\n",
    "            block = nn.ModuleDict({\n",
    "                'fuse_skip': GhostModuleV2(concat_ch, out_ch, kernel_size=1, use_attn=False),\n",
    "                'aspp': GhostASPPFELAN(in_channels=out_ch, out_channels=out_ch),\n",
    "                'coord': CoordAttention(channels=out_ch, reduction=coord_reduction),\n",
    "                'upsample': DySample(in_channels=out_ch, scale=2, use_feat_transform=use_feat_transform)\n",
    "            })\n",
    "\n",
    "            self.stages.append(block)\n",
    "            in_ch = out_ch  # Output becomes input for next stage\n",
    "\n",
    "    def forward(self, x, skips):\n",
    "        # skips must be [E4, E3, E2] — highest to lowest\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            skip = skips[i]  # Correct: access E4, then E3, then E2\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "            x = T.cat([x, skip], dim=1)\n",
    "            x = stage['fuse_skip'](x)\n",
    "            x = stage['aspp'](x)\n",
    "            x = stage['coord'](x)\n",
    "            x = stage['upsample'](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69880fa-0804-47e6-b80e-b0df9a721751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtOutputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Separate output layer that takes decoder features at 1/4 resolution\n",
    "    and produces final segmentation masks at full input resolution.\n",
    "    \n",
    "    Supports multiple upsampling strategies:\n",
    "    - Learnable transpose convolution (ConvTranspose2d)\n",
    "    - Bilinear interpolation + refinement conv\n",
    "    - DySample-based learnable upsampling\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 num_classes: int,\n",
    "                 upsampling_method: str = 'transpose',\n",
    "                 use_ghost_conv: bool = False,\n",
    "                 refinement_layers: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_channels:        channels from final decoder stage\n",
    "          num_classes:        number of segmentation classes\n",
    "          upsampling_method:  'transpose', 'bilinear', 'dysample'\n",
    "          use_ghost_conv:     whether to use GhostModuleV2 for final conv\n",
    "          refinement_layers:  number of refinement convs after upsampling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.upsampling_method = upsampling_method\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Upsampling strategies\n",
    "        if upsampling_method == 'transpose':\n",
    "            # Learnable 4× upsampling via transpose convolution\n",
    "            self.upsample = nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=in_channels,\n",
    "                kernel_size=4,\n",
    "                stride=4,\n",
    "                padding=0,\n",
    "                bias=False\n",
    "            )\n",
    "            \n",
    "        elif upsampling_method == 'bilinear':\n",
    "            # Fixed bilinear interpolation\n",
    "            self.upsample = lambda x: F.interpolate(\n",
    "                x, scale_factor=4, \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            \n",
    "        elif upsampling_method == 'dysample':\n",
    "            # Content-aware DySample upsampling\n",
    "            self.upsample = DySample(\n",
    "                in_channels=in_channels,\n",
    "                scale=4,  # 4× upsampling in one step\n",
    "                use_feat_transform=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown upsampling method: {upsampling_method}\")\n",
    "        \n",
    "        # Optional refinement layers\n",
    "        refinement = []\n",
    "        for i in range(refinement_layers):\n",
    "            if use_ghost_conv:\n",
    "                refinement.append(GhostModuleV2(\n",
    "                    in_channels, in_channels,\n",
    "                    kernel_size=3, use_attn=False\n",
    "                ))\n",
    "            else:\n",
    "                refinement.append(nn.Conv2d(\n",
    "                    in_channels, in_channels,\n",
    "                    kernel_size=3, padding=1, bias=False\n",
    "                ))\n",
    "            refinement.append(nn.BatchNorm2d(in_channels))\n",
    "            refinement.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "        self.refinement = nn.Sequential(*refinement)\n",
    "        \n",
    "        # Final classification layer\n",
    "        if use_ghost_conv:\n",
    "            self.classifier = GhostModuleV2(\n",
    "                in_channels, num_classes,\n",
    "                kernel_size=1, use_attn=False\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Conv2d(\n",
    "                in_channels, num_classes,\n",
    "                kernel_size=1, bias=True\n",
    "            )\n",
    "\n",
    "    def forward(self, x: T.Tensor, target_size: tuple = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (B, C, H/4, W/4) decoder output at 1/4 resolution\n",
    "          target_size: (H, W) target output size, if None uses 4×input size\n",
    "        Returns:\n",
    "          logits: (B, num_classes, H, W) at full resolution\n",
    "        \"\"\"\n",
    "        # Upsample to full resolution\n",
    "        if self.upsampling_method == 'bilinear':\n",
    "            if target_size is not None:\n",
    "                x = F.interpolate(x, size=target_size, \n",
    "                                mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                x = self.upsample(x)\n",
    "        else:\n",
    "            x = self.upsample(x)\n",
    "            \n",
    "        # Apply refinement layers\n",
    "        x = self.refinement(x)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        # Ensure exact target size if specified\n",
    "        if target_size is not None and logits.shape[-2:] != target_size:\n",
    "            logits = F.interpolate(logits, size=target_size,\n",
    "                                 mode='bilinear', align_corners=False)\n",
    "            \n",
    "        return logits\n",
    "\n",
    "# Example usage:\n",
    "def create_asppnext_with_separate_output(feature_channels=512,\n",
    "                                       skip_channels=[128, 256, 512],\n",
    "                                       decoder_channels=[256, 128, 64],\n",
    "                                       num_classes=21,\n",
    "                                       upsampling_method='transpose'):\n",
    "    \"\"\"\n",
    "    Factory function to create decoder + output layer\n",
    "    \"\"\"\n",
    "    decoder = ASPPNeXtDecoder(\n",
    "        feature_channels=feature_channels,\n",
    "        skip_channels=skip_channels,\n",
    "        decoder_channels=decoder_channels\n",
    "    )\n",
    "    \n",
    "    output_layer = ASPPNeXtOutputLayer(\n",
    "        in_channels=decoder_channels[-1],  # Final decoder stage channels\n",
    "        num_classes=num_classes,\n",
    "        upsampling_method=upsampling_method,\n",
    "        use_ghost_conv=False,  # Set to True to experiment with GhostModuleV2\n",
    "        refinement_layers=2\n",
    "    )\n",
    "    \n",
    "    return decoder, output_layer\n",
    "\n",
    "# Complete forward pass example:\n",
    "# decoder, output_layer = create_asppnext_with_separate_output()\n",
    "# \n",
    "# # From your pipeline:\n",
    "# decoder_features = decoder(post_daaf_output, encoder_skips)  # (B, 64, 128, 96)\n",
    "# final_logits = output_layer(decoder_features, target_size=(384, 512))  # (B, 21, 384, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b166c0af-fc53-4aa1-9d9d-506b561da4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_ch_rgb: int = 3,\n",
    "                 in_ch_depth: int = 1,\n",
    "                 base_dim: int = 64,\n",
    "                 num_heads: tuple = (4, 8, 16, 32),\n",
    "                 window_sizes: tuple = (8, 4, 2, 1),\n",
    "                 mlp_ratio: int = 4,\n",
    "                 skip_channels: list = None,\n",
    "                 decoder_channels: list = None,\n",
    "                 num_classes: int = 21,\n",
    "                 coord_reduction: int = 4,\n",
    "                 output_upsample: str = 'transpose',\n",
    "                 use_feat_transform: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Dual Encoders (RGB and Depth)\n",
    "        self.rgb_encoder = ASPPNeXtEncoder(\n",
    "            in_ch=in_ch_rgb,\n",
    "            base_dim=base_dim,\n",
    "            num_heads=num_heads,\n",
    "            window_sizes=window_sizes,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "\n",
    "        self.depth_encoder = ASPPNeXtEncoder(\n",
    "            in_ch=in_ch_depth,\n",
    "            base_dim=base_dim,\n",
    "            num_heads=num_heads,\n",
    "            window_sizes=window_sizes,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "\n",
    "        enc_out_ch = base_dim * 2 ** 3  # Final encoder stage channels (E4 = 512)\n",
    "\n",
    "        # 2) PreDAAF for RGB and Depth independently\n",
    "        self.rgb_predaaf = PreDAAFGhostV2(channels=enc_out_ch, reduction=4, ratio=2)\n",
    "        self.depth_predaaf = PreDAAFGhostV2(channels=enc_out_ch, reduction=4, ratio=2)\n",
    "\n",
    "        # 3) DAAF Fusion\n",
    "        self.daaf = DAAFBlock(channels=enc_out_ch, num_heads=num_heads[-1])\n",
    "\n",
    "        # 4) PostDAAF Enhancement\n",
    "        self.post_daaf = PostDAAFGhostV2(channels=enc_out_ch, reduction=4, ratio=2)\n",
    "\n",
    "        # 5) Decoder\n",
    "        if skip_channels is None:\n",
    "            # ✅ Fixed order: E4, E3, E2 → 512, 256, 128\n",
    "            skip_channels = [base_dim * 2 ** i for i in reversed(range(1, 4))]  # [512, 256, 128]\n",
    "        if decoder_channels is None:\n",
    "            decoder_channels = [base_dim * 2 ** i for i in reversed(range(3))]  # [256, 128, 64]\n",
    "\n",
    "        self.decoder = ASPPNeXtDecoder(\n",
    "            feature_channels=enc_out_ch,\n",
    "            skip_channels=skip_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            coord_reduction=coord_reduction,\n",
    "            use_feat_transform=use_feat_transform\n",
    "        )\n",
    "\n",
    "        # 6) Output Layer\n",
    "        self.output = ASPPNeXtOutputLayer(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            num_classes=num_classes,\n",
    "            upsampling_method=output_upsample,\n",
    "            use_ghost_conv=False,\n",
    "            refinement_layers=2\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb: T.Tensor, depth: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        Full forward pass:\n",
    "        encoder → pre-DAAF (per stream) → DAAF → post-DAAF → decoder → output\n",
    "        \"\"\"\n",
    "        # Step 1: Dual Encoding\n",
    "        _, rgb_feats = self.rgb_encoder(rgb)\n",
    "        _, depth_feats = self.depth_encoder(depth)\n",
    "\n",
    "        # Step 2: PreDAAF on each feature stream\n",
    "        rgb_refined = self.rgb_predaaf(rgb_feats[-1])\n",
    "        depth_refined = self.depth_predaaf(depth_feats[-1])\n",
    "\n",
    "        # Step 3: DAAF Fusion\n",
    "        fused = self.daaf(rgb_refined, depth_refined)\n",
    "\n",
    "        # Step 4: Post-DAAF Enhancement\n",
    "        enhanced = self.post_daaf(fused)\n",
    "\n",
    "        # Step 5: Decode (using RGB skip connections, [E4, E3, E2])\n",
    "        decoder_feats = self.decoder(enhanced, rgb_feats[-1:-4:-1])  # ✅ reversed skip list\n",
    "\n",
    "        # Step 6: Final Classification\n",
    "        logits = self.output(decoder_feats, target_size=rgb.shape[2:])  # Restore original res\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2539a3f9-ec54-418a-a964-8c7d37869bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset loaded with 1600 samples\n",
      "val dataset loaded with 352 samples\n",
      "test dataset loaded with 1200 samples\n"
     ]
    }
   ],
   "source": [
    "class ASPPNeXtDataset(Dataset):\n",
    "    def __init__(self, image_dir, depth_dir, mask_dir, window_size=8):\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.image_files = sorted(\n",
    "            [f for f in os.listdir(image_dir) if f.endswith('.jpg')],\n",
    "            key=lambda x: int(x.split('_')[0])\n",
    "        )\n",
    "\n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"No JPG files found in {image_dir}\")\n",
    "\n",
    "        self.depth_files = [f.replace('.jpg', '.png').replace('image', 'image_depth') for f in self.image_files]\n",
    "        self.mask_files = [f.replace('.jpg', '.png') for f in self.image_files]\n",
    "\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in self.image_files]\n",
    "        self.depth_paths = [os.path.join(depth_dir, f) for f in self.depth_files]\n",
    "        self.mask_paths = [os.path.join(mask_dir, f) for f in self.mask_files]\n",
    "\n",
    "        for p in self.image_paths + self.depth_paths + self.mask_paths:\n",
    "            if not os.path.exists(p):\n",
    "                raise FileNotFoundError(f\"File {p} does not exist\")\n",
    "\n",
    "    def pad_to_square_multiple(self, arr, multiple=8):\n",
    "        h, w = arr.shape[:2]\n",
    "        max_dim = max(h, w)\n",
    "        final_dim = ((max_dim + multiple - 1) // multiple) * multiple\n",
    "        pad_h = final_dim - h\n",
    "        pad_w = final_dim - w\n",
    "    \n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "    \n",
    "        if arr.ndim == 3:\n",
    "            return np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            return np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.image_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        depth = cv2.imread(self.depth_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = self.pad_to_square_multiple(img, self.window_size)\n",
    "        depth = self.pad_to_square_multiple(depth, self.window_size)\n",
    "        mask = self.pad_to_square_multiple(mask, self.window_size)\n",
    "\n",
    "        return (\n",
    "            T.from_numpy(img).float().permute(2, 0, 1) / 255.0,  # [3, H, W]\n",
    "            T.from_numpy(depth).float().unsqueeze(0),            # [1, H, W]\n",
    "            T.from_numpy(mask).long()                            # [H, W]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "# ------------------ Dataloader Setup ------------------\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Base directories\n",
    "base_img = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\"\n",
    "base_depth = os.path.join(current_dir, \"Depth_Features\")\n",
    "base_mask = os.path.join(current_dir, \"Weed_Masks\")\n",
    "\n",
    "for dir_path in [base_img, base_depth, base_mask]:\n",
    "    if not os.path.isdir(dir_path):\n",
    "        raise FileNotFoundError(f\"Directory {dir_path} does not exist\")\n",
    "\n",
    "# Dataset splits\n",
    "splits = {\n",
    "    \"train\": (\"train_new\", \"train_new\", \"train\"),\n",
    "    \"val\": (\"validation_new\", \"validation_new\", \"val\"),\n",
    "    \"test\": (\"test_new\", \"test_new\", \"test\"),\n",
    "}\n",
    "\n",
    "# Match window size to model's max attention window size\n",
    "model_window_sizes = (8, 4, 2, 1)\n",
    "window_size = max(model_window_sizes)  # -> 8\n",
    "\n",
    "dataloaders = {}\n",
    "batch_size = 4\n",
    "\n",
    "for phase, (img_s, depth_s, mask_s) in splits.items():\n",
    "    img_dir = os.path.join(base_img, img_s)\n",
    "    depth_dir = os.path.join(base_depth, depth_s)\n",
    "    mask_dir = os.path.join(base_mask, mask_s)\n",
    "\n",
    "    for d in [img_dir, depth_dir, mask_dir]:\n",
    "        if not os.path.isdir(d):\n",
    "            raise FileNotFoundError(f\"Directory {d} does not exist\")\n",
    "\n",
    "    try:\n",
    "        ds = ASPPNeXtDataset(\n",
    "            image_dir=img_dir,\n",
    "            depth_dir=depth_dir,\n",
    "            mask_dir=mask_dir,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        print(f\"{phase} dataset loaded with {len(ds)} samples\")\n",
    "\n",
    "        dataloaders[phase] = DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(phase == \"train\"),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {phase} dataset: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18f1e8a-6562-4fba-b4ea-f9ab72f14c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, targets, num_classes=2):\n",
    "    \"\"\"\n",
    "    Computes all metrics for a batch\n",
    "    Returns: dict of {\n",
    "        'miou', 'weed_iou', 'mPA', 'accuracy', \n",
    "        'precision', 'recall', 'f1', 'fnr'\n",
    "    }\n",
    "    \"\"\"\n",
    "    preds = preds.argmax(1)\n",
    "    mask = (targets >= 0) & (targets < num_classes)\n",
    "    hist = T.bincount(\n",
    "        num_classes * targets[mask] + preds[mask],\n",
    "        minlength=num_classes**2\n",
    "    ).reshape(num_classes, num_classes).float()\n",
    "    \n",
    "    tp = T.diag(hist)\n",
    "    fp = hist.sum(0) - tp\n",
    "    fn = hist.sum(1) - tp\n",
    "    \n",
    "    metrics = {\n",
    "        'miou': T.mean(tp / (tp + fp + fn + 1e-10)).item(),\n",
    "        'weed_iou': (tp[1] / (tp[1] + fp[1] + fn[1] + 1e-10)).item(),\n",
    "        'mPA': T.mean(tp / (tp + fp + 1e-10)).item(),\n",
    "        'accuracy': (tp.sum() / hist.sum()).item(),\n",
    "        'precision': (tp[1] / (tp[1] + fp[1] + 1e-10)).item(),\n",
    "        'recall': (tp[1] / (tp[1] + fn[1] + 1e-10)).item(),\n",
    "        'f1': (2 * tp[1] / (2 * tp[1] + fp[1] + fn[1] + 1e-10)).item(),\n",
    "        'fnr': (fn[1] / (fn[1] + tp[1] + 1e-10)).item()\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26bcf88d-90ee-4018-8c22-c782b345d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASPPNeXtModel(\n",
    "    in_ch_rgb=3,\n",
    "    in_ch_depth=1,\n",
    "    base_dim=64,\n",
    "    num_heads=(4,8,16,32),\n",
    "    window_sizes=(16,8,4,2),\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    output_upsample='dysample',\n",
    "    use_feat_transform=True           # <-- ✅ control GhostModule or Conv1x1\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd1ec84a-1489-48c8-8dad-19b72b431930",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = T.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=6e-5,          # Base learning rate\n",
    "    weight_decay=0.01, # L2 regularization strength\n",
    "    betas=(0.9, 0.999) # Momentum parameters:\n",
    "                       # - beta1: gradient moving average decay (0.9)\n",
    "                       # - beta2: squared gradient moving average decay (0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbbaa26b-07d3-4b6c-9bc7-fed3c4915697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3, gamma=0.75, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "        self.eps = 1e-8  # Numerical stability\n",
    "\n",
    "    def update_hyperparams(self, epoch):\n",
    "        \"\"\"Dynamically adjust hyperparameters every 5 epochs\"\"\"\n",
    "        steps = epoch // 5\n",
    "        self.alpha = max(0.4, 0.7 - 0.03*steps)  # Linearly decrease\n",
    "        self.beta = min(0.6, 0.3 + 0.03*steps)   # Complementary to alpha\n",
    "        self.gamma = min(1.5, 0.5 + 0.1*steps)   # Gradually increase focus\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # Convert targets to one-hot\n",
    "        targets_one_hot = F.one_hot(targets.clamp(0,1), num_classes=2).permute(0,3,1,2).float()\n",
    "        \n",
    "        # Softmax probabilities\n",
    "        probs = F.softmax(preds, dim=1)\n",
    "        \n",
    "        # Calculate components\n",
    "        TP = (probs * targets_one_hot).sum((0,2,3))  # True Positives\n",
    "        FP = (probs * (1-targets_one_hot)).sum((0,2,3))  # False Positives\n",
    "        FN = ((1-probs) * targets_one_hot).sum((0,2,3))  # False Negatives\n",
    "        \n",
    "        # Tversky index per class\n",
    "        tversky = (TP + self.smooth) / (TP + self.alpha*FP + self.beta*FN + self.smooth)\n",
    "        \n",
    "        # Focal Tversky loss\n",
    "        return T.mean(T.pow((1 - tversky), self.gamma))\n",
    "\n",
    "loss_fn = FocalTverskyLoss(alpha=0.7, beta=0.3, gamma=0.75).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0908b4cd-4766-4e13-8103-aec2dcc97be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model, filename):\n",
    "    dummy_rgb   = T.randn(1, 3, 384, 512).to(device)\n",
    "    dummy_depth = T.randn(1, 1, 384, 512).to(device)\n",
    "\n",
    "    T.onnx.export(\n",
    "        model,\n",
    "        (dummy_rgb, dummy_depth),\n",
    "        filename,\n",
    "        export_params=True,\n",
    "        opset_version=13,  # For TensorRT compatibility\n",
    "        do_constant_folding=True,\n",
    "        input_names=['rgb_input', 'depth_input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'rgb_input': {0: 'batch_size'},\n",
    "            'depth_input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    print(f\"Exported {filename} successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f47e552c-44cf-4100-ad99-5b5281caf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine_from_onnx(onnx_filepath: str,\n",
    "                           engine_filepath: str,\n",
    "                           max_workspace_size: int = 1 << 30) -> None:\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    network = builder.create_network(\n",
    "        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    )\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "    with open(onnx_filepath, 'rb') as model_f:\n",
    "        if not parser.parse(model_f.read()):\n",
    "            for i in range(parser.num_errors):\n",
    "                print(parser.get_error(i))\n",
    "            raise RuntimeError(\"Failed to parse ONNX model\")\n",
    "\n",
    "    config = builder.create_builder_config()\n",
    "    config.max_workspace_size = max_workspace_size\n",
    "    config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "    engine = builder.build_engine(network, config)\n",
    "    if engine is None:\n",
    "        raise RuntimeError(\"Failed to build the TensorRT engine\")\n",
    "\n",
    "    with open(engine_filepath, 'wb') as f:\n",
    "        f.write(engine.serialize())\n",
    "\n",
    "    print(f\"✅ Serialized TRT engine to {engine_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2de0cc14-0dec-4e55-a53f-2d0018cecd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=50):\n",
    "    best_mPA = 0.0\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs+1), desc=\"Epochs\", leave=True):\n",
    "        loss_fn.update_hyperparams(epoch)\n",
    "\n",
    "        # -------- Training Phase --------\n",
    "        model.train()\n",
    "        train_loader = dataloaders['train']\n",
    "        train_pbar = tqdm(train_loader,\n",
    "                          desc=f\"  Train (E{epoch})\",\n",
    "                          leave=False,\n",
    "                          unit=\"batch\")\n",
    "        for img, depth, mask in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img.to(device), depth.to(device))\n",
    "            loss = loss_fn(outputs, mask.to(device))\n",
    "            loss.backward()\n",
    "            T.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        T.cuda.empty_cache()\n",
    "\n",
    "        # -------- Validation Phase --------\n",
    "        model.eval()\n",
    "        val_metrics = {'mPA': 0.0, 'accuracy': 0.0}\n",
    "        val_loader = dataloaders['val']\n",
    "        val_pbar = tqdm(val_loader,\n",
    "                        desc=f\"  Val   (E{epoch})\",\n",
    "                        leave=False,\n",
    "                        unit=\"batch\")\n",
    "        with T.no_grad():\n",
    "            for img, depth, mask in val_pbar:\n",
    "                outputs = model(img.to(device), depth.to(device))\n",
    "                m = compute_metrics(outputs, mask.to(device))\n",
    "                val_metrics['mPA']      += m['mPA']\n",
    "                val_metrics['accuracy'] += m['accuracy']\n",
    "\n",
    "        num_val = len(val_loader)\n",
    "        val_metrics = {k: v / num_val for k, v in val_metrics.items()}\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} — \"\n",
    "              f\"Train Loss: {loss.item():.4f} | \"\n",
    "              f\"Val mPA: {val_metrics['mPA']:.4f} | \"\n",
    "              f\"Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "        # -------- Checkpoint & Engine Build --------\n",
    "        if val_metrics['mPA'] > best_mPA:\n",
    "            best_mPA = val_metrics['mPA']\n",
    "            T.save(model.state_dict(), os.path.join(MODELS_DIR,'best_mPA_model.pth'))\n",
    "            export_to_onnx(model, os.path.join(MODELS_DIR,'best_mPA_model.onnx'))\n",
    "            build_engine_from_onnx(os.path.join(MODELS_DIR,'best_mPA_model.onnx'),\n",
    "                                   os.path.join(MODELS_DIR,'best_mPA_model.engine'))\n",
    "\n",
    "        if val_metrics['accuracy'] > best_accuracy:\n",
    "            best_accuracy = val_metrics['accuracy']\n",
    "            T.save(model.state_dict(), os.path.join(MODELS_DIR,'best_accuracy_model.pth'))\n",
    "            export_to_onnx(model, os.path.join(MODELS_DIR,'best_accuracy_model.onnx'))\n",
    "            build_engine_from_onnx(os.path.join(MODELS_DIR,'best_accuracy_model.onnx'),\n",
    "                                   os.path.join(MODELS_DIR,'best_accuracy_model.engine'))\n",
    "\n",
    "        T.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38520630-315b-4403-aca5-7e577e322f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f00288174894c84a8c356a375535597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502250cc27934d379c61f38289df8b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Train (E1):   0%|          | 0/400 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.00 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(num_epochs)\u001b[39m\n\u001b[32m     17\u001b[39m outputs = model(img.to(device), depth.to(device))\n\u001b[32m     18\u001b[39m loss = loss_fn(outputs, mask.to(device))\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m T.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     21\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Python\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Python\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Python\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.00 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8789e47-7a06-4d50-97a4-db6b465855f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HostDeviceMem:\n",
    "    \"\"\"Simple holder for host/device buffers.\"\"\"\n",
    "    def __init__(self, host_mem, device_mem, name: str):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "        self.name = name\n",
    "\n",
    "class TRTInfer:\n",
    "    \"\"\"\n",
    "    Wrapper for TensorRT inference. Allocates proper CUDA buffers\n",
    "    and manages bindings by name.\n",
    "    \"\"\"\n",
    "    def __init__(self, engine_path: str, batch_size: int = 1):\n",
    "        self.batch_size = batch_size\n",
    "        self.logger = trt.Logger(trt.Logger.ERROR)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        self.engine = self.runtime.deserialize_cuda_engine(engine_data)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "        self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers()\n",
    "\n",
    "    def _allocate_buffers(self):\n",
    "        inputs, outputs, bindings = [], [], []\n",
    "        stream = cuda.Stream()\n",
    "\n",
    "        for binding in self.engine:\n",
    "            shape = tuple(self.engine.get_binding_shape(binding))\n",
    "            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n",
    "            size = self.batch_size * int(trt.volume(shape))\n",
    "\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            if self.engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem, binding))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem, binding))\n",
    "\n",
    "        return inputs, outputs, bindings, stream\n",
    "\n",
    "    def infer(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Run inference.\n",
    "        input_batch: np.ndarray of shape (batch_size, C, H, W), dtype matching engine.\n",
    "        Returns the first (or only) output as a numpy array.\n",
    "        \"\"\"\n",
    "        # Copy input to host buffer, then to device\n",
    "        np.copyto(self.inputs[0].host, input_batch.ravel())\n",
    "        cuda.memcpy_htod_async(self.inputs[0].device,\n",
    "                               self.inputs[0].host, self.stream)\n",
    "\n",
    "        # Set all binding addresses\n",
    "        for idx, binding_ptr in enumerate(self.bindings):\n",
    "            name = self.engine.get_binding_name(idx)\n",
    "            self.context.set_tensor_address(name, binding_ptr)\n",
    "\n",
    "        # Execute asynchronously\n",
    "        self.context.execute_async_v3(stream_handle=self.stream.handle)\n",
    "\n",
    "        # Retrieve outputs\n",
    "        for out in self.outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream)\n",
    "        self.stream.synchronize()\n",
    "\n",
    "        # Reshape and return\n",
    "        out_binding = self.outputs[0]\n",
    "        out_shape = (self.batch_size,) + tuple(\n",
    "            self.engine.get_binding_shape(out_binding.name)\n",
    "        )\n",
    "        return out_binding.host.reshape(out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011cff1-a5f7-4c7d-b19e-2a0947e7bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRTInfer:\n",
    "    \"\"\"\n",
    "    Wrapper for TensorRT inference. Allocates proper CUDA buffers\n",
    "    and manages bindings by name.\n",
    "    \"\"\"\n",
    "    def __init__(self, engine_path: str, batch_size: int = 1):\n",
    "        self.batch_size = batch_size\n",
    "        self.logger = trt.Logger(trt.Logger.ERROR)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        self.engine = self.runtime.deserialize_cuda_engine(engine_data)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "        self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers()\n",
    "\n",
    "    def _allocate_buffers(self):\n",
    "        inputs, outputs, bindings = [], [], []\n",
    "        stream = cuda.Stream()\n",
    "\n",
    "        for binding in self.engine:\n",
    "            shape = tuple(self.engine.get_binding_shape(binding))\n",
    "            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n",
    "            size = self.batch_size * int(trt.volume(shape))\n",
    "\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            if self.engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem, binding))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem, binding))\n",
    "\n",
    "        return inputs, outputs, bindings, stream\n",
    "\n",
    "    def infer(self, input_batches):\n",
    "        \"\"\"\n",
    "        Run inference.\n",
    "\n",
    "        Args:\n",
    "            input_batches: tuple or list of np.ndarray, one per input binding.\n",
    "                           Each array shape should be (batch_size, C, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            The first (or only) output as a numpy array.\n",
    "        \"\"\"\n",
    "        # Copy each input to host buffer, then to device\n",
    "        for inp, buf in zip(input_batches, self.inputs):\n",
    "            np.copyto(buf.host, inp.ravel())\n",
    "            cuda.memcpy_htod_async(buf.device, buf.host, self.stream)\n",
    "\n",
    "        # Set all binding addresses\n",
    "        for idx, binding_ptr in enumerate(self.bindings):\n",
    "            name = self.engine.get_binding_name(idx)\n",
    "            self.context.set_tensor_address(name, binding_ptr)\n",
    "\n",
    "        # Execute asynchronously\n",
    "        self.context.execute_async_v3(stream_handle=self.stream.handle)\n",
    "\n",
    "        # Retrieve outputs\n",
    "        for out in self.outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream)\n",
    "        self.stream.synchronize()\n",
    "\n",
    "        # Reshape and return first output\n",
    "        out_binding = self.outputs[0]\n",
    "        out_shape = (self.batch_size,) + tuple(\n",
    "            self.engine.get_binding_shape(out_binding.name)\n",
    "        )\n",
    "        return out_binding.host.reshape(out_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed1a2c-4c55-4ba0-b3f1-7b4b887fc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(dataloaders, device, batch_size=4, engine_dir=MODELS_DIR):\n",
    "    # Instantiate TensorRT inference wrappers\n",
    "    trt_inf_mpa = TRTInfer(f'{engine_dir}/best_mPA_model.engine', batch_size=batch_size)\n",
    "    trt_inf_acc = TRTInfer(f'{engine_dir}/best_accuracy_model.engine', batch_size=batch_size)\n",
    "\n",
    "    # Prepare metric accumulators\n",
    "    test_metrics_mpa = {'mPA':0.0, 'accuracy':0.0, 'miou':0.0,\n",
    "                        'weed_iou':0.0, 'precision':0.0, 'recall':0.0}\n",
    "    test_metrics_acc = {'mPA':0.0, 'accuracy':0.0, 'miou':0.0,\n",
    "                        'weed_iou':0.0, 'precision':0.0, 'recall':0.0}\n",
    "\n",
    "    with T.no_grad():\n",
    "        for img, depth, mask in dataloaders['test']:\n",
    "            # Convert to NumPy float32\n",
    "            in_rgb   = img.cpu().numpy().astype(np.float32)\n",
    "            in_depth = depth.cpu().numpy().astype(np.float32)\n",
    "\n",
    "            # Perform inference with both inputs\n",
    "            out_mpa = trt_inf_mpa.infer((in_rgb, in_depth))  # (B, num_classes, H, W)\n",
    "            out_acc = trt_inf_acc.infer((in_rgb, in_depth))\n",
    "\n",
    "            # Convert outputs to PyTorch tensors on GPU\n",
    "            preds_mpa = T.from_numpy(out_mpa).to(device)\n",
    "            preds_acc = T.from_numpy(out_acc).to(device)\n",
    "\n",
    "            # Compute batch metrics\n",
    "            metrics_mpa = compute_metrics(preds_mpa, mask.to(device))\n",
    "            metrics_acc = compute_metrics(preds_acc, mask.to(device))\n",
    "\n",
    "            # Accumulate metrics\n",
    "            for k in test_metrics_mpa:\n",
    "                test_metrics_mpa[k] += metrics_mpa[k]\n",
    "                test_metrics_acc[k] += metrics_acc[k]\n",
    "\n",
    "    # Average over batches\n",
    "    num_batches = len(dataloaders['test'])\n",
    "    for k in test_metrics_mpa:\n",
    "        test_metrics_mpa[k] /= num_batches\n",
    "        test_metrics_acc[k] /= num_batches\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTensorRT mPA-Optimized Model Test Results:\")\n",
    "    for k, v in test_metrics_mpa.items():\n",
    "        print(f\"{k:>10}: {v:.4f}\")\n",
    "\n",
    "    print(\"\\nTensorRT Accuracy-Optimized Model Test Results:\")\n",
    "    for k, v in test_metrics_acc.items():\n",
    "        print(f\"{k:>10}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587df3c-0d13-4ae8-82ca-501d6b188bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models(dataloaders, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
