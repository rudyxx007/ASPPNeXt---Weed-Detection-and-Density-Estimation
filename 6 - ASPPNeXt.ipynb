{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aea3a0-ce64-45ec-ac09-1cfef823c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ğŸ”§ 1. Ghost MLP Block (ConvNeXt-style)\n",
    "# -----------------------------\n",
    "class GhostMLPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, expansion_ratio=4):\n",
    "        super(GhostMLPBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expansion_ratio\n",
    "        self.fc1 = GhostModule(in_channels, hidden_dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = GhostModule(hidden_dim, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ”§ 2. Modified ConvNeXt-Attention Block\n",
    "# -----------------------------\n",
    "class HybridConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, channels, attention_module):\n",
    "        super(HybridConvNeXtBlock, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size=7, padding=3, groups=channels)  # Positional encoding\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.attn = attention_module(channels)  # Could be MHSA, Axial, etc.\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.mlp = GhostMLPBlock(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = x + residual.permute(0, 2, 3, 1)\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x.permute(0, 3, 1, 2))\n",
    "        x = x.permute(0, 2, 3, 1) + residual\n",
    "        return x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ”§ 3. Ghost CoordAttention Block\n",
    "# -----------------------------\n",
    "class GhostCoordAttention(nn.Module):\n",
    "    def __init__(self, inp, reduction=32):\n",
    "        super(GhostCoordAttention, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mid = max(8, inp // reduction)\n",
    "        self.conv1 = GhostModule(inp, mid, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(mid)\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv_h = GhostModule(mid, inp, kernel_size=1)\n",
    "        self.conv_w = GhostModule(mid, inp, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n, c, h, w = x.size()\n",
    "\n",
    "        x_h = self.pool_h(x).permute(0, 1, 3, 2)\n",
    "        x_w = self.pool_w(x)\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y)\n",
    "\n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_h = self.conv_h(x_h.permute(0, 1, 3, 2))\n",
    "        x_w = self.conv_w(x_w)\n",
    "\n",
    "        out = identity * torch.sigmoid(x_h) * torch.sigmoid(x_w)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ”§ 4. Ghost ASPPFELAN Block (Simplified)\n",
    "# -----------------------------\n",
    "class GhostASPPFELAN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GhostASPPFELAN, self).__init__()\n",
    "        self.branch1 = GhostModule(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch2 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        self.branch3 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=4, padding=4)\n",
    "        self.fuse = GhostModule(out_channels * 3, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        x = torch.cat([b1, b2, b3], dim=1)\n",
    "        x = self.fuse(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ—ï¸ Full Architecture (Pseudo)\n",
    "# -----------------------------\n",
    "class ASPPNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ASPPNeXt, self).__init__()\n",
    "        self.encoder_stages = nn.ModuleList([\n",
    "            HybridConvNeXtBlock(96, attention_module),\n",
    "            HybridConvNeXtBlock(192, attention_module),\n",
    "            HybridConvNeXtBlock(384, attention_module),\n",
    "            HybridConvNeXtBlock(768, attention_module)\n",
    "        ])\n",
    "\n",
    "        self.bottleneck = DAAFModule(768)  # Assume DAAFModule is implemented\n",
    "\n",
    "        self.decoder_stages = nn.ModuleList([\n",
    "            GhostASPPFELAN(768, 384),\n",
    "            GhostASPPFELAN(384, 192),\n",
    "            GhostASPPFELAN(192, 96)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList([\n",
    "            GhostCoordAttention(384),\n",
    "            GhostCoordAttention(192),\n",
    "            GhostCoordAttention(96)\n",
    "        ])\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(96, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_feats = []\n",
    "        for stage in self.encoder_stages:\n",
    "            x = stage(x)\n",
    "            enc_feats.append(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i in range(3):\n",
    "            x = self.upsample(x)\n",
    "            x = x + enc_feats[-(i+2)]  # Skip connection\n",
    "            x = self.decoder_stages[i](x)\n",
    "            x = self.attentions[i](x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Note: GhostModule and DAAFModule need to be implemented or imported\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c133a51c-fd32-48e9-afa1-8098e6d1e257",
   "metadata": {},
   "source": [
    "--------------------xX GhostModule Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  â†“\n",
    "Primary Features: \n",
    "  - 1Ã—1 Conv â†’ C_mid = C_out // ratio\n",
    "  â†“\n",
    "Ghost Features:\n",
    "  - Depthwise Conv (or cheap linear ops) on C_mid\n",
    "  â†“\n",
    "Concatenate Primary + Ghost â†’ C_out\n",
    "  â†“\n",
    "BatchNorm (optional)\n",
    "  â†“\n",
    "Output: (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c0e33b-905a-4c8c-a202-e860c4ae83a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--------------------xX Modified ConvNeXt Block with Self-Attention + Ghost MLP Xx--------------------\n",
    "\n",
    "Input: (B, C, H, W)\n",
    "  â†“\n",
    "7Ã—7 Depthwise Convolution (Positional Info)\n",
    "  â†“\n",
    "Permute to (B, H, W, C)\n",
    "  â†“\n",
    "LayerNorm\n",
    "  â†“\n",
    "Self-Attention Module (e.g. MHSA / Axial Attention)\n",
    "  â†“\n",
    "Residual Add\n",
    "  â†“\n",
    "LayerNorm\n",
    "  â†“\n",
    "Ghost MLP:\n",
    "    - GhostConv(1Ã—1, C â†’ 4C)\n",
    "    - GELU\n",
    "    - GhostConv(1Ã—1, 4C â†’ C)\n",
    "  â†“\n",
    "Residual Add\n",
    "  â†“\n",
    "Permute back to (B, C, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fb663-315e-4b3a-90e6-ed035184a557",
   "metadata": {},
   "source": [
    "--------------------xX Ghost ASPPFELAN Decoder Block Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  â†“\n",
    "Branch 1: GhostConv (1Ã—1)\n",
    "Branch 2: GhostConv (3Ã—3, dilation=2)\n",
    "Branch 3: GhostConv (3Ã—3, dilation=4)\n",
    "  â†“\n",
    "Concatenate all branches â†’ (B, 3Ã—C_out, H, W)\n",
    "  â†“\n",
    "GhostConv (1Ã—1) for fusion â†’ (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ab70a1c-1825-4e74-81b4-23fceb8604a0",
   "metadata": {},
   "source": [
    "--------------------xX Full Architecture â€“ ASPPNeXt Xx--------------------\n",
    "\n",
    "                    RGB Input        Depth Input\n",
    "                         â”‚                â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚                                                    â”‚\n",
    " â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    " â”‚ Encoder â”‚ â† Hybrid ConvNeXt Blocks (x4)            â”‚ Encoder   â”‚ â† Hybrid ConvNeXt Blocks (x4)\n",
    " â”‚ (RGB)   â”‚                                          â”‚ (Depth)   â”‚\n",
    " â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚                                                       â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€[ Feature Fusion ]â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â–¼                           â–¼\n",
    "                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                  â”‚      DAAF (RGB âŠ• Depth Fusion)    â”‚\n",
    "                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚ [Optional] GhostModule Pre/Post Bottleneck â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Decoder Stage 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚ Ghost ASPPFELAN + CoordAttention + Upsampleâ”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Decoder Stage 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚ Ghost ASPPFELAN + CoordAttention + Upsampleâ”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Decoder Stage 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚ Ghost ASPPFELAN + CoordAttention + Upsampleâ”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â†“\n",
    "                      Final 1Ã—1 Conv â†’ Output Map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
