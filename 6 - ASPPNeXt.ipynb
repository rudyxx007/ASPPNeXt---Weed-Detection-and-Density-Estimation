{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aea3a0-ce64-45ec-ac09-1cfef823c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ğŸ”§ 1. Ghost MLP Block (ConvNeXt-style)\n",
    "# -----------------------------\n",
    "class GhostMLPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, expansion_ratio=4):\n",
    "        super(GhostMLPBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expansion_ratio\n",
    "        self.fc1 = GhostModule(in_channels, hidden_dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = GhostModule(hidden_dim, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ”§ 2. Modified ConvNeXt-Attention Block\n",
    "# -----------------------------\n",
    "class HybridConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, channels, attention_module):\n",
    "        super(HybridConvNeXtBlock, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size=7, padding=3, groups=channels)  # Positional encoding\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.attn = attention_module(channels)  # Could be MHSA, Axial, etc.\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.mlp = GhostMLPBlock(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = x + residual.permute(0, 2, 3, 1)\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x.permute(0, 3, 1, 2))\n",
    "        x = x.permute(0, 2, 3, 1) + residual\n",
    "        return x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ”§ 3. Ghost CoordAttention Block\n",
    "# -----------------------------\n",
    "class GhostCoordAttention(nn.Module):\n",
    "    def __init__(self, inp, reduction=32):\n",
    "        super(GhostCoordAttention, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mid = max(8, inp // reduction)\n",
    "        self.conv1 = GhostModule(inp, mid, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(mid)\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv_h = GhostModule(mid, inp, kernel_size=1)\n",
    "        self.conv_w = GhostModule(mid, inp, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n, c, h, w = x.size()\n",
    "\n",
    "        x_h = self.pool_h(x).permute(0, 1, 3, 2)\n",
    "        x_w = self.pool_w(x)\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y)\n",
    "\n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_h = self.conv_h(x_h.permute(0, 1, 3, 2))\n",
    "        x_w = self.conv_w(x_w)\n",
    "\n",
    "        out = identity * torch.sigmoid(x_h) * torch.sigmoid(x_w)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ”§ 4. Ghost ASPPFELAN Block (Simplified)\n",
    "# -----------------------------\n",
    "class GhostASPPFELAN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GhostASPPFELAN, self).__init__()\n",
    "        self.branch1 = GhostModule(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch2 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        self.branch3 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=4, padding=4)\n",
    "        self.fuse = GhostModule(out_channels * 3, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        x = torch.cat([b1, b2, b3], dim=1)\n",
    "        x = self.fuse(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# ğŸ—ï¸ Full Architecture (Pseudo)\n",
    "# -----------------------------\n",
    "class ASPPNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ASPPNeXt, self).__init__()\n",
    "        self.encoder_stages = nn.ModuleList([\n",
    "            HybridConvNeXtBlock(96, attention_module),\n",
    "            HybridConvNeXtBlock(192, attention_module),\n",
    "            HybridConvNeXtBlock(384, attention_module),\n",
    "            HybridConvNeXtBlock(768, attention_module)\n",
    "        ])\n",
    "\n",
    "        self.bottleneck = DAAFModule(768)  # Assume DAAFModule is implemented\n",
    "\n",
    "        self.decoder_stages = nn.ModuleList([\n",
    "            GhostASPPFELAN(768, 384),\n",
    "            GhostASPPFELAN(384, 192),\n",
    "            GhostASPPFELAN(192, 96)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList([\n",
    "            GhostCoordAttention(384),\n",
    "            GhostCoordAttention(192),\n",
    "            GhostCoordAttention(96)\n",
    "        ])\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(96, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_feats = []\n",
    "        for stage in self.encoder_stages:\n",
    "            x = stage(x)\n",
    "            enc_feats.append(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i in range(3):\n",
    "            x = self.upsample(x)\n",
    "            x = x + enc_feats[-(i+2)]  # Skip connection\n",
    "            x = self.decoder_stages[i](x)\n",
    "            x = self.attentions[i](x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Note: GhostModule and DAAFModule need to be implemented or imported\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ab70a1c-1825-4e74-81b4-23fceb8604a0",
   "metadata": {},
   "source": [
    "--------------------xX ASPPNeXt Architecture Xx--------------------\n",
    "\n",
    "             RGB Input         Depth Input\n",
    "                â”‚                  â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚ Hybrid ConvNeXt â”‚ â”‚ Hybrid ConvNeXt â”‚\n",
    "       â”‚ Block Ã—4 (with  â”‚ â”‚ Block Ã—4 (with  â”‚\n",
    "       â”‚  VWA + GhostMLP)â”‚ â”‚  VWA + GhostMLP)â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚                   â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚ Pre-DAAF        â”‚ â”‚ Pre-DAAF        â”‚\n",
    "       â”‚ GhostModule     â”‚ â”‚ GhostModule     â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚                   â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â–º DAAF â—„â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚ Post-DAAF         â”‚\n",
    "                â”‚ GhostModule       â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”‚ Decoder Stage 1         â”‚\n",
    "             â”‚ GhostASPPFELAN          â”‚\n",
    "             â”‚ CoordAttention          â”‚\n",
    "             â”‚ DySample (Ã—2)           â”‚\n",
    "             â”‚ +Skip Connections (E4)  â”‚\n",
    "             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”‚ Decoder Stage 2         â”‚\n",
    "             â”‚ GhostASPPFELAN          â”‚\n",
    "             â”‚ CoordAttention          â”‚\n",
    "             â”‚ DySample (Ã—2)           â”‚\n",
    "             â”‚ +Skip Connections (E3)  â”‚\n",
    "             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”‚ Decoder Stage 3         â”‚\n",
    "             â”‚ GhostASPPFELAN          â”‚\n",
    "             â”‚ CoordAttention          â”‚\n",
    "             â”‚ DySample (Ã—2)           â”‚\n",
    "             â”‚ +Skip Connections (E2)  â”‚\n",
    "             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                   Final 1Ã—1 Conv\n",
    "                          â†“\n",
    "                     Output Map\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c133a51c-fd32-48e9-afa1-8098e6d1e257",
   "metadata": {},
   "source": [
    "--------------------xX GhostModule Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  â†“\n",
    "Primary Features: \n",
    "  - 1Ã—1 Conv â†’ C_mid = C_out // ratio\n",
    "  â†“\n",
    "Ghost Features:\n",
    "  - Depthwise Conv (or cheap linear ops) on C_mid\n",
    "  â†“\n",
    "Concatenate Primary + Ghost â†’ C_out\n",
    "  â†“\n",
    "BatchNorm (optional)\n",
    "  â†“\n",
    "Output: (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c0e33b-905a-4c8c-a202-e860c4ae83a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--------------------xX Hybrid ConvNeXt Block Xx--------------------\n",
    "\n",
    "Input: X âˆˆ â„^(BÃ—CÃ—HÃ—W)\n",
    "  â†“\n",
    "7Ã—7 DepthwiseConv\n",
    "  â†“\n",
    "Permute â†’ â„^(BÃ—HÃ—WÃ—C)\n",
    "  â†“\n",
    "LayerNorm\n",
    "  â†“\n",
    "Varying Window Attention:\n",
    "  â€¢ Query window: PÃ—P fixed\n",
    "  â€¢ Context window: (RÃ—P)Ã—(RÃ—P)\n",
    "  â€¢ MHSA with zero extra cost\n",
    "  â†“\n",
    "Residual Add\n",
    "  â†“\n",
    "LayerNorm\n",
    "  â†“\n",
    "Ghost MLP:\n",
    "  â€¢ GhostConv(1Ã—1): Câ†’4C â†’ GELU\n",
    "  â€¢ GhostConv(1Ã—1): 4Câ†’C\n",
    "  â†“\n",
    "Residual Add\n",
    "  â†“\n",
    "Permute back â†’ â„^(BÃ—CÃ—HÃ—W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7cf83-6981-40db-a9bb-c38e1adbf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------xX DAAF Block with GhostModule Integrations Xx--------------------\n",
    "\n",
    "Inputs:\n",
    "  f_rgb_preGhost âˆˆ â„^(BÃ—CÃ—HÃ—W)\n",
    "  f_depth_preGhost âˆˆ â„^(BÃ—CÃ—HÃ—W)\n",
    "\n",
    "1. Local Branch (RDSCB + LIA)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ RDSCB (nâˆˆ{1,3,5,7}):               â”‚\n",
    "   â”‚   For each n: GhostConv(nÃ—n)       â”‚\n",
    "   â”‚   LeakyReLU                        â”‚\n",
    "   â”‚ Concatenate â†’ GhostConv(1Ã—1)       â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "   LIA: Conv1D pooling â†’ GhostConv(1Ã—1)\n",
    "\n",
    "2. Global Branch (ITB)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Interactive Self-Attention (ISA)   â”‚\n",
    "   â”‚   Cross-modal MSA on (f_rgb, f_d)  â”‚\n",
    "   â”‚ Residual Add â†’ LayerNorm           â”‚\n",
    "   â”‚ GhostConv FFN:                     â”‚\n",
    "   â”‚   â€¢ GhostConv(1Ã—1): Câ†’4C           â”‚\n",
    "   â”‚   â€¢ GELU                           â”‚\n",
    "   â”‚   â€¢ GhostConv(1Ã—1): 4Câ†’C           â”‚\n",
    "   â”‚ Residual Add                       â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "3. Fusion and Reconstruction\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Concat(local, global_rgb, global_depth)  â”‚\n",
    "   â”‚ GhostConv(3Ã—3) â†’ LeakyReLU               â”‚\n",
    "   â”‚ (This is the â€œGlobal Fusionâ€ Conv)       â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "   Reconstruction Head:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ GhostConv(3Ã—3) â†’ LeakyReLU         â”‚\n",
    "   â”‚ GhostConv(3Ã—3) â†’ LeakyReLU         â”‚\n",
    "   â”‚ GhostConv(3Ã—3) â†’ LeakyReLU         â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â†“\n",
    "   Output: f_fused âˆˆ â„^(BÃ—CÃ—HÃ—W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4a95455-17d8-4fa5-b1d1-66c58ed40f57",
   "metadata": {},
   "source": [
    "The decoder block at each stage of ASPPNeXt with skip-connections, GhostASPPFELAN, CoordAttention and DySample executes in exactly this order:\n",
    "\n",
    "Gather Inputs\n",
    "â€“ If Stage 1: take the Post-DAAF fused feature map\n",
    "â€“ Otherwise: take the upsampled output from the previous decoder stage\n",
    "â€“ Also fetch the skip feature from the corresponding encoder block (Eâ‚„â†’Stage 1, Eâ‚ƒâ†’Stage 2, Eâ‚‚â†’Stage 3)\n",
    "\n",
    "Fuse Skip Connection\n",
    "â€“ Concatenate along channels:\n",
    "input âŠ• skip â†’ Fâ‚€ (shape: BÃ—(2C)Ã—HÃ—W)\n",
    "\n",
    "GhostASPPFELAN\n",
    "a. Branch 1: GhostConv 1Ã—1 â†’ LeakyReLU\n",
    "b. Branch 2: GhostConv 3Ã—3, dilation=2 â†’ LeakyReLU\n",
    "c. Branch 3: GhostConv 3Ã—3, dilation=4 â†’ LeakyReLU\n",
    "d. Concatenate branches (BÃ—3C_outÃ—HÃ—W) â†’ GhostConv 1Ã—1 fusion â†’ LeakyReLU\n",
    "â†’ Output Fâ‚ (BÃ—C_outÃ—HÃ—W)\n",
    "\n",
    "CoordAttention\n",
    "a. Pool along H â†’ z_h (BÃ—CÃ—1Ã—W)\n",
    "b. Pool along W â†’ z_w (BÃ—CÃ—HÃ—1)\n",
    "c. Concat(z_h, z_w) â†’ GhostConv(1Ã—1,Câ†’C/r) â†’ LeakyReLU â†’ split into t_h, t_w\n",
    "d. t_h â†’ GhostConv(1Ã—1,C/râ†’C) â†’ sigmoid â†’ attn_h (BÃ—CÃ—1Ã—W)\n",
    "e. t_w â†’ GhostConv(1Ã—1,C/râ†’C) â†’ sigmoid â†’ attn_w (BÃ—CÃ—HÃ—1)\n",
    "f. Multiply: Fâ‚‚ = Fâ‚ Ã— attn_h Ã— attn_w\n",
    "\n",
    "DySample Upsampling (Ã—2)\n",
    "â€“ Optionally pre-process with GhostConv(1Ã—1)\n",
    "â€“ Apply content-aware DySample to Fâ‚‚ â†’ Fâ‚ƒ (BÃ—CÃ—2HÃ—2W)\n",
    "\n",
    "Pass to Next Stage or Final\n",
    "â€“ Fâ‚ƒ is fed as input to the next decoder stage (or into the final 1Ã—1 conv at the end of Stage 3)\n",
    "\n",
    "Summary of per-stage flow\n",
    "Input + Skip â†’ GhostASPPFELAN â†’ CoordAttention â†’ DySample â†’ Next stage."
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fb663-315e-4b3a-90e6-ed035184a557",
   "metadata": {},
   "source": [
    "--------------------xX Ghost ASPPFELAN Block Xx--------------------\n",
    "\n",
    "Input: X âˆˆ â„^(BÃ—C_inÃ—HÃ—W)\n",
    "  â†“\n",
    "Branch 1: GhostConv(1Ã—1) â†’ LeakyReLU\n",
    "Branch 2: GhostConv(3Ã—3, dilation=2) â†’ LeakyReLU\n",
    "Branch 3: GhostConv(3Ã—3, dilation=4) â†’ LeakyReLU\n",
    "  â†“\n",
    "Concat(branch1,2,3) âˆˆ â„^(BÃ—3C_outÃ—HÃ—W)\n",
    "  â†“\n",
    "GhostConv(1Ã—1) â†’ LeakyReLU\n",
    "  â†“\n",
    "Output: Y âˆˆ â„^(BÃ—C_outÃ—HÃ—W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d717db5-a2b0-47fd-91e4-3f678caedb7e",
   "metadata": {},
   "source": [
    "--------------------xX CoordAttention Block Xx--------------------\n",
    "\n",
    "Input: X âˆˆ â„^(BÃ—CÃ—HÃ—W)\n",
    "  â†“\n",
    "1. Pool along H: z_h âˆˆ â„^(BÃ—CÃ—1Ã—W)\n",
    "2. Pool along W: z_w âˆˆ â„^(BÃ—CÃ—HÃ—1)\n",
    "  â†“\n",
    "Concat(z_h, z_w) â†’ GhostConv(1Ã—1, Câ†’C/r) â†’ LeakyReLU\n",
    "  â†“\n",
    "Split into t_h, t_w channels\n",
    "  â†“\n",
    "t_h â†’ GhostConv(1Ã—1, C/râ†’C) â†’ sigmoid â†’ attn_h âˆˆ â„^(BÃ—CÃ—1Ã—W)\n",
    "t_w â†’ GhostConv(1Ã—1, C/râ†’C) â†’ sigmoid â†’ attn_w âˆˆ â„^(BÃ—CÃ—HÃ—1)\n",
    "  â†“\n",
    "Output: X' = X Ã— attn_h Ã— attn_w\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "813b60a6-67c5-461d-8120-201c989d29eb",
   "metadata": {},
   "source": [
    "--------------------xX DySample Upsampling Xx--------------------\n",
    "\n",
    "Input: X âˆˆ â„^(BÃ—CÃ—HÃ—W)\n",
    "  â†“\n",
    "1. Feature Transformation: GhostConv(1Ã—1)  # Optional\n",
    "2. DySample Operation:\n",
    "   - Dynamic kernel prediction based on local context\n",
    "   - Content-aware upsampling (Ã—2 scale)\n",
    "  â†“\n",
    "Output: X_up âˆˆ â„^(BÃ—CÃ—2HÃ—2W)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
