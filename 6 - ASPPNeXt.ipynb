{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aea3a0-ce64-45ec-ac09-1cfef823c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 🔧 1. Ghost MLP Block (ConvNeXt-style)\n",
    "# -----------------------------\n",
    "class GhostMLPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, expansion_ratio=4):\n",
    "        super(GhostMLPBlock, self).__init__()\n",
    "        hidden_dim = in_channels * expansion_ratio\n",
    "        self.fc1 = GhostModule(in_channels, hidden_dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = GhostModule(hidden_dim, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 🔧 2. Modified ConvNeXt-Attention Block\n",
    "# -----------------------------\n",
    "class HybridConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, channels, attention_module):\n",
    "        super(HybridConvNeXtBlock, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size=7, padding=3, groups=channels)  # Positional encoding\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.attn = attention_module(channels)  # Could be MHSA, Axial, etc.\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.mlp = GhostMLPBlock(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = x + residual.permute(0, 2, 3, 1)\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x.permute(0, 3, 1, 2))\n",
    "        x = x.permute(0, 2, 3, 1) + residual\n",
    "        return x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔧 3. Ghost CoordAttention Block\n",
    "# -----------------------------\n",
    "class GhostCoordAttention(nn.Module):\n",
    "    def __init__(self, inp, reduction=32):\n",
    "        super(GhostCoordAttention, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mid = max(8, inp // reduction)\n",
    "        self.conv1 = GhostModule(inp, mid, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(mid)\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv_h = GhostModule(mid, inp, kernel_size=1)\n",
    "        self.conv_w = GhostModule(mid, inp, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n, c, h, w = x.size()\n",
    "\n",
    "        x_h = self.pool_h(x).permute(0, 1, 3, 2)\n",
    "        x_w = self.pool_w(x)\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y)\n",
    "\n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_h = self.conv_h(x_h.permute(0, 1, 3, 2))\n",
    "        x_w = self.conv_w(x_w)\n",
    "\n",
    "        out = identity * torch.sigmoid(x_h) * torch.sigmoid(x_w)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# 🔧 4. Ghost ASPPFELAN Block (Simplified)\n",
    "# -----------------------------\n",
    "class GhostASPPFELAN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GhostASPPFELAN, self).__init__()\n",
    "        self.branch1 = GhostModule(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch2 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        self.branch3 = GhostModule(in_channels, out_channels, kernel_size=3, dilation=4, padding=4)\n",
    "        self.fuse = GhostModule(out_channels * 3, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        x = torch.cat([b1, b2, b3], dim=1)\n",
    "        x = self.fuse(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 🏗️ Full Architecture (Pseudo)\n",
    "# -----------------------------\n",
    "class ASPPNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ASPPNeXt, self).__init__()\n",
    "        self.encoder_stages = nn.ModuleList([\n",
    "            HybridConvNeXtBlock(96, attention_module),\n",
    "            HybridConvNeXtBlock(192, attention_module),\n",
    "            HybridConvNeXtBlock(384, attention_module),\n",
    "            HybridConvNeXtBlock(768, attention_module)\n",
    "        ])\n",
    "\n",
    "        self.bottleneck = DAAFModule(768)  # Assume DAAFModule is implemented\n",
    "\n",
    "        self.decoder_stages = nn.ModuleList([\n",
    "            GhostASPPFELAN(768, 384),\n",
    "            GhostASPPFELAN(384, 192),\n",
    "            GhostASPPFELAN(192, 96)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList([\n",
    "            GhostCoordAttention(384),\n",
    "            GhostCoordAttention(192),\n",
    "            GhostCoordAttention(96)\n",
    "        ])\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.final_conv = nn.Conv2d(96, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_feats = []\n",
    "        for stage in self.encoder_stages:\n",
    "            x = stage(x)\n",
    "            enc_feats.append(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i in range(3):\n",
    "            x = self.upsample(x)\n",
    "            x = x + enc_feats[-(i+2)]  # Skip connection\n",
    "            x = self.decoder_stages[i](x)\n",
    "            x = self.attentions[i](x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Note: GhostModule and DAAFModule need to be implemented or imported\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ab70a1c-1825-4e74-81b4-23fceb8604a0",
   "metadata": {},
   "source": [
    "--------------------xX ASPPNeXt Architecture Xx--------------------\n",
    "\n",
    "             RGB Input         Depth Input\n",
    "                │                  │\n",
    "       ┌────────▼────────┐ ┌───────▼────────┐\n",
    "       │ Hybrid ConvNeXt │ │ Hybrid ConvNeXt │\n",
    "       │ Block ×4 (with  │ │ Block ×4 (with  │\n",
    "       │  VWA + GhostMLP)│ │  VWA + GhostMLP)│\n",
    "       └────────┬────────┘ └────────┬───────┘\n",
    "                │                   │\n",
    "       ┌────────▼────────┐ ┌────────▼────────┐\n",
    "       │ Pre-DAAF        │ │ Pre-DAAF        │\n",
    "       │ GhostModule     │ │ GhostModule     │\n",
    "       └────────┬────────┘ └────────┬────────┘\n",
    "                │                   │\n",
    "                └──────► DAAF ◄─────┘\n",
    "                          │\n",
    "                ┌─────────▼─────────┐\n",
    "                │ Post-DAAF         │\n",
    "                │ GhostModule       │\n",
    "                └─────────┬─────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 1         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E4)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 2         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E3)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 3         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E2)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "                   Final 1×1 Conv\n",
    "                          ↓\n",
    "                     Output Map\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c133a51c-fd32-48e9-afa1-8098e6d1e257",
   "metadata": {},
   "source": [
    "--------------------xX GhostModule Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  ↓\n",
    "Primary Features: \n",
    "  - 1×1 Conv → C_mid = C_out // ratio\n",
    "  ↓\n",
    "Ghost Features:\n",
    "  - Depthwise Conv (or cheap linear ops) on C_mid\n",
    "  ↓\n",
    "Concatenate Primary + Ghost → C_out\n",
    "  ↓\n",
    "BatchNorm (optional)\n",
    "  ↓\n",
    "Output: (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c0e33b-905a-4c8c-a202-e860c4ae83a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--------------------xX Hybrid ConvNeXt Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "7×7 DepthwiseConv\n",
    "  ↓\n",
    "Permute → ℝ^(B×H×W×C)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Varying Window Attention:\n",
    "  • Query window: P×P fixed\n",
    "  • Context window: (R×P)×(R×P)\n",
    "  • MHSA with zero extra cost\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Ghost MLP:\n",
    "  • GhostConv(1×1): C→4C → GELU\n",
    "  • GhostConv(1×1): 4C→C\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "Permute back → ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7cf83-6981-40db-a9bb-c38e1adbf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------xX DAAF Block with GhostModule Integrations Xx--------------------\n",
    "\n",
    "Inputs:\n",
    "  f_rgb_preGhost ∈ ℝ^(B×C×H×W)\n",
    "  f_depth_preGhost ∈ ℝ^(B×C×H×W)\n",
    "\n",
    "1. Local Branch (RDSCB + LIA)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ RDSCB (n∈{1,3,5,7}):               │\n",
    "   │   For each n: GhostConv(n×n)       │\n",
    "   │   LeakyReLU                        │\n",
    "   │ Concatenate → GhostConv(1×1)       │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   LIA: Conv1D pooling → GhostConv(1×1)\n",
    "\n",
    "2. Global Branch (ITB)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ Interactive Self-Attention (ISA)   │\n",
    "   │   Cross-modal MSA on (f_rgb, f_d)  │\n",
    "   │ Residual Add → LayerNorm           │\n",
    "   │ GhostConv FFN:                     │\n",
    "   │   • GhostConv(1×1): C→4C           │\n",
    "   │   • GELU                           │\n",
    "   │   • GhostConv(1×1): 4C→C           │\n",
    "   │ Residual Add                       │\n",
    "   └────────────────────────────────────┘\n",
    "\n",
    "3. Fusion and Reconstruction\n",
    "   ┌──────────────────────────────────────────┐\n",
    "   │ Concat(local, global_rgb, global_depth)  │\n",
    "   │ GhostConv(3×3) → LeakyReLU               │\n",
    "   │ (This is the “Global Fusion” Conv)       │\n",
    "   └──────────────────────────────────────────┘\n",
    "       ↓\n",
    "   Reconstruction Head:\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   Output: f_fused ∈ ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4a95455-17d8-4fa5-b1d1-66c58ed40f57",
   "metadata": {},
   "source": [
    "The decoder block at each stage of ASPPNeXt with skip-connections, GhostASPPFELAN, CoordAttention and DySample executes in exactly this order:\n",
    "\n",
    "Gather Inputs\n",
    "– If Stage 1: take the Post-DAAF fused feature map\n",
    "– Otherwise: take the upsampled output from the previous decoder stage\n",
    "– Also fetch the skip feature from the corresponding encoder block (E₄→Stage 1, E₃→Stage 2, E₂→Stage 3)\n",
    "\n",
    "Fuse Skip Connection\n",
    "– Concatenate along channels:\n",
    "input ⊕ skip → F₀ (shape: B×(2C)×H×W)\n",
    "\n",
    "GhostASPPFELAN\n",
    "a. Branch 1: GhostConv 1×1 → LeakyReLU\n",
    "b. Branch 2: GhostConv 3×3, dilation=2 → LeakyReLU\n",
    "c. Branch 3: GhostConv 3×3, dilation=4 → LeakyReLU\n",
    "d. Concatenate branches (B×3C_out×H×W) → GhostConv 1×1 fusion → LeakyReLU\n",
    "→ Output F₁ (B×C_out×H×W)\n",
    "\n",
    "CoordAttention\n",
    "a. Pool along H → z_h (B×C×1×W)\n",
    "b. Pool along W → z_w (B×C×H×1)\n",
    "c. Concat(z_h, z_w) → GhostConv(1×1,C→C/r) → LeakyReLU → split into t_h, t_w\n",
    "d. t_h → GhostConv(1×1,C/r→C) → sigmoid → attn_h (B×C×1×W)\n",
    "e. t_w → GhostConv(1×1,C/r→C) → sigmoid → attn_w (B×C×H×1)\n",
    "f. Multiply: F₂ = F₁ × attn_h × attn_w\n",
    "\n",
    "DySample Upsampling (×2)\n",
    "– Optionally pre-process with GhostConv(1×1)\n",
    "– Apply content-aware DySample to F₂ → F₃ (B×C×2H×2W)\n",
    "\n",
    "Pass to Next Stage or Final\n",
    "– F₃ is fed as input to the next decoder stage (or into the final 1×1 conv at the end of Stage 3)\n",
    "\n",
    "Summary of per-stage flow\n",
    "Input + Skip → GhostASPPFELAN → CoordAttention → DySample → Next stage."
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fb663-315e-4b3a-90e6-ed035184a557",
   "metadata": {},
   "source": [
    "--------------------xX Ghost ASPPFELAN Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C_in×H×W)\n",
    "  ↓\n",
    "Branch 1: GhostConv(1×1) → LeakyReLU\n",
    "Branch 2: GhostConv(3×3, dilation=2) → LeakyReLU\n",
    "Branch 3: GhostConv(3×3, dilation=4) → LeakyReLU\n",
    "  ↓\n",
    "Concat(branch1,2,3) ∈ ℝ^(B×3C_out×H×W)\n",
    "  ↓\n",
    "GhostConv(1×1) → LeakyReLU\n",
    "  ↓\n",
    "Output: Y ∈ ℝ^(B×C_out×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d717db5-a2b0-47fd-91e4-3f678caedb7e",
   "metadata": {},
   "source": [
    "--------------------xX CoordAttention Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Pool along H: z_h ∈ ℝ^(B×C×1×W)\n",
    "2. Pool along W: z_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Concat(z_h, z_w) → GhostConv(1×1, C→C/r) → LeakyReLU\n",
    "  ↓\n",
    "Split into t_h, t_w channels\n",
    "  ↓\n",
    "t_h → GhostConv(1×1, C/r→C) → sigmoid → attn_h ∈ ℝ^(B×C×1×W)\n",
    "t_w → GhostConv(1×1, C/r→C) → sigmoid → attn_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Output: X' = X × attn_h × attn_w\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "813b60a6-67c5-461d-8120-201c989d29eb",
   "metadata": {},
   "source": [
    "--------------------xX DySample Upsampling Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Feature Transformation: GhostConv(1×1)  # Optional\n",
    "2. DySample Operation:\n",
    "   - Dynamic kernel prediction based on local context\n",
    "   - Content-aware upsampling (×2 scale)\n",
    "  ↓\n",
    "Output: X_up ∈ ℝ^(B×C×2H×2W)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
