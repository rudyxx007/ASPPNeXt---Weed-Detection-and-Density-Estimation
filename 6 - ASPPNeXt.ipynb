{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7ab70a1c-1825-4e74-81b4-23fceb8604a0",
   "metadata": {},
   "source": [
    "--------------------xX ASPPNeXt Architecture Xx--------------------\n",
    "\n",
    "             RGB Input          Depth Input\n",
    "                │                   │\n",
    "       ┌────────▼────────┐ ┌────────▼────────┐\n",
    "       │ Hybrid ConvNeXt │ │ Hybrid ConvNeXt │\n",
    "       │ Block ×4 (with  │ │ Block ×4 (with  │\n",
    "       │  VWA + GhostMLP)│ │  VWA + GhostMLP)│\n",
    "       └────────┬────────┘ └────────┬────────┘\n",
    "                │                   │\n",
    "       ┌────────▼────────┐ ┌────────▼────────┐\n",
    "       │ Pre-DAAF        │ │ Pre-DAAF        │\n",
    "       │ GhostModule     │ │ GhostModule     │\n",
    "       └────────┬────────┘ └────────┬────────┘\n",
    "                │                   │\n",
    "                └──────► DAAF ◄─────┘\n",
    "                          │\n",
    "                ┌─────────▼─────────┐\n",
    "                │ Post-DAAF         │\n",
    "                │ GhostModule       │\n",
    "                └─────────┬─────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 1         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E4)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 2         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E3)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "             ┌────────────▼────────────┐\n",
    "             │ Decoder Stage 3         │\n",
    "             │ GhostASPPFELAN          │\n",
    "             │ CoordAttention          │\n",
    "             │ DySample (×2)           │\n",
    "             │ +Skip Connections (E2)  │\n",
    "             └────────────┬────────────┘\n",
    "                          │\n",
    "                   Final 1×1 Conv\n",
    "                          ↓\n",
    "                     Output Map\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c133a51c-fd32-48e9-afa1-8098e6d1e257",
   "metadata": {},
   "source": [
    "--------------------xX GhostModule Xx--------------------\n",
    "\n",
    "Input: (B, C_in, H, W)\n",
    "  ↓\n",
    "Primary Features: \n",
    "  - 1×1 Conv → C_mid = C_out // ratio\n",
    "  ↓\n",
    "Ghost Features:\n",
    "  - Depthwise Conv (or cheap linear ops) on C_mid\n",
    "  ↓\n",
    "Concatenate Primary + Ghost → C_out\n",
    "  ↓\n",
    "BatchNorm (optional)\n",
    "  ↓\n",
    "Output: (B, C_out, H, W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c0e33b-905a-4c8c-a202-e860c4ae83a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--------------------xX Hybrid ConvNeXt Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "7×7 DepthwiseConv\n",
    "  ↓\n",
    "Permute → ℝ^(B×H×W×C)\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Varying Window Attention:\n",
    "  • Query window: P×P fixed\n",
    "  • Context window: (R×P)×(R×P)\n",
    "  • MHSA with zero extra cost\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "LayerNorm\n",
    "  ↓\n",
    "Ghost MLP:\n",
    "  • GhostConv(1×1): C→4C → GELU\n",
    "  • GhostConv(1×1): 4C→C\n",
    "  ↓\n",
    "Residual Add\n",
    "  ↓\n",
    "Permute back → ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e060b4ec-ec2d-4178-a5a0-8fc57be6244c",
   "metadata": {},
   "source": [
    "--------------------xX DAAF Block with GhostModule Integrations Xx--------------------\n",
    "\n",
    "Inputs:\n",
    "  f_rgb_preGhost ∈ ℝ^(B×C×H×W)\n",
    "  f_depth_preGhost ∈ ℝ^(B×C×H×W)\n",
    "\n",
    "1. Local Branch (RDSCB + LIA)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ RDSCB (n∈{1,3,5,7}):               │\n",
    "   │   For each n: GhostConv(n×n)       │\n",
    "   │   LeakyReLU                        │\n",
    "   │ Concatenate → GhostConv(1×1)       │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   LIA: Conv1D pooling → GhostConv(1×1)\n",
    "\n",
    "2. Global Branch (ITB)\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ Interactive Self-Attention (ISA)   │\n",
    "   │   Cross-modal MSA on (f_rgb, f_d)  │\n",
    "   │ Residual Add → LayerNorm           │\n",
    "   │ GhostConv FFN:                     │\n",
    "   │   • GhostConv(1×1): C→4C           │\n",
    "   │   • GELU                           │\n",
    "   │   • GhostConv(1×1): 4C→C           │\n",
    "   │ Residual Add                       │\n",
    "   └────────────────────────────────────┘\n",
    "\n",
    "3. Fusion and Reconstruction\n",
    "   ┌──────────────────────────────────────────┐\n",
    "   │ Concat(local, global_rgb, global_depth)  │\n",
    "   │ GhostConv(3×3) → LeakyReLU               │\n",
    "   │ (This is the “Global Fusion” Conv)       │\n",
    "   └──────────────────────────────────────────┘\n",
    "       ↓\n",
    "   Reconstruction Head:\n",
    "   ┌────────────────────────────────────┐\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   │ GhostConv(3×3) → LeakyReLU         │\n",
    "   └────────────────────────────────────┘\n",
    "       ↓\n",
    "   Output: f_fused ∈ ℝ^(B×C×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3f1800c-6e89-4f1a-8a7a-c31a7ce7beae",
   "metadata": {},
   "source": [
    "The decoder block at each stage of ASPPNeXt with skip-connections, GhostASPPFELAN, CoordAttention and DySample executes in exactly this order:\n",
    "\n",
    "Gather Inputs\n",
    "– If Stage 1: take the Post-DAAF fused feature map\n",
    "– Otherwise: take the upsampled output from the previous decoder stage\n",
    "– Also fetch the skip feature from the corresponding encoder block (E₄→Stage 1, E₃→Stage 2, E₂→Stage 3)\n",
    "\n",
    "Fuse Skip Connection\n",
    "– Concatenate along channels:\n",
    "input ⊕ skip → F₀ (shape: B×(2C)×H×W)\n",
    "\n",
    "GhostASPPFELAN\n",
    "a. Branch 1: GhostConv 1×1 → LeakyReLU\n",
    "b. Branch 2: GhostConv 3×3, dilation=2 → LeakyReLU\n",
    "c. Branch 3: GhostConv 3×3, dilation=4 → LeakyReLU\n",
    "d. Concatenate branches (B×3C_out×H×W) → GhostConv 1×1 fusion → LeakyReLU\n",
    "→ Output F₁ (B×C_out×H×W)\n",
    "\n",
    "CoordAttention\n",
    "a. Pool along H → z_h (B×C×1×W)\n",
    "b. Pool along W → z_w (B×C×H×1)\n",
    "c. Concat(z_h, z_w) → GhostConv(1×1,C→C/r) → LeakyReLU → split into t_h, t_w\n",
    "d. t_h → GhostConv(1×1,C/r→C) → sigmoid → attn_h (B×C×1×W)\n",
    "e. t_w → GhostConv(1×1,C/r→C) → sigmoid → attn_w (B×C×H×1)\n",
    "f. Multiply: F₂ = F₁ × attn_h × attn_w\n",
    "\n",
    "DySample Upsampling (×2)\n",
    "– Optionally pre-process with GhostConv(1×1)\n",
    "– Apply content-aware DySample to F₂ → F₃ (B×C×2H×2W)\n",
    "\n",
    "Pass to Next Stage or Final\n",
    "– F₃ is fed as input to the next decoder stage (or into the final 1×1 conv at the end of Stage 3)\n",
    "\n",
    "Summary of per-stage flow\n",
    "Input + Skip → GhostASPPFELAN → CoordAttention → DySample → Next stage."
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fb663-315e-4b3a-90e6-ed035184a557",
   "metadata": {},
   "source": [
    "--------------------xX Ghost ASPPFELAN Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C_in×H×W)\n",
    "  ↓\n",
    "Branch 1: GhostConv(1×1) → LeakyReLU\n",
    "Branch 2: GhostConv(3×3, dilation=2) → LeakyReLU\n",
    "Branch 3: GhostConv(3×3, dilation=4) → LeakyReLU\n",
    "  ↓\n",
    "Concat(branch1,2,3) ∈ ℝ^(B×3C_out×H×W)\n",
    "  ↓\n",
    "GhostConv(1×1) → LeakyReLU\n",
    "  ↓\n",
    "Output: Y ∈ ℝ^(B×C_out×H×W)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d717db5-a2b0-47fd-91e4-3f678caedb7e",
   "metadata": {},
   "source": [
    "--------------------xX CoordAttention Block Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Pool along H: z_h ∈ ℝ^(B×C×1×W)\n",
    "2. Pool along W: z_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Concat(z_h, z_w) → GhostConv(1×1, C→C/r) → LeakyReLU\n",
    "  ↓\n",
    "Split into t_h, t_w channels\n",
    "  ↓\n",
    "t_h → GhostConv(1×1, C/r→C) → sigmoid → attn_h ∈ ℝ^(B×C×1×W)\n",
    "t_w → GhostConv(1×1, C/r→C) → sigmoid → attn_w ∈ ℝ^(B×C×H×1)\n",
    "  ↓\n",
    "Output: X' = X × attn_h × attn_w\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "813b60a6-67c5-461d-8120-201c989d29eb",
   "metadata": {},
   "source": [
    "--------------------xX DySample Upsampling Xx--------------------\n",
    "\n",
    "Input: X ∈ ℝ^(B×C×H×W)\n",
    "  ↓\n",
    "1. Feature Transformation: GhostConv(1×1)  # Optional\n",
    "2. DySample Operation:\n",
    "   - Dynamic kernel prediction based on local context\n",
    "   - Content-aware upsampling (×2 scale)\n",
    "  ↓\n",
    "Output: X_up ∈ ℝ^(B×C×2H×2W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11102300-cb45-4ee8-b15e-e0d75bc3470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e19c5-263b-4b77-9a24-342633cc4a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60399c-f18f-4b4b-b778-ce66925dcfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9616ca-6247-4716-9c14-503f8dda5e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456c00e-717f-494e-859d-4fe8a1af39dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5262a4-3066-4b54-b6f4-ab733ed9233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9544a6f3-e3fd-4213-bf8e-4f1c86f66f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFCAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dfc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1, bias=False),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid().add_(1e-6)  # Added epsilon for numerical stability\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.dfc(x)\n",
    "\n",
    "class GhostModuleV2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 ratio=2, kernel_size=1, dw_size=3,\n",
    "                 stride=1, use_attn=True):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        init_ch = math.ceil(out_channels / ratio)\n",
    "        new_ch = init_ch * (ratio - 1)\n",
    "\n",
    "        # Primary convolution with dynamic padding\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, init_ch,\n",
    "                     kernel_size, stride,\n",
    "                     (kernel_size - 1) // 2,  # More precise padding\n",
    "                     bias=False),\n",
    "            nn.BatchNorm2d(init_ch),\n",
    "            nn.Hardswish(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Cheap operation with dynamic padding\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_ch, new_ch, dw_size, 1,\n",
    "                     (dw_size - 1) // 2,  # More precise padding\n",
    "                     groups=init_ch, bias=False),\n",
    "            nn.BatchNorm2d(new_ch),\n",
    "            nn.Hardswish(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Attention with stride warning\n",
    "        self.use_attn = use_attn\n",
    "        if use_attn:\n",
    "            if stride > 1:\n",
    "                print(f\"Warning: Stride {stride}>1 may reduce attention effectiveness\")\n",
    "            self.attn = DFCAttention(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        out = T.cat([x1, x2], dim=1)[:, :self.out_channels]\n",
    "        return self.attn(out) if self.use_attn else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632c4c0f-bf1b-4412-ae5d-1f4f0c5907a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaryingWindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, context_ratio=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.context_ratio = context_ratio\n",
    "        \n",
    "        # Projections for Q/K/V\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.to_out = nn.Linear(dim, dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        P = self.window_size\n",
    "        R = self.context_ratio\n",
    "        \n",
    "        # Reshape to (B, H, W, C)\n",
    "        x_spatial = x.view(B, H, W, C)\n",
    "        \n",
    "        # 1. Create P×P query windows\n",
    "        q_windows = x_spatial.unfold(1, P, P).unfold(2, P, P)\n",
    "        q_windows = q_windows.contiguous().view(B, -1, P*P, C)\n",
    "        \n",
    "        # 2. Create (R*P)×(R*P) context windows\n",
    "        pad = (R * P - P) // 2\n",
    "        x_pad = F.pad(x_spatial, (0, 0, pad, pad, pad, pad))\n",
    "        ctx_windows = x_pad.unfold(1, R*P, P).unfold(2, R*P, P)\n",
    "        ctx_windows = ctx_windows.contiguous().view(B, -1, (R*P)**2, C)\n",
    "        \n",
    "        # 3. Concatenate queries and context\n",
    "        seq = T.cat([q_windows, ctx_windows], dim=2)\n",
    "        \n",
    "        # 4. Compute Q, K, V\n",
    "        qkv = self.to_qkv(seq)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Split into q, k, v\n",
    "        \n",
    "        # 5. Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * (C ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # 6. Keep only query window outputs\n",
    "        out = out[:, :, :P*P, :]\n",
    "        \n",
    "        # 7. Reconstruct spatial layout\n",
    "        out = out.view(B, H//P, W//P, P, P, C)\n",
    "        out = out.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, C)\n",
    "        out = out.view(B, N, C)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class GhostMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Ghost MLP with two GhostModuleV2 layers and GELU activation.\n",
    "    Expands C→4C then projects back 4C→C.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * mlp_ratio\n",
    "        self.fc1 = GhostModuleV2(dim, hidden_dim, use_attn=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = GhostModuleV2(hidden_dim, dim, use_attn=False)\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, N, C)\n",
    "        returns: (B, N, C)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        # reshape to (B, C, H, W) for GhostModuleV2, then back\n",
    "        H = W = int(N ** 0.5)\n",
    "        x_spatial = x.view(B, H, W, C).permute(0,3,1,2)\n",
    "        x_spatial = self.act(self.fc1(x_spatial))\n",
    "        x_spatial = self.fc2(x_spatial)\n",
    "        x_flat = x_spatial.permute(0,2,3,1).view(B, N, C)\n",
    "        return x_flat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3b707c-efb0-47c6-85f2-f7818a31dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size,\n",
    "                 context_ratio=2, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 7, padding=3,\n",
    "                                groups=dim, bias=False)\n",
    "        self.norm1  = nn.LayerNorm(dim)\n",
    "        self.attn   = VaryingWindowAttention(dim, num_heads,\n",
    "                                             window_size, context_ratio)\n",
    "        self.norm2  = nn.LayerNorm(dim)\n",
    "        self.mlp    = GhostMLP(dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        B,C,H,W = x.shape\n",
    "        seq = x.permute(0,2,3,1).reshape(B, H*W, C)\n",
    "        seq = seq + self.attn(self.norm1(seq))\n",
    "        seq = seq + self.mlp(self.norm2(seq))\n",
    "        return seq.view(B, H, W, C).permute(0,3,1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2a6e33-ef46-4fef-b486-631a450f042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, base_dim=64,\n",
    "                 num_heads=(4,8,16,32),\n",
    "                 window_sizes=(8,4,2,1),\n",
    "                 mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        # Stage 1 patch‐embedding (no further downsampling here)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, base_dim, 4, stride=4, bias=False),\n",
    "            nn.LayerNorm([base_dim, None, None])\n",
    "        )\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            dim = base_dim * (2**i)\n",
    "            # downsample before stages 2–4 (i=1,2,3)\n",
    "            down = None if i == 0 else nn.Sequential(\n",
    "                GhostModuleV2(in_channels=dim//2,\n",
    "                              out_channels=dim,\n",
    "                              ratio=2,\n",
    "                              kernel_size=2,\n",
    "                              stride=2,\n",
    "                              use_attn=True),\n",
    "                nn.LayerNorm([dim, None, None])\n",
    "            )\n",
    "            block = HybridConvNeXtBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_sizes[i],\n",
    "                context_ratio=2,\n",
    "                mlp_ratio=mlp_ratio\n",
    "            )\n",
    "            self.stages.append(nn.ModuleDict({'down': down, 'block': block}))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, in_ch, H, W)\n",
    "        x = self.stem(x)            # Stage 1 input → (B, base_dim, H/4, W/4)\n",
    "        skips = []\n",
    "        for stage in self.stages:\n",
    "            if stage['down'] is not None:\n",
    "                x = stage['down'](x) # downsample before stages 2–4\n",
    "            x = stage['block'](x)   # Hybrid block\n",
    "            skips.append(x)         # capture skip for decoder\n",
    "        # skips[0] = stage 1 output (no downsample)\n",
    "        # skips[1] = stage 2 output (1/8 input res)\n",
    "        # skips[2] = stage 3 output (1/16 input res)\n",
    "        # skips[3] = stage 4 output (1/32 input res)\n",
    "        return x, skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbc73c7-de29-46bb-9c89-f6eaf2268b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreDAAFGhostV2(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 4, ratio: int = 2):\n",
    "        super().__init__()\n",
    "        mid_channels = channels // reduction\n",
    "\n",
    "        self.reduce = GhostModuleV2(\n",
    "            in_channels=channels,\n",
    "            out_channels=mid_channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, mid_channels,\n",
    "                      kernel_size=3, stride=1, padding=1,\n",
    "                      groups=mid_channels, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.Hardswish(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.expand = GhostModuleV2(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        x = self.reduce(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.dwconv(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.expand(x)    # → (B, C, H, W)\n",
    "        return x\n",
    "\n",
    "# Example usage in the ASPPNeXt pipeline:\n",
    "\n",
    "# Instantiate once (channels must match final encoder output channels)\n",
    "# e.g., if encoder final stage outputs C=512 feature maps:\n",
    "#pre_daaf = PreDAAFGhostV2(channels=512, reduction=4, ratio=2)\n",
    "\n",
    "# In forward pass for RGB and Depth encoders:\n",
    "# f_rgb_out, rgb_skips   = rgb_encoder(rgb_input)\n",
    "# f_depth_out, depth_skips = depth_encoder(depth_input)\n",
    "# f_rgb_pre   = pre_daaf(f_rgb_out)\n",
    "# f_depth_pre = pre_daaf(f_depth_out)\n",
    "#\n",
    "# Then feed (f_rgb_pre, f_depth_pre) into your DAAF fusion block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa22efc-0c9d-4d73-9f6e-41edc8021df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAAFBlock(nn.Module):\n",
    "    def __init__(self, channels: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        # 1. Local branch per modality\n",
    "        self.local_branch = RDSCBLocal(channels)\n",
    "        # Fuse local RGB/depth outputs back to C channels\n",
    "        self.local_fuse = GhostModuleV2(\n",
    "            in_channels=channels * 2,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.lia = LIA(channels)\n",
    "\n",
    "        # 2. Global branch\n",
    "        self.itb = InteractiveTransformerBlock(channels, num_heads)\n",
    "\n",
    "        # 3. Global fusion conv: (local + global_rgb + global_depth) → channels\n",
    "        self.global_fuse = GhostModuleV2(\n",
    "            in_channels=channels * 3,\n",
    "            out_channels=channels,\n",
    "            kernel_size=3,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        # 4. Reconstruction head: three cascaded GhostConv(3×3)\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            GhostModuleV2(channels, channels, kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            GhostModuleV2(channels, channels, kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            GhostModuleV2(channels, channels, kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                f_rgb_pre: T.Tensor,\n",
    "                f_depth_pre: T.Tensor) -> T.Tensor:\n",
    "        # 1. Local branch on each modality\n",
    "        lr = self.lia(self.local_branch(f_rgb_pre))\n",
    "        ld = self.lia(self.local_branch(f_depth_pre))\n",
    "        # Fuse local RGB + Depth features\n",
    "        local = T.cat([lr, ld], dim=1)   # (B, 2C, H, W)\n",
    "        local = self.act(self.local_fuse(local))  # (B, C, H, W)\n",
    "\n",
    "        # 2. Global interactive attention\n",
    "        g_rgb, g_depth = self.itb(f_rgb_pre, f_depth_pre)\n",
    "\n",
    "        # 3. Global fusion\n",
    "        cat = T.cat([local, g_rgb, g_depth], dim=1)  # (B, 3C, H, W)\n",
    "        fused = self.act(self.global_fuse(cat))         # (B, C, H, W)\n",
    "\n",
    "        # 4. Reconstruction head\n",
    "        out = self.reconstruction(fused)  # (B, C, H, W)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dfaca2-2c3a-4569-902c-fb80cd145616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostDAAFGhostV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-DAAF GhostModule bottleneck using GhostModuleV2 with DFC attention.\n",
    "    Mirrors the Pre-DAAF structure:\n",
    "      1) GhostModuleV2 1×1 (C→C//reduction)\n",
    "      2) 3×3 depthwise conv\n",
    "      3) GhostModuleV2 1×1 (C//reduction→C)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 4, ratio: int = 2):\n",
    "        super().__init__()\n",
    "        mid_ch = channels // reduction\n",
    "\n",
    "        # 1×1 GhostModuleV2 reduce channels\n",
    "        self.reduce = GhostModuleV2(\n",
    "            in_channels=channels,\n",
    "            out_channels=mid_ch,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "        # 3×3 depthwise conv on reduced features\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                mid_ch, mid_ch,\n",
    "                kernel_size=3, stride=1,\n",
    "                padding=1, groups=mid_ch,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_ch),\n",
    "            nn.Hardswish(inplace=True)\n",
    "        )\n",
    "        # 1×1 GhostModuleV2 expand back to original channels\n",
    "        self.expand = GhostModuleV2(\n",
    "            in_channels=mid_ch,\n",
    "            out_channels=channels,\n",
    "            ratio=ratio,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            use_attn=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W) — fused feature map from DAAFBlock\n",
    "        returns: (B, C, H, W) — channel-refined output\n",
    "        \"\"\"\n",
    "        x = self.reduce(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.dwconv(x)    # → (B, C//reduction, H, W)\n",
    "        x = self.expand(x)    # → (B, C, H, W)\n",
    "        return x\n",
    "\n",
    "# Example integration:\n",
    "\n",
    "# After you obtain `out` from your DAAFBlock:\n",
    "# daaf = DAAFBlock(channels=..., num_heads=...)\n",
    "# fused = daaf(f_rgb_pre, f_depth_pre)\n",
    "#\n",
    "# post_daaf = PostDAAFGhostV2(channels=fused.shape[1], reduction=4, ratio=2)\n",
    "# refined = post_daaf(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07eebee2-ff13-402c-8885-b0d222c474bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GhostASPPFELAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Ghost ASPPFELAN Block:\n",
    "      - Branch 1: GhostConv(1×1) → LeakyReLU\n",
    "      - Branch 2: GhostConv(3×3, dilation=2) → LeakyReLU\n",
    "      - Branch 3: GhostConv(3×3, dilation=4) → LeakyReLU\n",
    "      - Concatenate branches → GhostConv(1×1) → LeakyReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        # Branch 1: 1×1 GhostConv\n",
    "        self.branch1 = nn.Sequential(\n",
    "            GhostModuleV2(in_channels, out_channels,\n",
    "                          kernel_size=1, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        # Branch 2: 3×3 GhostConv with dilation=2\n",
    "        self.branch2 = nn.Sequential(\n",
    "            GhostModuleV2(in_channels, out_channels,\n",
    "                          kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        # adjust dilation on the depthwise step inside GhostModuleV2\n",
    "        # primary_conv uses 1×1 so no change; cheap_operation is depthwise\n",
    "        self.branch2[0].cheap_operation[0].dilation = (2,2)\n",
    "        self.branch2[0].cheap_operation[0].padding = (2,2)\n",
    "\n",
    "        # Branch 3: 3×3 GhostConv with dilation=4\n",
    "        self.branch3 = nn.Sequential(\n",
    "            GhostModuleV2(in_channels, out_channels,\n",
    "                          kernel_size=3, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.branch3[0].cheap_operation[0].dilation = (4,4)\n",
    "        self.branch3[0].cheap_operation[0].padding = (4,4)\n",
    "\n",
    "        # Fusion: concat → 1×1 GhostConv → LeakyReLU\n",
    "        self.fuse = nn.Sequential(\n",
    "            GhostModuleV2(out_channels * 3, out_channels,\n",
    "                          kernel_size=1, use_attn=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, in_channels, H, W)\n",
    "        returns: (B, out_channels, H, W)\n",
    "        \"\"\"\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        # Concatenate along channel dimension\n",
    "        cat = T.cat([b1, b2, b3], dim=1)  # (B, 3*out_channels, H, W)\n",
    "        out = self.fuse(cat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7971a21-44bd-4f5e-a4c8-7f2d825618d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Coordinate Attention block with GhostModuleV2 replacing 1×1 convolutions.\n",
    "    Input: X ∈ ℝ^(B×C×H×W)\n",
    "    Output: X' = X × attn_h × attn_w\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.mid_channels = channels // reduction\n",
    "\n",
    "        # 1×1 GhostConv to reduce channels: C→C/r\n",
    "        self.conv1 = GhostModuleV2(\n",
    "            in_channels=channels,\n",
    "            out_channels=self.mid_channels,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "        # 1×1 GhostConvs to expand back: C/r→C\n",
    "        self.conv_h = GhostModuleV2(\n",
    "            in_channels=self.mid_channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.conv_w = GhostModuleV2(\n",
    "            in_channels=self.mid_channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            use_attn=False\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # 1. Pool along height and width\n",
    "        z_h = F.adaptive_avg_pool2d(x, (H, 1))  # (B, C, H, 1)\n",
    "        z_w = F.adaptive_avg_pool2d(x, (1, W))  # (B, C, 1, W)\n",
    "\n",
    "        # 2. Transpose z_w to match concat shape\n",
    "        z_w = z_w.permute(0, 1, 3, 2)           # (B, C, W, 1)\n",
    "\n",
    "        # 3. Concat and reduce channels\n",
    "        z = T.cat([z_h, z_w], dim=2)        # (B, C, H+W, 1)\n",
    "        z = z.permute(0, 3, 2, 1)               # (B, 1, H+W, C)\n",
    "        z = self.conv1(z)                       # (B, 1, H+W, C/r)\n",
    "        z = self.act(z)\n",
    "\n",
    "        # 4. Split into height and width contexts\n",
    "        z = z.permute(0, 3, 2, 1)               # (B, C/r, H+W, 1)\n",
    "        z_h, z_w = T.split(z, [H, W], dim=2)  # z_h: (B, C/r, H, 1), z_w: (B, C/r, W, 1)\n",
    "\n",
    "        # 5. Generate attention maps\n",
    "        a_h = self.sigmoid(self.conv_h(z_h))   # (B, C, H, 1)\n",
    "        a_w = self.sigmoid(self.conv_w(z_w.permute(0,1,3,2)))  # (B, C, 1, W)\n",
    "\n",
    "        # 6. Apply attention\n",
    "        out = x * a_h * a_w\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4414ae19-8da8-4538-b885-dc656341d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DySample(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic content-aware upsampling (×scale) via point sampling.\n",
    "    Based on “DySample: Learning to Upsample by Learning to Sample”[1].\n",
    "    Input:  X ∈ ℝ^(B×C×H×W)\n",
    "    Output: X_up ∈ ℝ^(B×C×(H·scale)×(W·scale))\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 scale: int = 2,\n",
    "                 use_feat_transform: bool = False):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        # Optional 1×1 GhostConv feature transform\n",
    "        if use_feat_transform:\n",
    "            self.transform = GhostModuleV2(\n",
    "                in_channels, in_channels,\n",
    "                kernel_size=1, use_attn=False\n",
    "            )\n",
    "        else:\n",
    "            self.transform = None\n",
    "        # Predict pixel‐level offsets: 2 coords × scale² shifts per input pixel\n",
    "        self.offset_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            2 * scale * scale,\n",
    "            kernel_size=1,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T.Tensor) -> T.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # 1. Optional feature transform\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        # 2. Initial upsampling via bilinear interpolation\n",
    "        H2, W2 = H * self.scale, W * self.scale\n",
    "        x_interp = F.interpolate(\n",
    "            x, size=(H2, W2),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        # 3. Predict offsets at input resolution\n",
    "        #    shape: (B, 2*scale*scale, H, W)\n",
    "        offsets = self.offset_conv(x)\n",
    "\n",
    "        # 4. Rearrange offsets to per-output-pixel shifts\n",
    "        #    from (B, 2·s², H, W)\n",
    "        # to (B, 2, H·s, W·s)\n",
    "        offsets = offsets.view(B, 2, self.scale, self.scale, H, W)\n",
    "        # reorder to (B, 2, H, s, W, s)\n",
    "        offsets = offsets.permute(0, 1, 4, 2, 5, 3)\n",
    "        # collapse (H, s)→H2 and (W, s)→W2\n",
    "        offsets = offsets.reshape(B, 2, H2, W2)\n",
    "\n",
    "        # 5. Build base sampling grid in normalized coords [-1,1]\n",
    "        # grid has shape (1, H2, W2, 2) with (x,y) in last dim\n",
    "        device = x.device\n",
    "        ys = T.linspace(-1, 1, H2, device=device)\n",
    "        xs = T.linspace(-1, 1, W2, device=device)\n",
    "        grid_y, grid_x = T.meshgrid(ys, xs, indexing='ij')\n",
    "        base_grid = T.stack((grid_x, grid_y), dim=-1)  # (H2, W2, 2)\n",
    "        base_grid = base_grid.unsqueeze(0).expand(B, -1, -1, -1)\n",
    "\n",
    "        # 6. Convert pixel offsets to normalized offsets\n",
    "        #    offset_x_pixel → offset_x_norm = offset_x * 2 / (W2 - 1)\n",
    "        #    offset_y_pixel → offset_y_norm = offset_y * 2 / (H2 - 1)\n",
    "        norm_factor = T.tensor(\n",
    "            [2.0 / max(W2 - 1, 1), 2.0 / max(H2 - 1, 1)],\n",
    "            device=device\n",
    "        ).view(1, 2, 1, 1)\n",
    "        offsets_norm = offsets * norm_factor\n",
    "\n",
    "        # 7. Form final sampling grid and sample\n",
    "        #    grid_sample expects grid in shape (B, H2, W2, 2)\n",
    "        sampling_grid = base_grid + offsets_norm.permute(0, 2, 3, 1)\n",
    "        x_up = F.grid_sample(\n",
    "            x_interp,\n",
    "            sampling_grid,\n",
    "            mode='bilinear',\n",
    "            padding_mode='border',\n",
    "            align_corners=True\n",
    "        )\n",
    "\n",
    "        return x_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92aa391-8453-4ca1-86d9-488b5db790da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ASPPNeXt decoder with 3 stages - WITHOUT final classification layer:\n",
    "      Stage 1: fuse Post-DAAF + skip E4 → GhostASPPFELAN → CoordAttention → DySample\n",
    "      Stage 2: fuse Stage1_out + skip E3 → GhostASPPFELAN → CoordAttention → DySample  \n",
    "      Stage 3: fuse Stage2_out + skip E2 → GhostASPPFELAN → CoordAttention → DySample\n",
    "      \n",
    "    Output: Raw feature maps at 1/4 input resolution (128×96 for 512×384 input)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_channels: int,\n",
    "                 skip_channels: list,\n",
    "                 decoder_channels: list,\n",
    "                 coord_reduction: int = 4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          feature_channels: channels of Post-DAAF output\n",
    "          skip_channels:    [E2, E3, E4] channels from encoder stages\n",
    "          decoder_channels: [stage1, stage2, stage3] desired output channels per stage\n",
    "          coord_reduction:  reduction ratio for CoordAttention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(skip_channels) == 3 and len(decoder_channels) == 3\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        in_ch = feature_channels\n",
    "        \n",
    "        for i in range(3):\n",
    "            skip_ch = skip_channels[2 - i]  # E4→stage0, E3→stage1, E2→stage2\n",
    "            out_ch  = decoder_channels[i]\n",
    "            \n",
    "            block = nn.ModuleDict({\n",
    "                'fuse_skip': GhostModuleV2(\n",
    "                    in_channels=in_ch + skip_ch,\n",
    "                    out_channels=out_ch,\n",
    "                    kernel_size=1,\n",
    "                    use_attn=False\n",
    "                ),\n",
    "                'aspp': GhostASPPFELAN(\n",
    "                    in_channels=out_ch,\n",
    "                    out_channels=out_ch\n",
    "                ),\n",
    "                'coord': CoordAttention(\n",
    "                    channels=out_ch,\n",
    "                    reduction=coord_reduction\n",
    "                ),\n",
    "                'upsample': DySample(\n",
    "                    in_channels=out_ch,\n",
    "                    scale=2,\n",
    "                    use_feat_transform=False\n",
    "                )\n",
    "            })\n",
    "            self.stages.append(block)\n",
    "            in_ch = out_ch  # next stage input\n",
    "\n",
    "    def forward(self, post_daaf: T.Tensor, skips: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          post_daaf: (B, C, H, W) output of Post-DAAF\n",
    "          skips:     [E1, E2, E3, E4] encoder features\n",
    "        Returns:\n",
    "          features: (B, decoder_channels[-1], H*8, W*8) at 1/4 input resolution\n",
    "        \"\"\"\n",
    "        x = post_daaf\n",
    "        \n",
    "        # Process each decoder stage\n",
    "        for stage, skip in zip(self.stages, reversed(skips[1:])):  \n",
    "            # skips[1:] = [E2,E3,E4]; reversed → [E4,E3,E2]\n",
    "            \n",
    "            # 1) Fuse skip connection\n",
    "            x = T.cat([x, skip], dim=1)\n",
    "            x = stage['fuse_skip'](x)\n",
    "            \n",
    "            # 2) GhostASPPFELAN\n",
    "            x = stage['aspp'](x)\n",
    "            \n",
    "            # 3) CoordAttention  \n",
    "            x = stage['coord'](x)\n",
    "            \n",
    "            # 4) DySample upsampling ×2\n",
    "            x = stage['upsample'](x)\n",
    "\n",
    "        return x  # Raw features at 1/4 input resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d69880fa-0804-47e6-b80e-b0df9a721751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtOutputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Separate output layer that takes decoder features at 1/4 resolution\n",
    "    and produces final segmentation masks at full input resolution.\n",
    "    \n",
    "    Supports multiple upsampling strategies:\n",
    "    - Learnable transpose convolution (ConvTranspose2d)\n",
    "    - Bilinear interpolation + refinement conv\n",
    "    - DySample-based learnable upsampling\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 num_classes: int,\n",
    "                 upsampling_method: str = 'transpose',\n",
    "                 use_ghost_conv: bool = False,\n",
    "                 refinement_layers: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_channels:        channels from final decoder stage\n",
    "          num_classes:        number of segmentation classes\n",
    "          upsampling_method:  'transpose', 'bilinear', 'dysample'\n",
    "          use_ghost_conv:     whether to use GhostModuleV2 for final conv\n",
    "          refinement_layers:  number of refinement convs after upsampling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.upsampling_method = upsampling_method\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Upsampling strategies\n",
    "        if upsampling_method == 'transpose':\n",
    "            # Learnable 4× upsampling via transpose convolution\n",
    "            self.upsample = nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=in_channels,\n",
    "                kernel_size=4,\n",
    "                stride=4,\n",
    "                padding=0,\n",
    "                bias=False\n",
    "            )\n",
    "            \n",
    "        elif upsampling_method == 'bilinear':\n",
    "            # Fixed bilinear interpolation\n",
    "            self.upsample = lambda x: F.interpolate(\n",
    "                x, scale_factor=4, \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            \n",
    "        elif upsampling_method == 'dysample':\n",
    "            # Content-aware DySample upsampling\n",
    "            self.upsample = DySample(\n",
    "                in_channels=in_channels,\n",
    "                scale=4,  # 4× upsampling in one step\n",
    "                use_feat_transform=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown upsampling method: {upsampling_method}\")\n",
    "        \n",
    "        # Optional refinement layers\n",
    "        refinement = []\n",
    "        for i in range(refinement_layers):\n",
    "            if use_ghost_conv:\n",
    "                refinement.append(GhostModuleV2(\n",
    "                    in_channels, in_channels,\n",
    "                    kernel_size=3, use_attn=False\n",
    "                ))\n",
    "            else:\n",
    "                refinement.append(nn.Conv2d(\n",
    "                    in_channels, in_channels,\n",
    "                    kernel_size=3, padding=1, bias=False\n",
    "                ))\n",
    "            refinement.append(nn.BatchNorm2d(in_channels))\n",
    "            refinement.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "        self.refinement = nn.Sequential(*refinement)\n",
    "        \n",
    "        # Final classification layer\n",
    "        if use_ghost_conv:\n",
    "            self.classifier = GhostModuleV2(\n",
    "                in_channels, num_classes,\n",
    "                kernel_size=1, use_attn=False\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Conv2d(\n",
    "                in_channels, num_classes,\n",
    "                kernel_size=1, bias=True\n",
    "            )\n",
    "\n",
    "    def forward(self, x: T.Tensor, target_size: tuple = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (B, C, H/4, W/4) decoder output at 1/4 resolution\n",
    "          target_size: (H, W) target output size, if None uses 4×input size\n",
    "        Returns:\n",
    "          logits: (B, num_classes, H, W) at full resolution\n",
    "        \"\"\"\n",
    "        # Upsample to full resolution\n",
    "        if self.upsampling_method == 'bilinear':\n",
    "            if target_size is not None:\n",
    "                x = F.interpolate(x, size=target_size, \n",
    "                                mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                x = self.upsample(x)\n",
    "        else:\n",
    "            x = self.upsample(x)\n",
    "            \n",
    "        # Apply refinement layers\n",
    "        x = self.refinement(x)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        # Ensure exact target size if specified\n",
    "        if target_size is not None and logits.shape[-2:] != target_size:\n",
    "            logits = F.interpolate(logits, size=target_size,\n",
    "                                 mode='bilinear', align_corners=False)\n",
    "            \n",
    "        return logits\n",
    "\n",
    "# Example usage:\n",
    "def create_asppnext_with_separate_output(feature_channels=512,\n",
    "                                       skip_channels=[128, 256, 512],\n",
    "                                       decoder_channels=[256, 128, 64],\n",
    "                                       num_classes=21,\n",
    "                                       upsampling_method='transpose'):\n",
    "    \"\"\"\n",
    "    Factory function to create decoder + output layer\n",
    "    \"\"\"\n",
    "    decoder = ASPPNeXtDecoder(\n",
    "        feature_channels=feature_channels,\n",
    "        skip_channels=skip_channels,\n",
    "        decoder_channels=decoder_channels\n",
    "    )\n",
    "    \n",
    "    output_layer = ASPPNeXtOutputLayer(\n",
    "        in_channels=decoder_channels[-1],  # Final decoder stage channels\n",
    "        num_classes=num_classes,\n",
    "        upsampling_method=upsampling_method,\n",
    "        use_ghost_conv=False,  # Set to True to experiment with GhostModuleV2\n",
    "        refinement_layers=2\n",
    "    )\n",
    "    \n",
    "    return decoder, output_layer\n",
    "\n",
    "# Complete forward pass example:\n",
    "# decoder, output_layer = create_asppnext_with_separate_output()\n",
    "# \n",
    "# # From your pipeline:\n",
    "# decoder_features = decoder(post_daaf_output, encoder_skips)  # (B, 64, 128, 96)\n",
    "# final_logits = output_layer(decoder_features, target_size=(384, 512))  # (B, 21, 384, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b166c0af-fc53-4aa1-9d9d-506b561da4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_ch_rgb: int = 3,\n",
    "                 in_ch_depth: int = 1,\n",
    "                 base_dim: int   = 64,\n",
    "                 num_heads: tuple = (4,8,16,32),\n",
    "                 window_sizes: tuple = (8,4,2,1),\n",
    "                 mlp_ratio: int = 4,\n",
    "                 skip_channels: list = None,\n",
    "                 decoder_channels: list = None,\n",
    "                 num_classes: int = 21,\n",
    "                 coord_reduction: int = 4,\n",
    "                 output_upsample: str = 'transpose'):\n",
    "        super().__init__()\n",
    "        # 1) Encoders for RGB and Depth\n",
    "        self.rgb_encoder   = ASPPNeXtEncoder(in_ch=in_ch_rgb,\n",
    "                                             base_dim=base_dim,\n",
    "                                             num_heads=num_heads,\n",
    "                                             window_sizes=window_sizes,\n",
    "                                             mlp_ratio=mlp_ratio)\n",
    "        self.depth_encoder = ASPPNeXtEncoder(in_ch=in_ch_depth,\n",
    "                                             base_dim=base_dim,\n",
    "                                             num_heads=num_heads,\n",
    "                                             window_sizes=window_sizes,\n",
    "                                             mlp_ratio=mlp_ratio)\n",
    "\n",
    "        # Final encoder feature channels\n",
    "        enc_out_ch = base_dim * 2**3  # stage4 channels\n",
    "\n",
    "        # 2) Pre-DAAF Ghost bottlenecks\n",
    "        self.pre_daaf = PreDAAFGhostV2(channels=enc_out_ch, reduction=4, ratio=2)\n",
    "\n",
    "        # 3) DAAF fusion block\n",
    "        self.daaf = DAAFBlock(channels=enc_out_ch, num_heads=num_heads[-1])\n",
    "\n",
    "        # 4) Post-DAAF Ghost bottleneck\n",
    "        self.post_daaf = PostDAAFGhostV2(channels=enc_out_ch, reduction=4, ratio=2)\n",
    "\n",
    "        # 5) Decoder\n",
    "        # If not provided, default skip and decoder channels:\n",
    "        if skip_channels is None:\n",
    "            skip_channels = [base_dim * 2**i for i in range(1,4)]  # [E2,E3,E4]\n",
    "        if decoder_channels is None:\n",
    "            decoder_channels = [base_dim * 2**i for i in reversed(range(3))]  # [C*4,C*2,C]\n",
    "        self.decoder = ASPPNeXtDecoder(feature_channels=enc_out_ch,\n",
    "                                       skip_channels=skip_channels,\n",
    "                                       decoder_channels=decoder_channels,\n",
    "                                       coord_reduction=coord_reduction)\n",
    "\n",
    "        # 6) Output layer to restore full resolution\n",
    "        self.output = ASPPNeXtOutputLayer(in_channels=decoder_channels[-1],\n",
    "                                          num_classes=num_classes,\n",
    "                                          upsampling_method=output_upsample,\n",
    "                                          use_ghost_conv=False,\n",
    "                                          refinement_layers=2)\n",
    "\n",
    "    def forward(self, rgb: T.Tensor, depth: T.Tensor):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          rgb:   (B, 3, H, W)\n",
    "          depth: (B, 1, H, W)\n",
    "        Output:\n",
    "          logits: (B, num_classes, H, W)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        f_rgb,   rgb_skips   = self.rgb_encoder(rgb)\n",
    "        f_depth, depth_skips = self.depth_encoder(depth)\n",
    "\n",
    "        # Pre-fusion\n",
    "        f_rgb_pre, f_depth_pre = self.pre_daaf(f_rgb, f_depth)\n",
    "\n",
    "        # Cross-modal fusion\n",
    "        daaf_out = self.daaf(f_rgb_pre, f_depth_pre)\n",
    "\n",
    "        # Post-fusion refine\n",
    "        post_daaf_out = self.post_daaf(daaf_out)\n",
    "\n",
    "        # Decoder (returns features at 1/4 resolution)\n",
    "        # Pass in skips: we use rgb_skips (or depth_skips—they have same shapes)\n",
    "        decoder_feats = self.decoder(post_daaf_out, rgb_skips)\n",
    "\n",
    "        # Final segmentation head (restore to full H×W)\n",
    "        logits = self.output(decoder_feats, target_size=rgb.shape[-2:])\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2539a3f9-ec54-418a-a964-8c7d37869bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPNeXtDataset(Dataset):\n",
    "    def __init__(self, image_dir, depth_dir, mask_dir, transform=None):\n",
    "        self.image_paths = sorted([os.path.join(image_dir, f)\n",
    "                                   for f in os.listdir(image_dir)])\n",
    "        self.depth_paths = sorted([os.path.join(depth_dir, f)\n",
    "                                   for f in os.listdir(depth_dir)])\n",
    "        self.mask_paths  = sorted([os.path.join(mask_dir, f)\n",
    "                                   for f in os.listdir(mask_dir)])\n",
    "        assert len(self.image_paths)==len(self.depth_paths)==len(self.mask_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load RGB image with cv2 and convert BGR→RGB\n",
    "        img_bgr = cv2.imread(self.image_paths[idx], cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 2. Load depth (assume single-channel image)\n",
    "        depth_path = self.depth_paths[idx]\n",
    "        if depth_path.endswith(\".npy\"):\n",
    "            depth_np = np.load(depth_path)\n",
    "        else:\n",
    "            depth_np = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n",
    "        depth = depth_np.astype(np.float32)\n",
    "\n",
    "        # 3. Load mask as grayscale\n",
    "        mask_np = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask_np.astype(np.int64)\n",
    "\n",
    "        # 4. Resize if needed (e.g., to 384×512)\n",
    "        if self.transform:\n",
    "            img = cv2.resize(img, self.transform[\"size\"], interpolation=cv2.INTER_LINEAR)\n",
    "            depth = cv2.resize(depth, self.transform[\"size\"], interpolation=cv2.INTER_NEAREST)\n",
    "            mask = cv2.resize(mask, self.transform[\"size\"], interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # 5. Normalize & to tensor\n",
    "        #   img: H×W×3 → 3×H×W, scale [0,255]→[0,1]\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.transpose(img, (2,0,1))\n",
    "        img_t = T.from_numpy(img)\n",
    "\n",
    "        #   depth: H×W → 1×H×W, assume already scaled appropriately\n",
    "        depth = depth[np.newaxis, ...]\n",
    "        depth_t = T.from_numpy(depth)\n",
    "\n",
    "        #   mask: H×W → H×W long tensor\n",
    "        mask_t = T.from_numpy(mask)\n",
    "\n",
    "        return img_t, depth_t, mask_t\n",
    "\n",
    "# Example transforms dict\n",
    "common_transform = {\"size\": (512,384)}  # width,height for cv2.resize\n",
    "\n",
    "# Paths\n",
    "base_img   = r\"D:\\AAU Internship\\Code\\CWF-788\\IMAGE512x384\"\n",
    "base_depth = r\"./Depth_Features\"\n",
    "base_mask  = r\"./Weed_Masks\"\n",
    "\n",
    "splits = {\n",
    "    \"train\":  (\"train_new\",    \"train_new\",    \"train\"),\n",
    "    \"val\":    (\"validation_new\",\"validation_new\",\"val\"),\n",
    "    \"test\":   (\"test_new\",     \"test_new\",     \"test\"),\n",
    "}\n",
    "\n",
    "dataloaders = {}\n",
    "batch_size = 4\n",
    "for phase, (img_s, depth_s, mask_s) in splits.items():\n",
    "    ds = ASPPNeXtDataset(\n",
    "        image_dir = os.path.join(base_img, img_s),\n",
    "        depth_dir = os.path.join(base_depth, depth_s),\n",
    "        mask_dir  = os.path.join(base_mask, mask_s),\n",
    "        transform = common_transform\n",
    "    )\n",
    "    dataloaders[phase] = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(phase==\"train\"),\n",
    "        pin_memory=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26bcf88d-90ee-4018-8c22-c782b345d97b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got NoneType\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mASPPNeXtModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_ch_rgb\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_ch_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_upsample\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbilinear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Create dummy inputs (batch_size=1)\u001b[39;00m\n\u001b[32m     13\u001b[39m rgb_input = T.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m384\u001b[39m, \u001b[32m512\u001b[39m)    \u001b[38;5;66;03m# (B, C, H, W) = (1, 3, 384, 512)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mASPPNeXtModel.__init__\u001b[39m\u001b[34m(self, in_ch_rgb, in_ch_depth, base_dim, num_heads, window_sizes, mlp_ratio, skip_channels, decoder_channels, num_classes, coord_reduction, output_upsample)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 1) Encoders for RGB and Depth\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mself\u001b[39m.rgb_encoder   = \u001b[43mASPPNeXtEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_ch\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_ch_rgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mbase_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.depth_encoder = ASPPNeXtEncoder(in_ch=in_ch_depth,\n\u001b[32m     22\u001b[39m                                      base_dim=base_dim,\n\u001b[32m     23\u001b[39m                                      num_heads=num_heads,\n\u001b[32m     24\u001b[39m                                      window_sizes=window_sizes,\n\u001b[32m     25\u001b[39m                                      mlp_ratio=mlp_ratio)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Final encoder feature channels\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mASPPNeXtEncoder.__init__\u001b[39m\u001b[34m(self, in_ch, base_dim, num_heads, window_sizes, mlp_ratio)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Stage 1 patch‐embedding (no further downsampling here)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.stem = nn.Sequential(\n\u001b[32m      9\u001b[39m     nn.Conv2d(in_ch, base_dim, \u001b[32m4\u001b[39m, stride=\u001b[32m4\u001b[39m, bias=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbase_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.stages = nn.ModuleList()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m4\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:196\u001b[39m, in \u001b[36mLayerNorm.__init__\u001b[39m\u001b[34m(self, normalized_shape, eps, elementwise_affine, bias, device, dtype)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28mself\u001b[39m.elementwise_affine = elementwise_affine\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.elementwise_affine:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     )\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    199\u001b[39m         \u001b[38;5;28mself\u001b[39m.bias = Parameter(\n\u001b[32m    200\u001b[39m             torch.empty(\u001b[38;5;28mself\u001b[39m.normalized_shape, **factory_kwargs)\n\u001b[32m    201\u001b[39m         )\n",
      "\u001b[31mTypeError\u001b[39m: empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got NoneType\""
     ]
    }
   ],
   "source": [
    "model = ASPPNeXtModel(\n",
    "    in_ch_rgb=3,\n",
    "    in_ch_depth=1,\n",
    "    base_dim=64,\n",
    "    num_heads=(4,8,16,32),\n",
    "    window_sizes=(8,4,2,1),\n",
    "    mlp_ratio=4,\n",
    "    num_classes=2,\n",
    "    output_upsample='bilinear'\n",
    ")\n",
    "\n",
    "# Create dummy inputs (batch_size=1)\n",
    "rgb_input = T.randn(1, 3, 384, 512)    # (B, C, H, W) = (1, 3, 384, 512)\n",
    "depth_input = T.randn(1, 1, 384, 512)   # (1, 1, 384, 512)\n",
    "\n",
    "# Forward pass\n",
    "with T.no_grad():\n",
    "    output = model(rgb_input, depth_input)\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Input shape (RGB):\", rgb_input.shape)\n",
    "print(\"Input shape (Depth):\", depth_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Verify resolution match\n",
    "assert output.shape[-2:] == (384, 512), \\\n",
    "    f\"Output size {output.shape[-2:]} ≠ Input size (384, 512)\"\n",
    "print(\"\\n✅ Size verification passed! Output matches input resolution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0cc14-0dec-4e55-a53f-2d0018cecd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
